{"version":"1","records":[{"hierarchy":{"lvl1":"Welcome"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Welcome"},"content":"This machine learning course is created with Jupyter notebooks that allow you to interact with all the machine learning concepts\nand algorithms to understand them better. At the same time, you’ll learn how to control these algorithms and use them in practice.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Welcome","lvl2":"Lectures"},"type":"lvl2","url":"/#lectures","position":2},{"hierarchy":{"lvl1":"Welcome","lvl2":"Lectures"},"content":"Lectures can be viewed online as notebooks, as slides (online or PDF), or as videos (hosted on YouTube). They all have the same content.\nUpon opening the notebooks, you can  launch them in Google Colab (or Binder), or run them locally.\n\n\n\nNotebooks\n\nSlides\n\nVideo\n\n1\n\nIntroduction\n\nHTML         - \n\nPDF\n\nYoutube\n\n2\n\nLinear Models\n\nHTML      - \n\nPDF\n\nYoutube\n\n3\n\nModel Evaluation\n\nHTML    - \n\nPDF\n\nYoutube2\n\n4\n\nEnsemble Learning\n\nHTML  - \n\nPDF\n\nYoutube\n\n5\n\nData Preprocessing\n\nHTML - \n\nPDF\n\nYoutube\n\n6\n\nNeural Networks1\n\nHTML  - \n\nPDF\n\nYoutube\n\n7\n\nConvolutional Neural Networks1\n\nHTML  - \n\nPDF\n\nYoutube\n\n8\n\nTransformers1\n\nHTML  - \n\nPDF\n\nYoutube\n\n9\n\nFinetuning Foundation Models1\n\nHTML  - \n\nPDF\n\nYoutube\n\n1 These lectures (slides and video recordings) are being updated.\n2 The order of the slides in the video is slightly different.\n\nGet your hands dirty\n\nRetrieve all materials by cloning the  \n\nGitHub repo. To run the notebooks locally, see the \n\nprerequisites.\n\nHave some feedback?\n\nIf you notice any issue, or have suggestions or requests, please go the\n \n\nissue tracker or directly click on the   icon on top of the page and then 'open issue`. We also welcome pull requests :).","type":"content","url":"/#lectures","position":3},{"hierarchy":{"lvl1":"Welcome","lvl2":"Labs"},"type":"lvl2","url":"/#labs","position":4},{"hierarchy":{"lvl1":"Welcome","lvl2":"Labs"},"content":"Download the lab notebooks and solve the questions locally, or launch them in Google Colab or Binder. Please review the relevant tutorials before starting the labs. Solutions will appear towards the end of each lab session.\n\n\n\nNotebooks\n\nTutorial\n\nSolutions\n\n1\n\nLinear Models for regression  \n\nLinear Models for classification\n\nTutorial\n\nLab 1a  \n\nLab 1b (Release date: 4 Feb, 12:00)\n\n2\n\nModel Evaluation\n\nTutorial\n\nLab 2 (Release date: 11 Feb, 12:00)\n\n3\n\nEnsembles  \n\nData engineering\n\nTutorial\n\nLab 3a  \n\nLab 3b (Release date: 25 Feb, 12:00)\n\n4\n\nNeural Networks\n\nTutorial\n\nLab 4 (Release date: 4 Mar, 12:00)\n\n5\n\nConvolutional Neural Networks\n\n/\n\nLab 5 (Release date: 11 Mar, 12:00)\n\n6\n\nTransformers1\n\nTutorial\n\nLab 6 (Release date: 18 Mar, 12:00)\n\n6\n\nFinetuning Foundation Models1\n\nTutorial\n\nLab 7 (Release date: 25 Mar, 12:00)","type":"content","url":"/#labs","position":5},{"hierarchy":{"lvl1":"Welcome","lvl2":"Background materials"},"type":"lvl2","url":"/#background-materials","position":6},{"hierarchy":{"lvl1":"Welcome","lvl2":"Background materials"},"content":"","type":"content","url":"/#background-materials","position":7},{"hierarchy":{"lvl1":"Welcome","lvl3":"Tutorials","lvl2":"Background materials"},"type":"lvl3","url":"/#tutorials","position":8},{"hierarchy":{"lvl1":"Welcome","lvl3":"Tutorials","lvl2":"Background materials"},"content":"General introductions into using Python for scientific programming and machine learning.\n\nPython basics\n\nPython for data analysis\n\nMachine learning in Python","type":"content","url":"/#tutorials","position":9},{"hierarchy":{"lvl1":"Welcome","lvl3":"Extra lectures","lvl2":"Background materials"},"type":"lvl3","url":"/#extra-lectures","position":10},{"hierarchy":{"lvl1":"Welcome","lvl3":"Extra lectures","lvl2":"Background materials"},"content":"Lectures on both basic machine learning techniques (useful for novices to cover any knowledge gaps), as well as additional useful techniques that we couldn’t fit into the schedule.\n\nDecision trees\n\nNearest Neighbors\n\nData Preprocessing Basics\n\nKernelization\n\nBayesian Learning","type":"content","url":"/#extra-lectures","position":11},{"hierarchy":{"lvl1":"Welcome","lvl3":"Recommended resources","lvl2":"Background materials"},"type":"lvl3","url":"/#recommended-resources","position":12},{"hierarchy":{"lvl1":"Welcome","lvl3":"Recommended resources","lvl2":"Background materials"},"content":"These resources help to further deepen your skills, and are well aligned with this course.\n\nScientific Python Lectures (by J.R. Johansson)\n\nMathematics for Machine Learning (by M.P. Deisenroth et al.)\n\nThe official PyTorch Tutorial\n\nfast.ai online course on practical deep learning\n\nGoogle Machine Learning crash course","type":"content","url":"/#recommended-resources","position":13},{"hierarchy":{"lvl1":"Prerequisites"},"type":"lvl1","url":"/labs/lab-0-prerequisites","position":0},{"hierarchy":{"lvl1":"Prerequisites"},"content":"This is a guide to set up a local development environment for this course.\n\n","type":"content","url":"/labs/lab-0-prerequisites","position":1},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Python"},"type":"lvl2","url":"/labs/lab-0-prerequisites#python","position":2},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Python"},"content":"You first need to set up a Python environment (if you do not have done so already). The easiest way to do this is by installing \n\nMiniconda, which will install Python as well as a set of commonly used packages. We will be using Python 3, so be sure to install the right version. Always install a 64-bit installer (if your machine supports it), and we recommend using Python 3.10 or later.\n\nIf you are completely new to Python, we recommend reading the \n\nPython Data Science Handbook or taking an introductory online course, such as the \n\nDefinite Guide to Python, the \n\nWhirlwind Tour of Python, or \n\nthis Python Course. If you like a step-by-step approach, try the \n\nDataCamp Intro to Python for Data Science.\n\nTo practice your skills, try some \n\nHackerrank challenges.\n\n","type":"content","url":"/labs/lab-0-prerequisites#python","position":3},{"hierarchy":{"lvl1":"Prerequisites","lvl3":"OS specific notes","lvl2":"Python"},"type":"lvl3","url":"/labs/lab-0-prerequisites#os-specific-notes","position":4},{"hierarchy":{"lvl1":"Prerequisites","lvl3":"OS specific notes","lvl2":"Python"},"content":"Windows users: If you are new to Anaconda, read the \n\nstarting guide. You’ll probably use the Anaconda Prompt to run any commands or to start Jupyter Lab.\n\nMac users: You’ll probably use your terminal to run any commands or to start Jupyter Lab. Make sure that you have Command Line tools installed. If not, run xcode-select --install. You won’t need a full XCode installation.\n\nAll: Install the correct version of \n\ngraphviz according to your OS.\n\n","type":"content","url":"/labs/lab-0-prerequisites#os-specific-notes","position":5},{"hierarchy":{"lvl1":"Prerequisites","lvl3":"Virtual environments","lvl2":"Python"},"type":"lvl3","url":"/labs/lab-0-prerequisites#virtual-environments","position":6},{"hierarchy":{"lvl1":"Prerequisites","lvl3":"Virtual environments","lvl2":"Python"},"content":"If you already have a custom Python environment set up, possibly using a different Python version, we highly recommend to set up a virtual environment to avoid interference with other projects and classes. This is not strictly needed if you use a fresh Anaconda install, since that will automatically create a new environment on installation.","type":"content","url":"/labs/lab-0-prerequisites#virtual-environments","position":7},{"hierarchy":{"lvl1":"Prerequisites","lvl4":"Using conda","lvl3":"Virtual environments","lvl2":"Python"},"type":"lvl4","url":"/labs/lab-0-prerequisites#using-conda","position":8},{"hierarchy":{"lvl1":"Prerequisites","lvl4":"Using conda","lvl3":"Virtual environments","lvl2":"Python"},"content":"To create a new conda environment called ‘mlcourse’ (or whatever you like), runconda create -n mlcourse python=3.10\n\nYou activate the environment with conda activate mlcourse and deacticate it with conda deactivate.","type":"content","url":"/labs/lab-0-prerequisites#using-conda","position":9},{"hierarchy":{"lvl1":"Prerequisites","lvl4":"Using virtualenv","lvl3":"Virtual environments","lvl2":"Python"},"type":"lvl4","url":"/labs/lab-0-prerequisites#using-virtualenv","position":10},{"hierarchy":{"lvl1":"Prerequisites","lvl4":"Using virtualenv","lvl3":"Virtual environments","lvl2":"Python"},"content":"To can also use \n\nvenv if you prefer:pip install virtualenv\nvirtualenv mlcourse\n\nActivate the environment with source mlcourse/bin/activate or mlcourse\\Scripts\\activate on Windows. To deactivate the virtual environment, type deactivate.\n\n","type":"content","url":"/labs/lab-0-prerequisites#using-virtualenv","position":11},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Course materials on GitHub"},"type":"lvl2","url":"/labs/lab-0-prerequisites#course-materials-on-github","position":12},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Course materials on GitHub"},"content":"The course materials are available on GitHub, so that you can easily pull (download) the latest updates. We recommend \n\ninstalling git (if you haven’t already), and then ‘clone’ the repository from the command line (you can also use a \n\nGUI)git clone https://github.com/ML-course/master.git\n\nTo download updates, run git pull\n\nFor more details on using git, see the \n\nGitHub 10-minute tutorial and \n\nGit for Ages 4 and up. We’ll use git extensively in the course (e.g., to submit assignments).\n\nAlternatively, you can download the course \n\nas a .zip file. Click ‘Code’ and then ‘Download ZIP’. Or, download individual files with right-click -> Save Link As...\n\n","type":"content","url":"/labs/lab-0-prerequisites#course-materials-on-github","position":13},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Installing required packages"},"type":"lvl2","url":"/labs/lab-0-prerequisites#installing-required-packages","position":14},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Installing required packages"},"content":"Next, you’ll need to install several packages that we’ll be using extensively in this course, using pip (the Python Package index).Run the following from the folder where you cloned (or downloaded) the course, or adjust the path to the requirements.txt file:pip install --upgrade pip\npip install -U -r requirements.txt\n\nNote: the -U option updates all packages, should you have older versions already installed.\n\n","type":"content","url":"/labs/lab-0-prerequisites#installing-required-packages","position":15},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Running the course notebooks"},"type":"lvl2","url":"/labs/lab-0-prerequisites#running-the-course-notebooks","position":16},{"hierarchy":{"lvl1":"Prerequisites","lvl2":"Running the course notebooks"},"content":"As our coding environment, we’ll be using Jupyter notebooks. They interleave documentation (in markdown) with executable Python code, and they run in your browser. That means that you can easily edit and re-run all the code in this course. If you are new to notebooks, \n\ntake this quick tutorial, or \n\nthis more detailed one. Optionally, for a more in-depth coverage, \n\ntry the DataCamp tutorial.\n\nRun jupyter lab from the folder where you have downloaded (or cloned) the course materials, using the Python environment you created above.jupyter lab\n\nA browser window should open with all course materials. Open one of the chapters and check if you can execute all code by clicking Cell > Run all.  You can shut down the notebook by typing CTRL-C in your terminal.\n\n","type":"content","url":"/labs/lab-0-prerequisites#running-the-course-notebooks","position":17},{"hierarchy":{"lvl1":"Prerequisites","lvl3":"An alternative: Google Colab","lvl2":"Running the course notebooks"},"type":"lvl3","url":"/labs/lab-0-prerequisites#an-alternative-google-colab","position":18},{"hierarchy":{"lvl1":"Prerequisites","lvl3":"An alternative: Google Colab","lvl2":"Running the course notebooks"},"content":"Google Colab allows you to run notebooks in your browser without any local installation. It also provides (limited) GPU resources. It is a useful alternative in case you encounter issues with your local installation or don’t have it available, or to easily use GPUs.\n\nThe \n\ncourse overview page has buttons to launch all materials in Colab (or Binder), or you can upload the notebooks to Colab yourself.","type":"content","url":"/labs/lab-0-prerequisites#an-alternative-google-colab","position":19},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python"},"type":"lvl1","url":"/labs/lab-1-tutorial","position":0},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python"},"content":"Joaquin Vanschoren, Pieter Gijsbers, Bilge Celik, Prabhant Singh\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial","position":1},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Overview"},"type":"lvl3","url":"/labs/lab-1-tutorial#overview","position":2},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Overview"},"content":"Why Python?\n\nIntro to scikit-learn\n\nExercises\n\n","type":"content","url":"/labs/lab-1-tutorial#overview","position":3},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl2":"Why Python?"},"type":"lvl2","url":"/labs/lab-1-tutorial#why-python","position":4},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl2":"Why Python?"},"content":"Many data-heavy applications are now developed in Python\n\nHighly readable, less complexity, fast prototyping\n\nEasy to offload number crunching to underlying C/Fortran/...\n\nEasy to install and import many rich libraries\n\nnumpy: efficient data structures\n\nscipy: fast numerical recipes\n\nmatplotlib: high-quality graphs\n\nscikit-learn: machine learning algorithms\n\ntensorflow: neural networks\n\n...\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#why-python","position":5},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl2":"Numpy, Scipy, Matplotlib"},"type":"lvl2","url":"/labs/lab-1-tutorial#numpy-scipy-matplotlib","position":6},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl2":"Numpy, Scipy, Matplotlib"},"content":"See the tutorials (in the course GitHub)\n\nMany good tutorials online\n\nJake VanderPlas’ book and notebooks\n\nJ.R. Johansson’s notebooks\n\nDataCamp\n\n...\n\n","type":"content","url":"/labs/lab-1-tutorial#numpy-scipy-matplotlib","position":7},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl2":"scikit-learn"},"type":"lvl2","url":"/labs/lab-1-tutorial#scikit-learn","position":8},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl2":"scikit-learn"},"content":"One of the most prominent Python libraries for machine learning:\n\nContains many state-of-the-art machine learning algorithms\n\nBuilds on numpy (fast), implements advanced techniques\n\nWide range of evaluation measures and techniques\n\nOffers \n\ncomprehensive documentation about each algorithm\n\nWidely used, and a wealth of \n\ntutorials and code snippets are available\n\nWorks well with numpy, scipy, pandas, matplotlib,...\n\n","type":"content","url":"/labs/lab-1-tutorial#scikit-learn","position":9},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Algorithms","lvl2":"scikit-learn"},"type":"lvl3","url":"/labs/lab-1-tutorial#algorithms","position":10},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Algorithms","lvl2":"scikit-learn"},"content":"See the \n\nReference\n\nSupervised learning:\n\nLinear models (Ridge, Lasso, Elastic Net, ...)\n\nSupport Vector Machines\n\nTree-based methods (Classification/Regression Trees, Random Forests,...)\n\nNearest neighbors\n\nNeural networks\n\nGaussian Processes\n\nFeature selection\n\nUnsupervised learning:\n\nClustering (KMeans, ...)\n\nMatrix Decomposition (PCA, ...)\n\nManifold Learning (Embeddings)\n\nDensity estimation\n\nOutlier detection\n\nModel selection and evaluation:\n\nCross-validation\n\nGrid-search\n\nLots of metrics\n\n","type":"content","url":"/labs/lab-1-tutorial#algorithms","position":11},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Data import","lvl2":"scikit-learn"},"type":"lvl3","url":"/labs/lab-1-tutorial#data-import","position":12},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Data import","lvl2":"scikit-learn"},"content":"Multiple options:\n\nA few toy datasets are included in sklearn.datasets\n\nImport \n\n1000s of datasets via sklearn.datasets.fetch_openml\n\nYou can import data files (CSV) with pandas or numpy\n\nfrom sklearn.datasets import load_iris, fetch_openml\niris_data = load_iris()\ndating_data = fetch_openml(\"SpeedDating\", version=1)\n\n\n\nThese will return a Bunch object (similar to a dict)\n\nprint(\"Keys of iris_dataset: {}\".format(iris_data.keys()))\nprint(iris_data['DESCR'][:193] + \"\\n...\")\n\n\n\nTargets (classes) and features are lists of strings\n\nData and target values are always numeric (ndarrays)\n\nprint(\"Targets: {}\".format(iris_data['target_names']))\nprint(\"Features: {}\".format(iris_data['feature_names']))\nprint(\"Shape of data: {}\".format(iris_data['data'].shape))\nprint(\"First 5 rows:\\n{}\".format(iris_data['data'][:5]))\nprint(\"Targets:\\n{}\".format(iris_data['target']))\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#data-import","position":13},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl3","url":"/labs/lab-1-tutorial#building-models","position":14},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl3":"Building models","lvl2":"scikit-learn"},"content":"All scikitlearn estimators follow the same interface\n\nclass SupervisedEstimator(...):\n    def __init__(self, hyperparam, ...):\n\n    def fit(self, X, y):   # Fit/model the training data\n        ...                # given data X and targets y\n        return self\n     \n    def predict(self, X):  # Make predictions\n        ...                # on unseen data X  \n        return y_pred\n    \n    def score(self, X, y): # Predict and compare to true\n        ...                # labels y                \n        return score\n\n","type":"content","url":"/labs/lab-1-tutorial#building-models","position":15},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Training and testing data","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl4","url":"/labs/lab-1-tutorial#training-and-testing-data","position":16},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Training and testing data","lvl3":"Building models","lvl2":"scikit-learn"},"content":"To evaluate our classifier, we need to test it on unseen data.train_test_split: splits data randomly in 75% training and 25% test data.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_data['data'], iris_data['target'], \n    random_state=0)\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))\n\n\n\nWe can also choose other ways to split the data. For instance, the following will create a training set of 10% of the data and a test set of 5% of the data. This is useful when dealing with very large datasets. stratify defines the target feature to stratify the data (ensure that the class distributions are kept the same).\n\nX, y = iris_data['data'], iris_data['target']\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X,y, stratify=y, train_size=0.1, test_size=0.05)\nprint(\"Xs_train shape: {}\".format(Xs_train.shape))\nprint(\"Xs_test shape: {}\".format(Xs_test.shape))\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#training-and-testing-data","position":17},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Looking at your data (with pandas)","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl4","url":"/labs/lab-1-tutorial#looking-at-your-data-with-pandas","position":18},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Looking at your data (with pandas)","lvl3":"Building models","lvl2":"scikit-learn"},"content":"\n\nfrom pandas.plotting import scatter_matrix\n\n# Build a DataFrame with training examples and feature names\niris_df = pd.DataFrame(X_train, \n                       columns=iris_data.feature_names)\n\n# scatter matrix from the dataframe, color by class\nsm = scatter_matrix(iris_df, c=y_train, figsize=(8, 8), \n                  marker='o', hist_kwds={'bins': 20}, s=60, \n                  alpha=.8)\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#looking-at-your-data-with-pandas","position":19},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Fitting a model","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl4","url":"/labs/lab-1-tutorial#fitting-a-model","position":20},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Fitting a model","lvl3":"Building models","lvl2":"scikit-learn"},"content":"\n\nThe first model we’ll build is a k-Nearest Neighbor classifier.kNN is included in sklearn.neighbors, so let’s build our first model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#fitting-a-model","position":21},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Making predictions","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl4","url":"/labs/lab-1-tutorial#making-predictions","position":22},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Making predictions","lvl3":"Building models","lvl2":"scikit-learn"},"content":"Let’s create a new example and ask the kNN model to classify it\n\nX_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n       iris_data['target_names'][prediction]))\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#making-predictions","position":23},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Evaluating the model","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl4","url":"/labs/lab-1-tutorial#evaluating-the-model","position":24},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Evaluating the model","lvl3":"Building models","lvl2":"scikit-learn"},"content":"Feeding all test examples to the model yields all predictions\n\ny_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\n\n\n\nThe score function computes the percentage of correct predictionsknn.score(X_test, y_test)\n\nprint(\"Score: {:.2f}\".format(knn.score(X_test, y_test) ))\n\n\n\nInstead of a single train-test split, we can use cross_validate do run a cross-validation.\nIt will return the test scores, as well as the fit and score times, for every fold.\nBy default, scikit-learn does a 5-fold cross-validation, hence returning 5 test scores.\n\n!pip install -U joblib\n\n\n\nfrom sklearn.model_selection import cross_validate\nxval = cross_validate(knn, X, y, return_train_score=True, n_jobs=-1)\nxval\n\n\n\nThe mean should give a better performance estimate\n\nnp.mean(xval['test_score'])\n\n\n\n","type":"content","url":"/labs/lab-1-tutorial#evaluating-the-model","position":25},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Introspecting the model","lvl3":"Building models","lvl2":"scikit-learn"},"type":"lvl4","url":"/labs/lab-1-tutorial#introspecting-the-model","position":26},{"hierarchy":{"lvl1":"Lab 1: Machine Learning with Python","lvl4":"Introspecting the model","lvl3":"Building models","lvl2":"scikit-learn"},"content":"Most models allow you to retrieve the trained model parameters, usually called coef_\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X_train, y_train)\nlr.coef_\n\n\n\nMatching these with the names of the features, we can see which features are primarily used by the model\n\nd = zip(iris_data.feature_names,lr.coef_)\nset(d)\n\n\n\nPlease see the course notebooks for more examples on how to analyse models.","type":"content","url":"/labs/lab-1-tutorial#introspecting-the-model","position":27},{"hierarchy":{"lvl1":"Lab 1a: Linear regression"},"type":"lvl1","url":"/labs/lab-1a-linear-models-for-regression","position":0},{"hierarchy":{"lvl1":"Lab 1a: Linear regression"},"content":"The \n\nNO2 dataset contains 500 measurement of pollution caused by cars. The goal is to predict the concentration of NO_2 from data about traffic and atmospheric conditions. The predictive variables include the number of cars per hour, temperature, wind, and time of day.\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nfrom matplotlib import cm\n\n# Hide convergence warning for now\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n\n\n# Download NO2 data. Takes a while the first time.\nno2 = oml.datasets.get_dataset(547)\nX, y, _, _ = no2.get_data(target=no2.default_target_attribute); \nattribute_names = list(X)\n\n\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression","position":1},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Quick visualization"},"type":"lvl2","url":"/labs/lab-1a-linear-models-for-regression#quick-visualization","position":2},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Quick visualization"},"content":"We can use pandas to quickly visualize the data. If you are new to pandas, take some time to understand the code.\n\nWe’ll remove the ‘day’ feature to focus on the non-temporal aspects of this interaction. We are not aiming to predict future levels, and even if we would it would require special treatment (e.g. different train-test splits). There also doesn’t seem to be a long term trend in the data, even though there are clear periodic trends in temperature.\n\ndf = pd.DataFrame(X, columns=attribute_names).join(pd.DataFrame(list(y),columns=['target']))\ndf = df.sort_values(['day','hour_of_day']).drop('day',axis=1)\ndf.plot(use_index=False,figsize=(20,5),cmap=cm.get_cmap('brg'));\nX = X.drop('day',axis=1)\n\n\n\ndf.head()\n\n\n\nIf we plot the data, ordered by time of measurement, we can see that the wind direction (measured in angular degrees) is scaled very differently from the other features. Let’s now zoom in to the other measures:\n\ndf.drop('wind_direction',axis=1).plot(use_index=False,figsize=(20,5),cmap=cm.get_cmap('brg'));\n\n\n\nWe can see that the target (NO_2 levels) seem to be correlated to the number of cars per hour, which makes sense because cars produce NO_2. Other influences (air temperature differences and wind) seem to have a more complex and subtle effect. Let’s try to model these using linear regression models.\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#quick-visualization","position":3},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Exercise 1: Model benchmark"},"type":"lvl2","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-model-benchmark","position":4},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Exercise 1: Model benchmark"},"content":"It is clear that NO_2 concentrations depend on a combination of these features, so we will now try to learn this complex relationship. We first evaluate a range of linear regression problems, i.e. Linear Regression, Ridge, Lasso and ElasticNet, as well as kNN. Since we observed that some features have very different scales, we’ll also build pipelines of all these measures with an additional scaling step. For now, we’ll stick to the default hyperparameter settings.\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-model-benchmark","position":5},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 1.1","lvl2":"Exercise 1: Model benchmark"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-1","position":6},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 1.1","lvl2":"Exercise 1: Model benchmark"},"content":"Implement a function below which evaluates each classifier passed into it on the given data, and then returns both the train and test scores of each as a list. You are allowed to import additional functions from whichever module you like, but you should be able to complete the function with \n\ncross_validate function and standard Python built-ins. Below you the function you will find example output.\n\ndef evaluate_learners(models, X, y):\n    \"\"\"     \n    Given a list of models [model1, model2, ..., modelN] return two lists:\n     - a list with the scores obtained on the training samples for each model,\n     - a list with the test scores obtained on the test samples for each model.\n     The order of scores should match the order in which the models were originally provided. E.g.:     \n     [Model1 train score, ..., ModelN train score], [Model1 test score, ..., ModelN test score]\n    \"\"\"\n    pass\n\n# # Example output:\n# train_scores, test_scores = ([[0.92 , 0.924, 0.916, 0.917, 0.921],  # Model 1 train score for each of 5 folds.\n#                               [0.963, 0.962, 0.953, 0.912, 0.934],  # Model 2 train score for each of 5 folds.\n#                               ..\n#                              [[0.801, 0.811, 0.806, 0.826, 0.804],  # Model 1 test score for each of 5 folds.\n#                               [0.766, 0.756, 0.773, 0.756, 0.741],  # Model 2 test score for each of 5 folds.\n#                               ..\n\n\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-1","position":7},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 1.2","lvl2":"Exercise 1: Model benchmark"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-2","position":8},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 1.2","lvl2":"Exercise 1: Model benchmark"},"content":"Call the function you created with a Linear Regression, Ridge, Lasso and ElasticNet, as well as kNN.\nStore the return values in the variables train_scores and test_scores. Then, run the code given below to produce a plot visualizing the scores.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Dummy code. Replace with the actual classifiers and scores\nmodels = [LinearRegression()]\ntrain_scores, test_scores = [[0.6,0.7,0.8]], [[0.5,0.6,0.7]]\n\n\n\n# Plot a bar chart of the train and test scores of all the classifiers, including the variance as error bars\nfig, ax = plt.subplots(figsize=(10,6))\nwidth=0.45\n\nax.barh(np.arange(len(train_scores)), np.mean(test_scores, axis=1), width,\n        yerr= np.std(test_scores, axis=1), color='green', label='test R^2')\nax.barh(np.arange(len(train_scores))-width, np.mean(train_scores, axis=1), width,\n        yerr= np.std(train_scores, axis=1), color='red', label='train R^2')\nfor i, te, tr in zip(np.arange(len(train_scores)),test_scores,train_scores):\n    ax.text(0, i, \"{:.3f} +- {:.3f}\".format(np.mean(te),np.std(te)), color=('white' if np.mean(te)>0.1 else 'black'), va='center')\n    ax.text(0, i-width, \"{:.3f} +- {:.3f}\".format(np.mean(tr),np.std(tr)), color=('white' if np.mean(tr)>0.1 else 'black'), va='center')\nlabels = [c.__class__.__name__ if not hasattr(c, 'steps') else c.steps[0][0] + \"_\" + c.steps[1][0] for c in models]\nax.set(yticks=np.arange(len(train_scores))-width/2, yticklabels=labels)\nax.legend(bbox_to_anchor=(1.05, 1), loc=2)\n\nplt.show()\n\n\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-2","position":9},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 1.3","lvl2":"Exercise 1: Model benchmark"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-3","position":10},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 1.3","lvl2":"Exercise 1: Model benchmark"},"content":"Interpret the plot. Which is the best regressor? Are any of the models overfitting? If so, what can we do to solve this? Is there a lot of variance in the results?\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-1-3","position":11},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Exercise 2: Regularization"},"type":"lvl2","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-regularization","position":12},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Exercise 2: Regularization"},"content":"We will now tune these algorithm’s main regularization hyperparameter: the regularization hyperparameter (alpha) in Lasso and Ridge, and the number of neighbors (n_neighbors) in kNN.\n\nWe expect the optimum for the alpha parameters to lie in [10^{-12},10^{12}] and for n_neighbors between 1 and 50. alpha should be varied on a log scale (i.e. [0.01, 0.1, 1, 10, 100]), n_neighbors should be varied uniformly (i.e. [1,2,3,4]).\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-regularization","position":13},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 2.1","lvl2":"Exercise 2: Regularization"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-1","position":14},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 2.1","lvl2":"Exercise 2: Regularization"},"content":"Vary the hyperparameters in the range given above and, for each regressor, create a line plot that plots both the training and test score for every value of the regularization hyperparameter. Hence, you should produce 3 plots, one for each regressor. Use the default 5-fold cross validation for all scores, but only plot the means.\n\nHints:\n\nThink about the time complexity of these models. Trying too many hyperparameter values may take too much time.\n\nYou can make use of numpy’s \n\nlogspace, \n\ngeomspace, and \n\nlinspace functions.\n\nYou can use matplotlib’s default \n\nplot function to plot the train and test scores.\n\nYou can manually loop over the hyperparameter ranges, or you can already check out scikit-learn’s \n\nGridSearchCV function to save some programming. We’ll see it again later in the course.\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-1","position":15},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 2.2","lvl2":"Exercise 2: Regularization"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-2","position":16},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 2.2","lvl2":"Exercise 2: Regularization"},"content":"Interpret the plots. When are the methods underfitting? When are they overfitting? How sensitive are they to the regularization hyperparameter?\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-2","position":17},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 2.3","lvl2":"Exercise 2: Regularization"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-3","position":18},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 2.3","lvl2":"Exercise 2: Regularization"},"content":"ElasticNet allows to mix L1 and L2 loss, and the l1_ratio hyperparameter defines the ratio of L1 loss. Hence, it has two interacting hyperparameters: l1_ratio and alpha. Run a grid search to obtain a matrix of l1_ratio and alpha values and the resulting cross-validation scores. Then, use the function provided below to plot a heatmap of all values and interpret the result. Can you explain how the two hyperparameters interact?\n\n# Generic heatmap\ndef heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\", printvalues=False):\n    \"\"\"\n    Plots a heatmap for the performance of a model for every combination of two hyperparameter values\n    \n    values: nxn array with all evaluation results, varying the first hyperparameter first\n    xlabel: name of the first hyperparameter\n    ylabel: name of the second hyperparameter\n    xticklabels: values of the first hyperparameter\n    yticklabels: values of the second hyperparameter\n    cmap: colormap\n    vmin: minimal score\n    vmax: maximal score\n    ax: plot axes\n    fmt: format for printing the scores\n    printvalues: whether to print the scores\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n    img = ax.pcolor(values, cmap=cmap, vmin=None, vmax=None)\n    img.update_scalarmappable()\n    ax.set_xlabel(xlabel, fontsize=10)\n    ax.set_ylabel(ylabel, fontsize=10)\n    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n    ax.set_xticklabels(xticklabels)\n    ax.set_yticklabels(yticklabels)\n    ax.set_aspect(1)\n    \n    ax.tick_params(axis='y', labelsize=12)\n    ax.tick_params(axis='x', labelsize=12, labelrotation=90)\n\n    if(printvalues):\n        for p, color, value in zip(img.get_paths(), img.get_facecolors(), img.get_array()):\n            x, y = p.vertices[:-2, :].mean(0)\n            if np.mean(color[:3]) > 0.5:\n                c = 'k'\n            else:\n                c = 'w'\n            ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\", size=10)\n    return img\n\n\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-2-3","position":19},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Exercise 3: Visualizing coefficients"},"type":"lvl2","url":"/labs/lab-1a-linear-models-for-regression#exercise-3-visualizing-coefficients","position":20},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl2":"Exercise 3: Visualizing coefficients"},"content":"Finally, let’s verify whether the different optimized linear models also find the same coefficients.","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-3-visualizing-coefficients","position":21},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 3.1","lvl2":"Exercise 3: Visualizing coefficients"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-3-1","position":22},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 3.1","lvl2":"Exercise 3: Visualizing coefficients"},"content":"Draw a \n\nscatterplot plotting the coefficients of the different models in different colors. Do you see much difference between the different models?\n\nFor all models, choose an alpha parameter that seems to work well in the previous exercise. When in doubt, use alpha=0.001.\n\n","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-3-1","position":23},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 3.2","lvl2":"Exercise 3: Visualizing coefficients"},"type":"lvl3","url":"/labs/lab-1a-linear-models-for-regression#exercise-3-2","position":24},{"hierarchy":{"lvl1":"Lab 1a: Linear regression","lvl3":"Exercise 3.2","lvl2":"Exercise 3: Visualizing coefficients"},"content":"Redraw the same plot but now using a large amount of regularization (e.g. alpha=1). What do you observe? Does this help you explain the performance difference between Ridge and Lasso in exercise 1.2?","type":"content","url":"/labs/lab-1a-linear-models-for-regression#exercise-3-2","position":25},{"hierarchy":{"lvl1":"Lab 1b: Linear classification"},"type":"lvl1","url":"/labs/lab-1b-linear-models-for-classification","position":0},{"hierarchy":{"lvl1":"Lab 1b: Linear classification"},"content":"The \n\nFashion-MNIST dataset contains 70,000 images of Zalando fashion products, classified into 10 types of clothing, each represented by 28 by 28 pixel values. We’s see how well we can classify these with linear models. Let’s start with looking at our data:\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nfrom matplotlib import cm\n\n# Hide convergence warning for now\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n\n\n\n# Download FMINST data. Takes a while the first time.\nfmnist = oml.datasets.get_dataset(40996)\nX, y, _, _ = fmnist.get_data(target=fmnist.default_target_attribute); \nfmnist_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n                  6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n\n\n\n# Take some random examples, reshape to a 32x32 image and plot\nfrom random import randint\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor i in range(5):\n    n = randint(0,70000)\n    axes[i].imshow(X.values[n].reshape(28, 28), cmap=plt.cm.gray_r)\n    axes[i].set_xlabel((fmnist_classes[int(y.values[n])]))\n    axes[i].set_xticks(()), axes[i].set_yticks(())\nplt.show();\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification","position":1},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 1: A quick benchmark"},"type":"lvl2","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-a-quick-benchmark","position":2},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 1: A quick benchmark"},"content":"First, we’ll try the default \n\nLogistic Regression and \n\nLinear SVMs. Click the links to read the documentation. We’ll also compare it to \n\nk-Nearest Neighbors as a point of reference. To see whether our models are overfitting, we also evaluate the training set error. This can be done using \n\ncross_validate instead of  \n\ncross_val_scores.\n\nFor now we are just interested in a quick approximation, so we don’t use the full dataset for our experiments. Instead, we use 10% of our samples:\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Take a 10% stratified subsample to speed up experimentation\nXs, _, ys, _ = train_test_split(X,y, stratify=y, train_size=0.1)\n\n\n\nWith this small sample of our data we can now train and evaluate the three classifiers.\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-a-quick-benchmark","position":3},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 1.1","lvl2":"Exercise 1: A quick benchmark"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-1","position":4},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 1.1","lvl2":"Exercise 1: A quick benchmark"},"content":"Implement a function below which evaluates each classifier passed into it on the given data, and then returns both the train and test scores of each as a list. You are allowed to import additional functions from whichever module you like, but you should be able to complete the function with \n\ncross_validate function and standard Python built-ins. Below the function you will find example output.\n\ndef evaluate_learners(classifiers, X, y):\n    \"\"\" Evaluate each classifier in 'classifiers' with cross-validation on the provided (X, y) data. \n    \n    Given a list of scikit-learn classifiers [Classifier1, Classifier2, ..., ClassifierN] return two lists:\n     - a list with the scores obtained on the training samples for each classifier,\n     - a list with the test scores obtained on the test samples for each classifier.\n     The order of scores should match the order in which the classifiers were originally provided. E.g.:     \n     [Classifier1 train score, ..., ClassifierN train score], [Classifier1 test score, ..., ClassifierN test score]\n    \"\"\"\n    pass\n\n# # Example output:\n# train_scores, test_scores = ([[0.92 , 0.924, 0.916, 0.917, 0.921],  # Classifier 1 train score for each of 5 folds.\n#                               [0.963, 0.962, 0.953, 0.912, 0.934],  # Classifier 2 train score for each of 5 folds.\n#                               [0.867, 0.868, 0.865, 0.866, 0.866]], # Classifier 3 train score for each of 5 folds.\n#                              [[0.801, 0.811, 0.806, 0.826, 0.804],  # Classifier 1 test score for each of 5 folds.\n#                               [0.766, 0.756, 0.773, 0.756, 0.741],  # Classifier 2 test score for each of 5 folds.\n#                               [0.804, 0.814, 0.806, 0.821, 0.806]]) # Classifier 3 test score for each of 5 folds.\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-1","position":5},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 1.2","lvl2":"Exercise 1: A quick benchmark"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-2","position":6},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 1.2","lvl2":"Exercise 1: A quick benchmark"},"content":"Call the function you created with a Logistic Regression, Linear SVM, and k-Nearest Neighbors Classifier.\nStore the return values in the variables train_scores and test_scores. Then, run the code given below to produce a plot visualizing the scores.\n\n# Dummy code. Replace with the actual classifiers and scores\nclassifiers = [LogisticRegression()]\ntrain_scores, test_scores = [[0.6,0.7,0.8]], [[0.5,0.6,0.7]]\n\n\n\n# Plot a bar chart of the train and test scores of all the classifiers, including the variance as error bars\nfig, ax = plt.subplots()\nwidth=0.3\nax.barh(np.arange(len(train_scores)), np.mean(test_scores, axis=1), width,\n        yerr= np.std(test_scores, axis=1), color='green', label='test')\nax.barh(np.arange(len(train_scores))-width, np.mean(train_scores, axis=1), width,\n        yerr= np.std(train_scores, axis=1), color='red', label='train')\nfor i, te, tr in zip(np.arange(len(train_scores)),test_scores,train_scores):\n    ax.text(0, i, \"{:.4f} +- {:.4f}\".format(np.mean(te),np.std(te)), color='white', va='center')\n    ax.text(0, i-width, \"{:.4f} +- {:.4f}\".format(np.mean(tr),np.std(tr)), color='white', va='center')\nax.set(yticks=np.arange(len(train_scores))-width/2, yticklabels=[c.__class__.__name__ for c in classifiers])\nax.set_xlabel('Accuracy')\nax.legend(bbox_to_anchor=(1.05, 1), loc=2)\n\nplt.show()\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-2","position":7},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 1.3","lvl2":"Exercise 1: A quick benchmark"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-3","position":8},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 1.3","lvl2":"Exercise 1: A quick benchmark"},"content":"Interpret the plot. Which is the best classifier? Are any of the models overfitting? If so, what can we do to solve this? Is there a lot of variance in the results?\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-1-3","position":9},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 2: Regularization"},"type":"lvl2","url":"/labs/lab-1b-linear-models-for-classification#exercise-2-regularization","position":10},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 2: Regularization"},"content":"We will now tune these algorithm’s main regularization hyperparameter: the misclassification cost in SVMs (C), the regularization parameter in logistic regression (C), and the number of neighbors (n_neighbors) in kNN. We expect the optimum for the C parameters to lie in [10^{-12},10^{12}] and for n_neighbors between 1 and 50. C should be varied on a log scale (i.e. [0.01, 0.1, 1, 10, 100]) and k should be varied uniformly (i.e. [1,2,3,4]).\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-2-regularization","position":11},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 2.1","lvl2":"Exercise 2: Regularization"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-2-1","position":12},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 2.1","lvl2":"Exercise 2: Regularization"},"content":"Vary the regularization parameters in the range given above and, for each classifier, create a line plot that plots both the training and test score for every value of the regularization hyperparameter. Hence, you should produce 3 plots, one for each classifier. Use the default 5-fold cross validation for all scores, but only plot the means.\n\nHints:\n\nThink about the time complexity of these models. Trying too many hyperparameter values may take too much time.\n\nYou can make use of numpy’s \n\nlogspace, \n\ngeomspace, and \n\nlinspace functions.\n\nYou can use matplotlib’s default \n\nplot function to plot the train and test scores.\n\nYou can manually loop over the hyperparameter ranges, or you can already check out scikit-learn’s \n\nGridSearchCV function to save some programming. We’ll see it again later in the course.\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-2-1","position":13},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl4":"Solution","lvl3":"Exercise 2.1","lvl2":"Exercise 2: Regularization"},"type":"lvl4","url":"/labs/lab-1b-linear-models-for-classification#solution","position":14},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl4":"Solution","lvl3":"Exercise 2.1","lvl2":"Exercise 2: Regularization"},"content":"\n\n# Generic plot for 1D grid search\n# grid_search: the result of the GridSearchCV\n# param_name: the name of the parameter that is being varied\ndef plot_tuning(grid_search, param_name, ax):\n    ax.plot(grid_search.param_grid[param_name], grid_search.cv_results_['mean_test_score'], marker = '.', label = 'Test score')\n    ax.plot(grid_search.param_grid[param_name], grid_search.cv_results_['mean_train_score'], marker = '.', label = 'Train score')\n    ax.set_ylabel('score (ACC)')\n    ax.set_xlabel(param_name)\n    ax.legend()\n    ax.set_xscale('log')\n    ax.set_title(grid_search.best_estimator_.__class__.__name__)\n    bp, bs = grid_search.best_params_[param_name], grid_search.best_score_\n    ax.text(bp,bs,\"  C:{:.2E}, ACC:{:.4f}\".format(bp,bs))\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#solution","position":15},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 2.2","lvl2":"Exercise 2: Regularization"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-2-2","position":16},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 2.2","lvl2":"Exercise 2: Regularization"},"content":"Interpret the plots. When are the methods underfitting? When are they overfitting? How sensitive are they to the regularization hyperparameter?\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-2-2","position":17},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 3: Interpreting misclassifications"},"type":"lvl2","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-interpreting-misclassifications","position":18},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 3: Interpreting misclassifications"},"content":"Chances are that your models are not yet perfect. It is important to understand what kind of errors it still makes. Let’s take a closer look at which instances are misclassified and which classes are often confused.\nTrain the logistic regression model with C=1e-7. Train the model on a training set, and make predictions for a test set (both sets should be  sampled from our 10% subsample).\n\n# Create a stratified train-test split on a sample\nX_train, X_test, y_train, y_test = train_test_split(Xs,ys, stratify=ys, random_state=0)\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-interpreting-misclassifications","position":19},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 3.1","lvl2":"Exercise 3: Interpreting misclassifications"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-1","position":20},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 3.1","lvl2":"Exercise 3: Interpreting misclassifications"},"content":"Train the classifier as described above, obtain the predictions y_pred on the test set, and identify all the misclassified samples misclassified_samples. Then, run the visualization code below to study the misclassifications\n\n# Implement the code to obtain the actual predictions on the test set\ny_pred = list(y_test) # dummy values, replace y_test with the actual predictions\n\n# Implement the code to obtain the indices of the misclassified samples\n# Example output:\n# misclassified_samples = [  11,   12,   14,   23,   30,   34,   39,   46,   50,   52,   55]\nmisclassified_samples = [0,1,2,3,4] # dummy values\n\n\n\n# Visualize the (first five) misclassifications, together with the predicted and actual class\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor nr, i in enumerate(misclassified_samples[:5]):\n    axes[nr].imshow(X_test.values[i].reshape(28, 28), cmap=plt.cm.gray_r)\n    axes[nr].set_xlabel(\"Predicted: %s,\\n Actual : %s\" % (fmnist_classes[int(y_pred[i])],fmnist_classes[int(y_test.values[i])]))\n    axes[nr].set_xticks(()), axes[nr].set_yticks(())\n\nplt.show();\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-1","position":21},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 3.2","lvl2":"Exercise 3: Interpreting misclassifications"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-2","position":22},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 3.2","lvl2":"Exercise 3: Interpreting misclassifications"},"content":"Interpret the results. Are these misclassifications to be expected?\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-2","position":23},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 3.3.","lvl2":"Exercise 3: Interpreting misclassifications"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-3","position":24},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 3.3.","lvl2":"Exercise 3: Interpreting misclassifications"},"content":"Run the code below on your results to draw the complete confusion matrix and get more insight on the systematic misclassifications\nof your model. A confusion matrix shows the amount of examples in for each pair of true and predicted classes. Interpret the results.\nDoes your model produce certain types of error more often than other types?\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nfig, ax = plt.subplots()\nim = ax.imshow(cm)\nax.set_xticks(np.arange(10)), ax.set_yticks(np.arange(10))\nax.set_xticklabels(list(fmnist_classes.values()), rotation=45, ha=\"right\")\nax.set_yticklabels(list(fmnist_classes.values()))\nax.set_ylabel('True')\nax.set_xlabel('Predicted')\nfor i in range(100):\n    ax.text(int(i/10),i%10,cm[i%10,int(i/10)], ha=\"center\", va=\"center\", color=\"w\")\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-3-3","position":25},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 4: Interpreting model parameters"},"type":"lvl2","url":"/labs/lab-1b-linear-models-for-classification#exercise-4-interpreting-model-parameters","position":26},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl2":"Exercise 4: Interpreting model parameters"},"content":"Finally, we’ll take a closer look at the model parameters, i.e. the coefficients of our linear models. Since we are dealing with 28x28 pixel images, we have to learn 784 coefficients. What do these coefficients mean? We’ll start by plotting them as 28x28 pixel images.\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-4-interpreting-model-parameters","position":27},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 4.1","lvl2":"Exercise 4: Interpreting model parameters"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-4-1","position":28},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 4.1","lvl2":"Exercise 4: Interpreting model parameters"},"content":"Train a Logistic Regression model and a Linear SVM using their tuned hyperparameters from exercise 2.\nWhen in doubt, use C=1e-7 for LogReg and C=1e-8 for the SVM.\nPass the trained model to the provided plotting function. Interpret the results in detail.\nWhy do you get multiple plots per model? What do the features represent in your data.\nDoes it seems like the models pay attention to the right features?\nDo you models seem to ignore certain features? Do you observe differences in quality between the different classes? Do you observe any differences between the models?\n\n# Plots the coefficients of the given model as 28x28 heatmaps. \n# The `name` attribute is optional, it is simply a title for the produced figure\ndef plot_coefficients(model, name=None):\n    fig, axes = plt.subplots(1,10,figsize=(20,2))\n    fig.suptitle(name if name else model.__class__.__name__)\n    for i, ax in enumerate(axes):\n        m = ax.imshow(model.coef_[i].reshape(28,28))\n        ax.set_xlabel(fmnist_classes[i])\n        ax.set_xticks(()), ax.set_yticks(())\n    fig.colorbar(m, ax=axes.ravel().tolist())\n\n\n\n","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-4-1","position":29},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 4.2","lvl2":"Exercise 4: Interpreting model parameters"},"type":"lvl3","url":"/labs/lab-1b-linear-models-for-classification#exercise-4-2","position":30},{"hierarchy":{"lvl1":"Lab 1b: Linear classification","lvl3":"Exercise 4.2","lvl2":"Exercise 4: Interpreting model parameters"},"content":"Repeat the previous exercise, but now only with logistic regression. In addition to a tuned version, also add a model that overfits a lot and one that underfits a lot. Interpret and explain the results.","type":"content","url":"/labs/lab-1b-linear-models-for-classification#exercise-4-2","position":31},{"hierarchy":{"lvl1":"Lab 2b: Model selection"},"type":"lvl1","url":"/labs/lab-2-model-evaluation","position":0},{"hierarchy":{"lvl1":"Lab 2b: Model selection"},"content":"","type":"content","url":"/labs/lab-2-model-evaluation","position":1},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Dark matter"},"type":"lvl2","url":"/labs/lab-2-model-evaluation#dark-matter","position":2},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Dark matter"},"content":"We’ll use the MAGIC telescope dataset (\n\nhttp://​www​.openml​.org​/d​/1120). The task is to classifying gamma rays, which consist of high-energy particles. When they hit our atmosphere, they produce chain reactions of other particles called ‘showers’. However, similar showers are also produced by other particles (hadrons). We want to be able to detect which ones originate from gamma rays and which ones come from background radiation. To do this, the observed shower patterns are observed and converted into 10 numeric features. You need to detect whether these are gamma rays or background radiation. This is a key aspect of research into dark matter, which is believed to generate such gamma rays. If we can detect where they occur, we can build a map of the origins of gamma radiation, and locate where dark matter may occur in the observed universe. However, we’ll first need to accurately detect these gamma rays first.\n\nA quick visualization of the features is shown below. Note that this is not a time series, we just plot the instances in the order they occur in the dataset. The first 12500 or so are examples of signal (gamma), the final 6700 or so are background (hadrons).\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nfrom matplotlib import cm\n\n\n\n# Download MAGIC Telescope data from OpenML. You can repeat this analysis with any other OpenML classification dataset.\nmagic = oml.datasets.get_dataset(1120)\nX, y, _, _ = magic.get_data(target=magic.default_target_attribute, dataset_format='array'); \nattribute_names = [f.name for i,f in magic.features.items()][:-1][1:]\n\n\n\n# Quick visualization of the features (top) and the target (bottom)\nmagic_df = pd.DataFrame(X, columns=attribute_names)\nmagic_df.plot(figsize=(12,6))\n# Also plot the target: 1 = background, 0 = gamma\npd.DataFrame(y).plot(figsize=(12,1));\n\n\n\n\n\n","type":"content","url":"/labs/lab-2-model-evaluation#dark-matter","position":3},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 1: Metrics"},"type":"lvl2","url":"/labs/lab-2-model-evaluation#exercise-1-metrics","position":4},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 1: Metrics"},"content":"Train and evaluate an SVM with RBF kernel (default hyperparameters) using a standard 25% holdout. Report the accuracy, precision, recall, F1 score, and area under the ROC curve (AUC).\n\nAnswer the following questions:\n\nHow many of the detected gamma rays are actually real gamma rays?\n\nHow many of all the gamma rays are we detecting?\n\nHow many false positives and false negatives occur?\n\n","type":"content","url":"/labs/lab-2-model-evaluation#exercise-1-metrics","position":5},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 2: Preprocessing"},"type":"lvl2","url":"/labs/lab-2-model-evaluation#exercise-2-preprocessing","position":6},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 2: Preprocessing"},"content":"SVMs require scaling to perform well. For now, use the following code to scale the data (we’ll get back to this in the lab about preprocessing and pipelines). Repeat question 2 on the scaled data. Have the results improved?\n\nfrom sklearn.preprocessing import StandardScaler\n# Important here is to fit the scaler on the training data alone\n# Then, use it to scale both the training set and test set\n# This assumes that you named your training set X_train. Adapt if needed.\nscaler = StandardScaler().fit(X_train)\nXs_train = scaler.transform(X_train)\nXs_test = scaler.transform(X_test)\n\n\n\n","type":"content","url":"/labs/lab-2-model-evaluation#exercise-2-preprocessing","position":7},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 3: Hyperparameter optimization"},"type":"lvl2","url":"/labs/lab-2-model-evaluation#exercise-3-hyperparameter-optimization","position":8},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 3: Hyperparameter optimization"},"content":"Use 50 iterations of random search to tune the C and gamma hyperparameters on the scaled training data. Vary both on a log scale (e.g. from 2^-12 to 2^12). Optimize on AUC and use 3 cross-validation (CV) folds for the inner CV to estimate performance. For the outer loop, just use the train-test split you used before (hence, no nested CV). Report the best hyperparameters and the corresponding AUC score. Is it better than the default? Finally, use them to evaluate the model on the held-out test set, for all 5 metrics we used before.\n\nExtra challenge: plot the samples used by the random search (C vs gamma)\n\nNote: The reason we don’t use a nested CV just yet is because we would need to rebuild the scaled training and test set multiple times. This is tedious, unless we use pipelines, which we’ll cover in a future lab.\n\n","type":"content","url":"/labs/lab-2-model-evaluation#exercise-3-hyperparameter-optimization","position":9},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 4: Threshold calibration"},"type":"lvl2","url":"/labs/lab-2-model-evaluation#exercise-4-threshold-calibration","position":10},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 4: Threshold calibration"},"content":"First, plot the Precision-Recall curve for the SVM using the default parameters on the scaled data. Then, calibrate the threshold to find a solution that yields better recall without sacrificing too much precision.\n\n","type":"content","url":"/labs/lab-2-model-evaluation#exercise-4-threshold-calibration","position":11},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 5: Cost function"},"type":"lvl2","url":"/labs/lab-2-model-evaluation#exercise-5-cost-function","position":12},{"hierarchy":{"lvl1":"Lab 2b: Model selection","lvl2":"Exercise 5: Cost function"},"content":"Assume that a false negative is twice as bad (costly) than a false positive. I.e. we would rather waste time checking gamma ray sources that are not real, than missing an interesting gamma ray source. Use ROC analysis to find the optimal threshold under this assumption.\n\nFinally, let the model make predictions using the optimal threshold and report all 5 scores. Is recall better now? Did we lose a lot of precision?","type":"content","url":"/labs/lab-2-model-evaluation#exercise-5-cost-function","position":13},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn"},"type":"lvl1","url":"/labs/lab-2-tutorial","position":0},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn"},"content":"# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nfrom matplotlib import cm\n\n# We can ignore ConvergenceWarnings for illustration purposes\nimport warnings\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial","position":1},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Evaluation procedures"},"type":"lvl2","url":"/labs/lab-2-tutorial#evaluation-procedures","position":2},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Evaluation procedures"},"content":"","type":"content","url":"/labs/lab-2-tutorial#evaluation-procedures","position":3},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Holdout","lvl2":"Evaluation procedures"},"type":"lvl3","url":"/labs/lab-2-tutorial#holdout","position":4},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Holdout","lvl2":"Evaluation procedures"},"content":"The simplest procedure is \n\ntrain_test_split, which splits arrays or matrices into random train and test subsets.\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# create a synthetic dataset\nX, y = make_blobs(centers=2, random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# Instantiate a model and fit it to the training set\nmodel = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(model.score(X_test, y_test)))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#holdout","position":5},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Cross-validation","lvl2":"Evaluation procedures"},"type":"lvl3","url":"/labs/lab-2-tutorial#cross-validation","position":6},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Cross-validation","lvl2":"Evaluation procedures"},"content":"cross_val_score\n\ncv parameter defines the kind of cross-validation splits, default is 5-fold CV\n\nscoring defines the scoring metric. Also see below.\n\nReturns list of all scores. Models are built internally, but not returned\n\ncross_validate\n\nSimilar, but also returns the fit and test times, and allows multiple scoring metrics.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\niris = load_iris()\nlogreg = LogisticRegression()\n\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nprint(\"Variance in cross-validation score: {:.4f}\".format(np.var(scores)))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#cross-validation","position":7},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Custom CV splits","lvl2":"Evaluation procedures"},"type":"lvl3","url":"/labs/lab-2-tutorial#custom-cv-splits","position":8},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Custom CV splits","lvl2":"Evaluation procedures"},"content":"You can build folds manually with \n\nKFold or \n\nStratifiedKFold\n\nrandomizable (shuffle parameter)\n\nLeaveOneOut does leave-one-out cross-validation\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nkfold = KFold(n_splits=5)\nprint(\"Cross-validation scores KFold(n_splits=5):\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nskfold = StratifiedKFold(n_splits=5, shuffle=True)\nprint(\"Cross-validation scores StratifiedKFold(n_splits=5, shuffle=True):\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=skfold)))\n\n\n\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#custom-cv-splits","position":9},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Shuffle-split","lvl2":"Evaluation procedures"},"type":"lvl3","url":"/labs/lab-2-tutorial#shuffle-split","position":10},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Shuffle-split","lvl2":"Evaluation procedures"},"content":"These shuffle the data before splitting it.\n\nShuffleSplit and StratifiedShuffleSplit (recommended for classification)\n\ntrain_size and test_size can be absolute numbers or a percentage of the total dataset\n\nfrom sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\nshuffle_split = StratifiedShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#shuffle-split","position":11},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Grouped cross-validation","lvl2":"Evaluation procedures"},"type":"lvl3","url":"/labs/lab-2-tutorial#grouped-cross-validation","position":12},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Grouped cross-validation","lvl2":"Evaluation procedures"},"content":"Add an array with group membership to cross_val_scores\n\nUse GroupKFold with the number of groups as CV procedure\n\nfrom sklearn.model_selection import GroupKFold\n# create synthetic dataset\nX, y = make_blobs(n_samples=12, random_state=0)\n# the first three samples belong to the same group, etc.\ngroups = [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\nscores = cross_val_score(logreg, X, y, groups=groups, cv=GroupKFold(n_splits=4))\nprint(\"Cross-validation scores :\\n{}\".format(scores))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#grouped-cross-validation","position":13},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Evaluation Metrics"},"type":"lvl2","url":"/labs/lab-2-tutorial#evaluation-metrics","position":14},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Evaluation Metrics"},"content":"\n\n","type":"content","url":"/labs/lab-2-tutorial#evaluation-metrics","position":15},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Binary classification","lvl2":"Evaluation Metrics"},"type":"lvl3","url":"/labs/lab-2-tutorial#binary-classification","position":16},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Binary classification","lvl2":"Evaluation Metrics"},"content":"confusion_matrix returns a matrix counting how many test examples are predicted correctly or ‘confused’ with other metrics.\n\nsklearn.metrics contains implementations many of the metrics discussed in class\n\nThey are all implemented so that ‘higher is better’.\n\naccuracy_score computes accuracy explictly\n\nclassification​_report returns a table of binary measures, per class, and aggregated according to different aggregation functions.\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint(\"confusion_matrix(y_test, y_pred): \\n\", confusion_matrix(y_test, y_pred))\nprint(\"accuracy_score(y_test, y_pred): \", accuracy_score(y_test, y_pred))\nprint(\"model.score(X_test, y_test): \", lr.score(X_test, y_test))\n\n\n\nplt.rcParams['figure.dpi'] = 100 \nprint(classification_report(y_test, lr.predict(X_test)))\n\n\n\nYou can explictly define the averaging function for class-level metrics\n\npred = lr.predict(X_test)\nprint(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\nprint(\"Weighted average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"weighted\")))\nprint(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#binary-classification","position":17},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Probabilistic predictions","lvl2":"Evaluation Metrics"},"type":"lvl3","url":"/labs/lab-2-tutorial#probabilistic-predictions","position":18},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Probabilistic predictions","lvl2":"Evaluation Metrics"},"content":"To retrieve the uncertainty in the prediction, scikit-learn offers 2 functions. Often, both are available for every learner, but not always.\n\ndecision_function: returns floating point (-Inf,Inf) value for each prediction\n\npredict_proba: returns probability [0,1] for each prediction\n\nYou can also use these to compute any metric with non-standard thresholds\n\nprint(\"Threshold -0.8\")\ny_pred_lower_threshold = lr.decision_function(X_test) > -.8\nprint(classification_report(y_test, y_pred_lower_threshold))  \n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#probabilistic-predictions","position":19},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Uncertainty in multi-class classification","lvl2":"Evaluation Metrics"},"type":"lvl3","url":"/labs/lab-2-tutorial#uncertainty-in-multi-class-classification","position":20},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Uncertainty in multi-class classification","lvl2":"Evaluation Metrics"},"content":"decision_function and predict_proba also work in the multiclass setting\n\nalways have shape (n_samples, n_classes)\n\nExample on the Iris dataset, which has 3 classes:\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX_train2, X_test2, y_train2, y_test2 = train_test_split(\n    iris.data, iris.target, random_state=42)\n\nlr2 = LogisticRegression()\nlr2 = lr2.fit(X_train2, y_train2)\n\nprint(\"Decision function:\\n{}\".format(lr2.decision_function(X_test2)[:6, :]))\n# show the first few entries of predict_proba\nprint(\"Predicted probabilities:\\n{}\".format(lr2.predict_proba(X_test2)[:6]))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#uncertainty-in-multi-class-classification","position":21},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Precision-Recall and ROC curves","lvl2":"Evaluation Metrics"},"type":"lvl3","url":"/labs/lab-2-tutorial#precision-recall-and-roc-curves","position":22},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Precision-Recall and ROC curves","lvl2":"Evaluation Metrics"},"content":"precision​_recall​_curve returns all precision and recall values for all possible thresholds\n\nroc_curve does the same for TPR and FPR.\n\nThe average precision score is returned by the average_precision_score measure\n\nThe area under the ROC curve is returned by the roc_auc_score measure\n\nDon’t use auc (this uses a less accurate trapezoidal rule)\n\nRequire a decision function or predict_proba.\n\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(\n    y_test, lr.decision_function(X_test))\n\n\n\nfrom sklearn.metrics import average_precision_score\nap_pp = average_precision_score(y_test, lr.predict_proba(X_test)[:, 1])\nap_df = average_precision_score(y_test, lr.decision_function(X_test))\nprint(\"Average precision of logreg: {:.3f}\".format(ap_df))\n\n\n\nfrom sklearn.metrics import roc_auc_score\nrf_auc = roc_auc_score(y_test, lr.predict_proba(X_test)[:, 1])\nsvc_auc = roc_auc_score(y_test, lr.decision_function(X_test))\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#precision-recall-and-roc-curves","position":23},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Multi-class prediction","lvl2":"Evaluation Metrics"},"type":"lvl3","url":"/labs/lab-2-tutorial#multi-class-prediction","position":24},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Multi-class prediction","lvl2":"Evaluation Metrics"},"content":"Build C models, one for every class vs all others\n\nUse micro-, macro-, or weighted averaging\n\nprint(\"Micro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"micro\")))\nprint(\"Weighted average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"weighted\")))\nprint(\"Macro average f1 score: {:.3f}\".format(f1_score(y_test, pred, average=\"macro\")))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#multi-class-prediction","position":25},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Using evaluation metrics in model selection"},"type":"lvl2","url":"/labs/lab-2-tutorial#using-evaluation-metrics-in-model-selection","position":26},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Using evaluation metrics in model selection"},"content":"You typically want to use AUC or other relevant measures in cross_val_score and GridSearchCV instead of the default accuracy.\n\nscikit-learn makes this easy through the scoring argument\n\nBut, you need to need to look the \n\nmapping between the scorer and the metric\n\n\n\nOr simply look up like this:\n\nfrom sklearn.metrics import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\n\n\n\nCross-validation with AUC\n\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn .svm import SVC\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\n\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n      cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9, \n                                     scoring=\"accuracy\")\nprint(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\nroc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                           scoring=\"roc_auc\")\nprint(\"AUC scoring: {}\".format(roc_auc))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#using-evaluation-metrics-in-model-selection","position":27},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Hyperparameter tuning"},"type":"lvl2","url":"/labs/lab-2-tutorial#hyperparameter-tuning","position":28},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl2":"Hyperparameter tuning"},"content":"Now that we know how to evaluate models, we can improve them by tuning their hyperparameters\n\n","type":"content","url":"/labs/lab-2-tutorial#hyperparameter-tuning","position":29},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Grid search","lvl2":"Hyperparameter tuning"},"type":"lvl3","url":"/labs/lab-2-tutorial#grid-search","position":30},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Grid search","lvl2":"Hyperparameter tuning"},"content":"Create a parameter grid as a dictionary\n\nKeys are parameter names\n\nValues are lists of hyperparameter values\n\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nprint(\"Parameter grid:\\n{}\".format(param_grid))\n\n\n\nGridSearchCV: like a classifier that uses CV to automatically optimize its hyperparameters internally\n\nInput: (untrained) model, parameter grid, CV procedure\n\nOutput: optimized model on given training data\n\nShould only have access to training data\n\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.svm import SVC\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\nX_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, random_state=0)\ngrid_search.fit(X_train, y_train)\n\n\n\nThe optimized test score and hyperparameters can easily be retrieved:\n\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n\n\n\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n\n\n\nprint(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n\n\n\nWhen hyperparameters depend on other parameters, we can use lists of dictionaries to define the hyperparameter space\n\nparam_grid = [{'kernel': ['rbf'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n              {'kernel': ['linear'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nprint(\"List of grids:\\n{}\".format(param_grid))\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#grid-search","position":31},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Nested cross-validation","lvl2":"Hyperparameter tuning"},"type":"lvl3","url":"/labs/lab-2-tutorial#nested-cross-validation","position":32},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Nested cross-validation","lvl2":"Hyperparameter tuning"},"content":"Nested cross-validation:\n\nOuter loop: split data in training and test sets\n\nInner loop: run grid search, splitting the training data into train and validation sets\n\nResult is a just a list of scores\n\nThere will be multiple optimized models and hyperparameter settings (not returned)\n\nTo apply on future data, we need to train GridSearchCV on all data again\n\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())\n\n\n\n","type":"content","url":"/labs/lab-2-tutorial#nested-cross-validation","position":33},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Parallelizing cross-validation and grid-search","lvl2":"Hyperparameter tuning"},"type":"lvl3","url":"/labs/lab-2-tutorial#parallelizing-cross-validation-and-grid-search","position":34},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Parallelizing cross-validation and grid-search","lvl2":"Hyperparameter tuning"},"content":"On a practical note, it is easy to parallellize CV and grid search\n\ncross_val_score and GridSearchCV have a n_jobs parameter defining the number of cores it can use.\n\nset it to n_jobs=-1 to use all available cores.\n\n","type":"content","url":"/labs/lab-2-tutorial#parallelizing-cross-validation-and-grid-search","position":35},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Random Search","lvl2":"Hyperparameter tuning"},"type":"lvl3","url":"/labs/lab-2-tutorial#random-search","position":36},{"hierarchy":{"lvl1":"Lab 2: Model Selection in scikit-learn","lvl3":"Random Search","lvl2":"Hyperparameter tuning"},"content":"RandomizedSearchCV works like GridSearchCV\n\nHas n_iter parameter for the number of iterations\n\nSearch grid can use distributions instead of fixed lists\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon\n\nparam_grid = {'C': expon(scale=100), \n              'gamma': expon(scale=.1)}\nrandom_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,\n                                   n_iter=20)\nX_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, random_state=0)\nrandom_search.fit(X_train, y_train)\n\n","type":"content","url":"/labs/lab-2-tutorial#random-search","position":37},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn"},"type":"lvl1","url":"/labs/lab-3-tutorial","position":0},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn"},"content":"# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nimport seaborn as sns\n\n\n\n","type":"content","url":"/labs/lab-3-tutorial","position":1},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl2":"Applying data transformations (recap from lecture)"},"type":"lvl2","url":"/labs/lab-3-tutorial#applying-data-transformations-recap-from-lecture","position":2},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl2":"Applying data transformations (recap from lecture)"},"content":"Data transformations should always follow a fit-predict paradigm\n\nFit the transformer on the training data only\n\nE.g. for a standard scaler: record the mean and standard deviation\n\nTransform (e.g. scale) the training data, then train the learning model\n\nTransform (e.g. scale) the test data, then evaluate the model\n\nOnly scale the input features (X), not the targets (y)!\n\nIf you fit and transform the whole dataset before splitting, you get data leakage\n\nYou have looked at the test data before training the model\n\nModel evaluations will be misleading\n\nIf you fit and transform the training and test data separately, you distort the data\n\nE.g. training and test points are scaled differently\n\n","type":"content","url":"/labs/lab-3-tutorial#applying-data-transformations-recap-from-lecture","position":3},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"In practice (scikit-learn)","lvl2":"Applying data transformations (recap from lecture)"},"type":"lvl3","url":"/labs/lab-3-tutorial#in-practice-scikit-learn","position":4},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"In practice (scikit-learn)","lvl2":"Applying data transformations (recap from lecture)"},"content":"# choose scaling method and fit on training data\nscaler = StandardScaler()\nscaler.fit(X_train)\n\n# transform training and test data\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nAlternative:# calling fit and transform in sequence\nX_train_scaled = scaler.fit(X_train).transform(X_train)\n# same result, but more efficient computation\nX_train_scaled = scaler.fit_transform(X_train)\n\n","type":"content","url":"/labs/lab-3-tutorial#in-practice-scikit-learn","position":5},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Scikit-learn processing pipelines","lvl2":"Applying data transformations (recap from lecture)"},"type":"lvl3","url":"/labs/lab-3-tutorial#scikit-learn-processing-pipelines","position":6},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Scikit-learn processing pipelines","lvl2":"Applying data transformations (recap from lecture)"},"content":"Scikit-learn pipelines have a fit, predict, and score method\n\nInternally applies transformations correctly\n\n","type":"content","url":"/labs/lab-3-tutorial#scikit-learn-processing-pipelines","position":7},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl2":"Building Pipelines"},"type":"lvl2","url":"/labs/lab-3-tutorial#building-pipelines","position":8},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl2":"Building Pipelines"},"content":"In scikit-learn, a pipeline combines multiple processing steps in a single estimator\n\nAll but the last step should be transformer (have a transform method)\n\nThe last step can be a transformer too (e.g. Scaler+PCA)\n\nIt has a fit, predict, and score method, just like any other learning algorithm\n\nPipelines are built as a list of steps, which are (name, algorithm) tuples\n\nThe name can be anything you want, but can’t contain '__'\n\nWe use '__' to refer to the hyperparameters, e.g. svm__C\n\nLet’s build, train, and score a MinMaxScaler + LinearSVC pipeline:\n\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\npipe.fit(X_train, y_train).score(X_test, y_test)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\npipe.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\n\n\n\nNow with cross-validation:scores = cross_val_score(pipe, cancer.data, cancer.target)\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipe, cancer.data, cancer.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n\n\n\nWe can retrieve the trained SVM by querying the right step indicespipe.steps[1][1]\n\npipe.fit(X_train, y_train)\nprint(\"SVM component: {}\".format(pipe.steps[1][1]))\n\n\n\nOr we can use the named_steps dictionarypipe.named_steps['svm']\n\nprint(\"SVM component: {}\".format(pipe.named_steps['svm']))\n\n\n\nWhen you don’t need specific names for specific steps, you can use make_pipeline\n\nAssigns names to steps automaticallypipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\nprint(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n\nfrom sklearn.pipeline import make_pipeline\n# abbreviated syntax\npipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\nprint(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n\n\n\nVisualization of a pipeline fit and predict\n\n\n\n","type":"content","url":"/labs/lab-3-tutorial#building-pipelines","position":9},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Pipeline selection","lvl2":"Building Pipelines"},"type":"lvl3","url":"/labs/lab-3-tutorial#pipeline-selection","position":10},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Pipeline selection","lvl2":"Building Pipelines"},"content":"We can safely use pipelines in model selection (e.g. grid search)\n\nUse '__' to refer to the hyperparameters of a step, e.g. svm__C# Correct grid search (can have hyperparameters of any step)\nparam_grid = {'svm__C': [0.001, 0.01],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid).fit(X,y)\n# Best estimator is now the best pipeline\nbest_pipe = grid.best_estimator_\n\n# Tune pipeline and evaluate on held-out test set\ngrid = GridSearchCV(pipe, param_grid=param_grid).fit(X_train,y_train)\ngrid.score(X_test,y_test)\n\n\n","type":"content","url":"/labs/lab-3-tutorial#pipeline-selection","position":11},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Using Pipelines in Grid-searches","lvl2":"Building Pipelines"},"type":"lvl3","url":"/labs/lab-3-tutorial#using-pipelines-in-grid-searches","position":12},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Using Pipelines in Grid-searches","lvl2":"Building Pipelines"},"content":"We can use the pipeline as a single estimator in cross_val_score or GridSearchCV\n\nTo define a grid, refer to the hyperparameters of the steps\n\nStep svm, parameter C becomes svm__C\n\nparam_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n\n\nfrom sklearn import pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\npipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n\n\n\nWhen we request the best estimator of the grid search, we’ll get the best pipelinegrid.best_estimator_\n\nprint(\"Best estimator:\\n{}\".format(grid.best_estimator_))\n\n\n\nAnd we can drill down to individual components and their propertiesgrid.best_estimator_.named_steps[\"svm\"]\n\n# Get the SVM\nprint(\"SVM step:\\n{}\".format(\n      grid.best_estimator_.named_steps[\"svm\"]))\n\n\n\n# Get the SVM dual coefficients (support vector weights)\nprint(\"SVM support vector coefficients:\\n{}\".format(\n      grid.best_estimator_.named_steps[\"svm\"].dual_coef_))\n\n\n\n","type":"content","url":"/labs/lab-3-tutorial#using-pipelines-in-grid-searches","position":13},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Grid-searching preprocessing steps and model parameters","lvl2":"Building Pipelines"},"type":"lvl3","url":"/labs/lab-3-tutorial#grid-searching-preprocessing-steps-and-model-parameters","position":14},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"Grid-searching preprocessing steps and model parameters","lvl2":"Building Pipelines"},"content":"We can use grid search to optimize the hyperparameters of our preprocessing steps and learning algorithms at the same time\n\nConsider the following pipeline:\n\nStandardScaler, without hyperparameters\n\nPolynomialFeatures, with the max. degree of polynomials\n\nRidge regression, with L2 regularization parameter alpha\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\n\nhousing = fetch_california_housing()\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,\n                                                    random_state=0)\nfrom sklearn.preprocessing import PolynomialFeatures\npipe = pipeline.make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    Ridge())\n\n\n\nWe don’t know the optimal polynomial degree or alpha value, so we use a grid search (or random search) to find the optimal valuesparam_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\ngrid.fit(X_train, y_train)\n\nparam_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n# Note: I had to use n_jobs=1. (n_jobs=-1 stalls on my machine)\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=1)\ngrid.fit(X_train, y_train);\n\n\n\nVisualing the R^2 results as a heatmap:\n\nimport matplotlib.pyplot as plt\n\nplt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n            vmin=0, cmap=\"viridis\")\nplt.xlabel(\"ridge__alpha\")\nplt.ylabel(\"polynomialfeatures__degree\")\nplt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\nplt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n           param_grid['polynomialfeatures__degree'])\n\nplt.colorbar();\n\n\n\n# Another example (different dataset)\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import fetch_openml\n\nboston = fetch_openml(name=\"boston\", as_frame=True)\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    random_state=0)\nfrom sklearn.preprocessing import PolynomialFeatures\npipe = make_pipeline(StandardScaler(),PolynomialFeatures(),Ridge())\nparam_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train);\n\n#plot\nfig, ax = plt.subplots(figsize=(6, 3))\nim = ax.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n            vmin=0, cmap=\"viridis\")\nax.set_xlabel(\"ridge__alpha\")\nax.set_ylabel(\"polynomialfeatures__degree\")\nax.set_xticks(range(len(param_grid['ridge__alpha'])))\nax.set_xticklabels(param_grid['ridge__alpha'])\nax.set_yticks(range(len(param_grid['polynomialfeatures__degree'])))\nax.set_yticklabels(param_grid['polynomialfeatures__degree'])\nplt.colorbar(im);\n\n\n\n\n\nHere, degree-2 polynomials help (but degree-3 ones don’t), and tuning the alpha parameter helps as well.\n\nNot using the polynomial features leads to suboptimal results (see the results for degree 1)\n\nprint(\"Best parameters: {}\".format(grid.best_params_))\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\n\n\n\n","type":"content","url":"/labs/lab-3-tutorial#grid-searching-preprocessing-steps-and-model-parameters","position":15},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"FeatureUnions","lvl2":"Building Pipelines"},"type":"lvl3","url":"/labs/lab-3-tutorial#featureunions","position":16},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"FeatureUnions","lvl2":"Building Pipelines"},"content":"Sometimes you want to apply multiple preprocessing techniques and use the combined produced features\n\nSimply appending the produced features is called a FeatureJoin\n\nExample: Apply both PCA and feature selection, and run an SVM on both\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\niris = load_iris()\n\nX, y = iris.data, iris.target\n\n# This dataset is way too high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\nprint(\"Combined space has\", X_features.shape[1], \"features\")\n\nsvm = SVC(kernel=\"linear\")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([(\"features\", combined_features), (\"svm\", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)\n\n\n\n","type":"content","url":"/labs/lab-3-tutorial#featureunions","position":17},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"ColumnTransformer","lvl2":"Building Pipelines"},"type":"lvl3","url":"/labs/lab-3-tutorial#columntransformer","position":18},{"hierarchy":{"lvl1":"Lab 4: Data engineering pipelines with scikit-learn","lvl3":"ColumnTransformer","lvl2":"Building Pipelines"},"content":"A pipeline applies a transformer on all columns\n\nIf your dataset has both numeric and categorical features, you often want to apply different techniques on each\n\nYou could manually split up the dataset, and then feature-join the processed features (tedious)\n\nColumnTransformer allows you to specify on which columns a preprocessor has to be run\n\nEither by specifying the feature names, indices, or a binary mask\n\nYou can include multiple transformers in a ColumnTransformer\n\nIn the end the results will be feature-joined\n\nHence, the order of the features will change!\nThe features of the last transformer will be at the end\n\nEach transformer can be a pipeline\n\nHandy if you need to apply multiple preprocessing steps on a set of features\n\nE.g. use a ColumnTransformer with one sub-pipeline for numerical features and one for categorical features.\n\nIn the end, the columntransformer can again be included as part of a pipeline\n\nE.g. to add a classfier and include the whole pipeline in a grid search\n\n# 2 sub-pipelines, one for numeric features, other for categorical ones\nnumeric_pipe = make_pipeline(SimpleImputer(),StandardScaler())\ncategorical_pipe = make_pipeline(SimpleImputer(),OneHotEncoder())\n\n# Using categorical pipe for features A,B,C, numeric pipe otherwise\npreprocessor = make_column_transformer((categorical_pipe,\n                                        [\"A\",\"B\",\"C\"]), \n                                        remainder=numeric_pipe)\n\n# Combine with learning algorithm in another pipeline\n\n``` python\npipe = make_pipeline(preprocessor, LinearSVC())\n\nCareful: ColumnTransformer concatenates features in orderpipe = make_column_transformer((StandardScaler(),numeric_features), \n                               (PCA(),numeric_features),\n                               (OneHotEncoder(),categorical_features))\n\nExample: Handle a dataset (Titanic) with both categorical an numeric features\n\nNumeric features: impute missing values and scale\n\nCategorical features: Impute missing values and apply one-hot-encoding\n\nFinally, run an SVM\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nnp.random.seed(0)\n\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n\n# Alternatively X and y can be obtained directly from the frame attribute:\n# X = titanic.frame.drop('survived', axis=1)\n# y = titanic.frame['survived']\n\n# We will train our classifier with the following features:\n# Numeric Features:\n# - age: float.\n# - fare: float.\n# Categorical Features:\n# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n# - sex: categories encoded as strings {'female', 'male'}.\n# - pclass: ordinal integers {1, 2, 3}.\n\n# We create the preprocessing pipelines for both numeric and categorical data.\nnumeric_features = ['age', 'fare']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['embarked', 'sex', 'pclass']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\n\n\n\nYou can again run optimize any of the hyperparameters (preprocessing-related ones included) in a grid search\n\nparam_grid = {\n    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n    'classifier__C': [0.1, 1.0, 10, 100],\n}\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\nprint((\"best logistic regression from grid search: %.3f\"\n       % grid_search.score(X_test, y_test)))\n\n","type":"content","url":"/labs/lab-3-tutorial#columntransformer","position":19},{"hierarchy":{"lvl1":"Lab 3: Ensembles"},"type":"lvl1","url":"/labs/lab-3a-ensembles","position":0},{"hierarchy":{"lvl1":"Lab 3: Ensembles"},"content":"","type":"content","url":"/labs/lab-3a-ensembles","position":1},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Using trees to detect trees"},"type":"lvl2","url":"/labs/lab-3a-ensembles#using-trees-to-detect-trees","position":2},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Using trees to detect trees"},"content":"We will be using tree-based ensemble methods on the \n\nCovertype dataset.\nIt contains about 100,000 observations of 7 types of trees (Spruce, Pine, Cottonwood, Aspen,...) described by 55 features describing elevation, distance to water, soil type, etc.\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nimport seaborn as sns\n\n\n\n# Download Covertype data. Takes a while the first time.\ncovertype = oml.datasets.get_dataset(180)\nX, y, _, _ = covertype.get_data(target=covertype.default_target_attribute, dataset_format='array'); \nclasses = covertype.retrieve_class_labels()\nfeatures = [f.name for i,f in covertype.features.items()][:-1]\n\n\n\nclasses\n\n\n\nfeatures[0:20]\n\n\n\nTo understand the data a bit better, we can use a scatter matrix. From this, it looks like elevation is a relevant feature.\nDouglas Fir and Aspen grow at low elevations, while only Krummholz pines survive at very high elevations.\n\n# Using seaborn to build the scatter matrix\n# only first 3 columns, first 1000 examples\nn_points = 1500\ndf = pd.DataFrame(X[:n_points,:3], columns=features[:3])\ndf['class'] = [classes[i] for i in y[:n_points]]\nsns.set(style=\"ticks\")\nsns.pairplot(df, hue=\"class\");\n\n\n\n","type":"content","url":"/labs/lab-3a-ensembles#using-trees-to-detect-trees","position":3},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 1: Random Forests"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-1-random-forests","position":4},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 1: Random Forests"},"content":"Implement a function evaluate_RF that measures the performance of a Random Forest Classifier, using trees\nof (max) depth 2,8,32,64, for any number of trees in the ensemble (n_estimators).\nFor the evaluation you should measure accuracy using 3-fold cross-validation.\nUse random_state=1 to ensure reproducibility. Finally, plot the results for at least 5 values of n_estimators ranging from 1 to 30. You can, of course, reuse code from earlier labs and assignments. Interpret the results.\nYou can take a 50% subsample to speed the plotting.\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-1-random-forests","position":5},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 2: Other measures"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-2-other-measures","position":6},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 2: Other measures"},"content":"Repeat the same plot but now use balanced_accuracy as the evaluation measure. See the \n\ndocumentation.\nOnly use the optimal max_depth from the previous question. Do you see an important difference?\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-2-other-measures","position":7},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 3: Feature importance"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-3-feature-importance","position":8},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 3: Feature importance"},"content":"Retrieve the feature importances according to the (tuned) random forest model. Which feature are most important?\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-3-feature-importance","position":9},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 4: Feature selection"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-4-feature-selection","position":10},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 4: Feature selection"},"content":"Re-build your tuned random forest, but this time only using the first 10 features.\nReturn both the balanced accuracy and training time. Interpret the results.\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-4-feature-selection","position":11},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 5: Confusion matrix"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-5-confusion-matrix","position":12},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 5: Confusion matrix"},"content":"Do a standard stratified holdout and generate the confusion matrix of the tuned random forest. Which classes are still often confused?\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-5-confusion-matrix","position":13},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 6: A second-level model"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-6-a-second-level-model","position":14},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 6: A second-level model"},"content":"Build a binary model specifically to correctly choose between the first and the second class.\nSelect only the data points with those classes and train a new random forest. Do a standard stratified split and plot the resulting ROC curve. Can we still improve the model by calibrating the threshold?\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-6-a-second-level-model","position":15},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 7: Model calibration"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-7-model-calibration","position":16},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 7: Model calibration"},"content":"For the trained binary random forest model, plot a calibration curve (see \n\ncourse notebook).\nNext, try to correct for this using Platt Scaling (or sigmoid scaling).\n\nProbability calibration should be done on new data not used for model fitting. The class \n\nCalibratedClassifierCV uses a cross-validation generator and estimates for each split the model parameter on the train samples and the calibration of the test samples. The probabilities predicted for the folds are then averaged. Already fitted classifiers can be calibrated by CalibratedClassifierCV via the parameter cv=”prefit”. \n\nRead more\n\n","type":"content","url":"/labs/lab-3a-ensembles#exercise-7-model-calibration","position":17},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 8: Gradient Boosting"},"type":"lvl2","url":"/labs/lab-3a-ensembles#exercise-8-gradient-boosting","position":18},{"hierarchy":{"lvl1":"Lab 3: Ensembles","lvl2":"Exercise 8: Gradient Boosting"},"content":"Implement a function evaluate_GB that measures the performance of GradientBoostingClassifier or the XGBoostClassifier for\ndifferent learning rates (0.01, 0.1, 1, and 10). As before, use a 3-fold cross-validation. You can use a 5% stratified sample of the whole dataset.\nFinally plot the results for n_estimators ranging from 1 to 100. Run all the GBClassifiers with random_state=1 to ensure reproducibility.\n\nImplement a function that plots the score of evaluate_GB for n_estimators = 10,20,30,...,100 on a linear scale.","type":"content","url":"/labs/lab-3a-ensembles#exercise-8-gradient-boosting","position":19},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing"},"type":"lvl1","url":"/labs/lab-3b-pipelines","position":0},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing"},"content":"We explore the performance of several linear regression models on a real-world dataset, i.e. \n\nMoneyBall. See the description on OpenML for more information. In short, this dataset captures performance data from baseball players. The regression task is to accurately predict the number of ‘runs’ each player can score, and understanding which are the most important factors.\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nimport seaborn as sns\n\n\n\n# Download MoneyBall data from OpenML\nmoneyball = oml.datasets.get_dataset(41021)\n# Get the pandas dataframe (default)\nX, y, _, attribute_names = moneyball.get_data(target=moneyball.default_target_attribute)\n\n\n\n","type":"content","url":"/labs/lab-3b-pipelines","position":1},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exploratory analysis and visualization"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exploratory-analysis-and-visualization","position":2},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exploratory analysis and visualization"},"content":"First, we visually explore the data by visualizing the value distribution and the interaction between every other feature in a scatter matrix. We use the target feature as the color variable to see which features are correlated with the target.\n\nFor the plotting to work, however, we need to remove the categorical features (the first 2) and fill in the missing values. Let’s find out which columns have missing values. This matches what we already saw on the OpenML page (\n\nhttps://​www​.openml​.org​/d​/41021).\n\npd.isnull(X).any()\n\n\n\nFor this first quick visualization, we will simply impute the missing values using the median. Removing all instances with missing values is not really an option since some features have consistent missing values: we would have to remove a lot of data.\n\n# Impute missing values with sklearn and rebuild the dataframe\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nX_clean_array = imputer.fit_transform(X[attribute_names[2:]]) # skip the first 2 features\n# The imputer will return a numpy array. To plot it we make it a pandas dataframe again.\nX_clean = pd.DataFrame(X_clean_array, columns = attribute_names[2:]) #\n\n\n\nNext, we build the scatter matrix. We include the target column to see which features strongly correlate with the target, and also use the target value as the color to see which combinations of features correlate with the target.\n\nfrom pandas.plotting import scatter_matrix\n\n# Scatter matrix of dataframe including the target feature\ncopyframe = X_clean.copy() \ncopyframe['y'] = pd.Series(y, index=copyframe.index)\nscatter_matrix(copyframe, c=y, figsize=(25,25), \n               marker='o', s=20, alpha=.8, cmap='viridis');\n\n\n\nSeveral things immediately stand out:\n\nOBP, SLG and BA strongly correlate with the target (near-diagonals in the final column), but also combinations of either of these and W or R seem useful.\n\nRA, W, OBP, SLG and BA seem normally distributed, most others do not.\n\nOOBP and OSLG have a very peaked distribution.\n\n‘Playoffs’ seems to be categorical and should probably be encoded as such.\n\n","type":"content","url":"/labs/lab-3b-pipelines#exploratory-analysis-and-visualization","position":3},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 1: Build a pipeline"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-1-build-a-pipeline","position":4},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 1: Build a pipeline"},"content":"\n\nImplement a function build_pipeline that does the following:\n\nImpute missing values by replacing NaN’s with the feature median for numerical features.\n\nEncode the categorical features using OneHotEncoding.\n\nIf the attribute scaling=True, also scale the data using standard scaling.\n\nAttach the given regression model to the end of the pipeline\n\ndef build_pipeline(regressor, numerical, categorical, scaling=False):\n    \"\"\" Build a robust pipeline with the given regression model\n    Keyword arguments:\n    regressor -- the regression model\n    categorical -- the list of categorical features\n    scaling -- whether or not to scale the data\n    \n    Returns: a pipeline\n    \"\"\"\n    pass\n\n\n\n","type":"content","url":"/labs/lab-3b-pipelines#exercise-1-build-a-pipeline","position":5},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 2: Test the pipeline"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-2-test-the-pipeline","position":6},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 2: Test the pipeline"},"content":"Test the pipeline by evaluating linear regression (without scaling) on the dataset, using 5-fold cross-validation and R^2. Make sure to run it on the original dataset (‘X’), not the manually cleaned version (‘X_clean’).\n\n","type":"content","url":"/labs/lab-3b-pipelines#exercise-2-test-the-pipeline","position":7},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 3: A first benchmark"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-3-a-first-benchmark","position":8},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 3: A first benchmark"},"content":"Evaluate the following algorithms in their default settings, both with and without scaling, and interpret the results:\n\nLinear regression\n\nRidge\n\nLasso\n\nSVM (RBF)\n\nRandomForests\n\nGradientBoosting\n\n","type":"content","url":"/labs/lab-3b-pipelines#exercise-3-a-first-benchmark","position":9},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 4: Tuning linear models"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-4-tuning-linear-models","position":10},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 4: Tuning linear models"},"content":"Next, visualize the effect of the alpha regularizer for Ridge and Lasso. Vary alpha from 1e-4 to 1e6 and plot the R^2 score as a line plot (one line for each algorithm). Always use scaling. Interpret the results.\n\n","type":"content","url":"/labs/lab-3b-pipelines#exercise-4-tuning-linear-models","position":11},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 5: Tuning SVMs"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-5-tuning-svms","position":12},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 5: Tuning SVMs"},"content":"Next, tune the SVM’s C and gamma. You can stay within the 1e-6 to 1e6 range. Plot the R^2 score as a heatmap.\n\ndef heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n    if ax is None:\n        ax = plt.gca()\n    # plot the mean cross-validation scores\n    img = ax.pcolor(values, cmap=cmap, vmin=None, vmax=None)\n    img.update_scalarmappable()\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n    ax.set_xticklabels(xticklabels)\n    ax.set_yticklabels(yticklabels)\n    ax.set_aspect(1)\n\n    for p, color, value in zip(img.get_paths(), img.get_facecolors(), img.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.mean(color[:3]) > 0.5:\n            c = 'k'\n        else:\n            c = 'w'\n        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n    return img\n\n\n\n","type":"content","url":"/labs/lab-3b-pipelines#exercise-5-tuning-svms","position":13},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 5b: Tuning SVMs (2)"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-5b-tuning-svms-2","position":14},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 5b: Tuning SVMs (2)"},"content":"Redraw the heatmap, but now use scaling. What do you observe?\n\n","type":"content","url":"/labs/lab-3b-pipelines#exercise-5b-tuning-svms-2","position":15},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 6: Feature importance"},"type":"lvl2","url":"/labs/lab-3b-pipelines#exercise-6-feature-importance","position":16},{"hierarchy":{"lvl1":"Lab 4:  Data preprocessing","lvl2":"Exercise 6: Feature importance"},"content":"Retrieve the coefficients from the optimized Lasso, Ridge, and the feature importances from the default RandomForest and GradientBoosting models.\nCompare the results. Do the different models agree on which features are important? You will need to map the encoded feature names to the correct coefficients and feature importances. If you can, plot the importances as a bar chart.","type":"content","url":"/labs/lab-3b-pipelines#exercise-6-feature-importance","position":17},{"hierarchy":{"lvl1":"Lab 4: Neural networks"},"type":"lvl1","url":"/labs/lab-4-neural-networks","position":0},{"hierarchy":{"lvl1":"Lab 4: Neural networks"},"content":"In this lab we will build dense neural networks on the MNIST dataset.\n\nMake sure you read the tutorial for this lab first.\n\n","type":"content","url":"/labs/lab-4-neural-networks","position":1},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Load the data and create train-test splits"},"type":"lvl2","url":"/labs/lab-4-neural-networks#load-the-data-and-create-train-test-splits","position":2},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Load the data and create train-test splits"},"content":"\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport openml as oml\nimport matplotlib.pyplot as plt\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\n\n\n# Download MNIST data. Takes a while the first time.\nmnist = oml.datasets.get_dataset(554)\nX, y, _, _ = mnist.get_data(target=mnist.default_target_attribute, dataset_format='array');\nX = X.reshape(70000, 28, 28)\n\n# Take some random examples\nfrom random import randint\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor i in range(5):\n    n = randint(0,70000)\n    axes[i].imshow(X[n], cmap=plt.cm.gray_r)\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n    axes[i].set_xlabel(\"{}\".format(y[n]))\nplt.show();\n\n\n\n# For MNIST, there exists a predefined stratified train-test split of 60000-10000. We therefore don't shuffle or stratify here.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000, random_state=0)\n\n\n\n","type":"content","url":"/labs/lab-4-neural-networks#load-the-data-and-create-train-test-splits","position":3},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 1: Preprocessing"},"type":"lvl2","url":"/labs/lab-4-neural-networks#exercise-1-preprocessing","position":4},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 1: Preprocessing"},"content":"Normalize the data: map each feature value from its current representation (an integer between 0 and 255) to a floating-point value between 0 and 1.0.\n\nCreate a train-test split using the first 60000 examples for training\n\nFlatten the data\n\nConvert the data (numpy arrays) to PyTorch tensors\n\nCreate a TensorDataset for the training data, and another for the testing data\n\n","type":"content","url":"/labs/lab-4-neural-networks#exercise-1-preprocessing","position":5},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 2: Create a deep neural net model"},"type":"lvl2","url":"/labs/lab-4-neural-networks#exercise-2-create-a-deep-neural-net-model","position":6},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 2: Create a deep neural net model"},"content":"Implement a create_model function which defines the topography of the deep neural net, specifying the following:\n\nThe number of layers in the deep neural net: Use 2 dense layers for now (one hidden en one output layer)\n\nThe number of nodes in each layer: these are parameters of your function.\n\nAny regularization layers. Add at least one dropout layer.\n\nConsider:\n\nWhat should be the shape of the input layer?\n\nWhich activation function you will need for the last layer, since this is a 10-class classification problem?\n\n### Create and compile a 'deep' neural net\ndef create_model(layer_1_units=32, layer_2_units=10, dropout_rate=0.3):\n    pass\n\n\n\n","type":"content","url":"/labs/lab-4-neural-networks#exercise-2-create-a-deep-neural-net-model","position":7},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 3: Create a training function"},"type":"lvl2","url":"/labs/lab-4-neural-networks#exercise-3-create-a-training-function","position":8},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 3: Create a training function"},"content":"Implement a train_model function which trains and evaluates a given model.\nIt should print out the train and validation loss and accuracy.\n\ndef train_model(model, train_dataset, val_dataset, epochs=10, batch_size=64, learning_rate=0.001):\n    \"\"\"\n    model: the model to train\n    train_dataset: the training data and labels\n    test_dataset: the test data and labels\n    epochs: the number of epochs to train for\n    batch_size: the batch size for minibatch SGD\n    learning_rate: the learning rate for the optimizer\n    \"\"\"\n    pass\n\n\n\n","type":"content","url":"/labs/lab-4-neural-networks#exercise-3-create-a-training-function","position":9},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 4: Evaluate the model"},"type":"lvl2","url":"/labs/lab-4-neural-networks#exercise-4-evaluate-the-model","position":10},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 4: Evaluate the model"},"content":"Train the model with a learning rate of 0.003, 50 epochs, batch size 4000, and a validation set that is 20% of the total training data.\n\nUse default settings otherwise. Plot the learning curve of the loss, validation loss, accuracy, and validation accuracy. Finally, report the performance on the test set.\n\nTry to run the model on GPU.\n\nFeel free to use the plotting function below, or implement the callback from the tutorial to see results in real time.\n\n# Helper plotting function\n#\n# history: the history object returned by the training function\n\ndef plot_curve(history):\n    \"\"\"\n    Plots the learning curves for accuracy and loss.\n\n    history: Dictionary containing 'accuracy', 'val_accuracy', 'loss', 'val_loss' per epoch.\n    \"\"\"\n    epochs = range(1, len(history[\"accuracy\"]) + 1)\n\n    plt.figure(figsize=(12, 5))\n\n    # Accuracy Plot\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history[\"accuracy\"], label=\"Train Accuracy\")\n    plt.plot(epochs, history[\"val_accuracy\"], label=\"Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy Curve\")\n    plt.legend()\n\n    # Loss Plot\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history[\"loss\"], label=\"Train Loss\")\n    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Curve\")\n    plt.legend()\n\n    plt.show()\n\n\n\n","type":"content","url":"/labs/lab-4-neural-networks#exercise-4-evaluate-the-model","position":11},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 5: Optimize the model"},"type":"lvl2","url":"/labs/lab-4-neural-networks#exercise-5-optimize-the-model","position":12},{"hierarchy":{"lvl1":"Lab 4: Neural networks","lvl2":"Exercise 5: Optimize the model"},"content":"Try to optimize the model, either manually or with a tuning method. At least optimize the following:\n\nthe number of hidden layers\n\nthe number of nodes in each layer\n\nthe amount of dropout layers and the dropout rate\n\nTry to reach at least 96% accuracy against the test set.","type":"content","url":"/labs/lab-4-neural-networks#exercise-5-optimize-the-model","position":13},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch"},"type":"lvl1","url":"/labs/lab-4-tutorial","position":0},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch"},"content":"# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial","position":1},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Torch tensors"},"type":"lvl2","url":"/labs/lab-4-tutorial#torch-tensors","position":2},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Torch tensors"},"content":"Whereas we have been using numpy arrays and pandas dataframes for most of the course, PyTorch (we’ll often say Torch for simplicity) uses tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n\nTensors are multi-dimensional arrays, and very similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing. Tensors are stored on a device, like a cpu or gpu.","type":"content","url":"/labs/lab-4-tutorial#torch-tensors","position":3},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Initialization","lvl2":"Torch tensors"},"type":"lvl3","url":"/labs/lab-4-tutorial#initialization","position":4},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Initialization","lvl2":"Torch tensors"},"content":"You can create tensors from existing lists, arrays, or a range of existing initializers.\n\n# List to Tensor\ndata = [[1, 2], [3, 4]]\nx_data = torch.tensor(data)\n\n# Initialize with a given shape\nshape = (2, 3, )\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)   \n\n\nprint(f\"From list Tensor: \\n {x_data} \\n\")\nprint(f\"From Numpy: \\n {x_np} \\n\")\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\n\n\n\n\nYou can move between Numpy arrays and tensors in both directions.\nCareful: if your tensor is stored on cpu, it will point to the same object in memory as the Numpy array, and changing one will change the other.\n\n# Numpy to Tensor\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nprint(f\"Tensor original: \\n {x_np} \\n\")\n\n# Tensor to Numpy\nnp_array = x_np.numpy()\n\n# Change the tensor\nx_np.add_(1) # The underscore _ means that the operation is done in place\nprint(f\"Tensor changed: \\n {x_np} \\n\")\nprint(f\"Numpy array has also changed: \\n {np_array} \\n\")\n\n\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#initialization","position":5},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Attributes","lvl2":"Torch tensors"},"type":"lvl3","url":"/labs/lab-4-tutorial#attributes","position":6},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Attributes","lvl2":"Torch tensors"},"content":"You can query a tensor’s shape, datatype, and device.\n\ntensor = torch.rand(3, 4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#attributes","position":7},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Switching to GPU","lvl2":"Torch tensors"},"type":"lvl3","url":"/labs/lab-4-tutorial#switching-to-gpu","position":8},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Switching to GPU","lvl2":"Torch tensors"},"content":"You can move the tensor to the GPU using the .to method.\n\n# We move our tensor to the GPU if available\n# For CUDA-based systems\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n  device = torch.device('cuda:0')\n  print(f\"Device tensor is stored on: {tensor.device}\")\n\n# For Mac M1/M4-based systems\nif torch.backends.mps.is_available():\n  device = torch.device(\"mps\")\n  tensor = tensor.to(device)\n\nprint(f\"Tensor is stored on device: {tensor.device}\")\n\n\n\nNOTE:\n\nIf your laptop has a GPU, we strongly recommend that you figure out how to use it.\n\nFor Windows machines, you probably need to \n\ninstall CUDA.\n\nFor Macs with M1/M4 chips, it should work out of the box.\n\nIf you’re using Colab, allocate a GPU by going to Edit > Notebook Settings.\nAt the time of writing, T4 GPUs are free to use, and fine for doing the labs. Better GPUs like the A100 will consume credits. On the top right, click the down arrow and then ‘View resources’.\n\nFor large-scale experiments, see the guide on using the Snellius Supercomputer using your course credits.\n\n","type":"content","url":"/labs/lab-4-tutorial#switching-to-gpu","position":9},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Tensor operations","lvl2":"Torch tensors"},"type":"lvl3","url":"/labs/lab-4-tutorial#tensor-operations","position":10},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Tensor operations","lvl2":"Torch tensors"},"content":"There are over 100 tensor operations, including transposing, indexing, slicing,\nmathematical operations, linear algebra, and random sampling. Since you are already familiar with the Numpy API, you’ll find these very familiar. See\n\n\nhere for a full list of tensor operations.\n\n# Indexing and slicing\ntensor = torch.ones(4, 4)\ntensor[:,1] = 0\nprint(tensor)\n\n\n\n# Joining\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\n\n\n# Matrix multiplication (matmul)\nprint(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n# Alternative syntax:\nprint(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#tensor-operations","position":11},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Neural Network training (backpropagation)"},"type":"lvl2","url":"/labs/lab-4-tutorial#neural-network-training-backpropagation","position":12},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Neural Network training (backpropagation)"},"content":"As a reminder, training a neural network (NN) is a series of functions (e.g. matrix multiplications, activation functions, softmax operations), many of which have parameters (model weights and biases) that define the exact output. Training\ntypically occurs in two steps:\n\nForward Propagation: The NN runs the input data through each of its\nfunctions to make its best guess about the correct output. After this, we compute compute the error (loss).\n\nBackward Propagation: In backprop, the NN adjusts its parameters\nproportionate to the error in its guess. It does this by traversing\nbackwards from the output, collecting the derivatives of the error with\nrespect to the parameters of the functions (gradients), and optimizing\nthe parameters using gradient descent. For a more detailed walkthrough\nof backprop, check out \n\nthis and \n\nthis video from 3Blue1Brown.\n\nTo illustrate how this works, we’ll start from a pretrained resnet18 model. We create a random 64x64 input image and feed it to the network.\nNote: this example only works on CPU.\n\n# Import a pretrained model and its weights\nfrom torchvision.models import resnet18, ResNet18_Weights\nmodel = resnet18(weights=ResNet18_Weights.DEFAULT)\n\n# Create a random input. The model expects a 4D tensor.\n# The first dimension is the batch size (1), the second is the number of channels (red-green-blue),\n# and the last two are the height and width of the input.\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)\n\n\n\nForward pass: simply feed the input to the model to get its predictions.\nThen, based on the predictions, we can compute the loss.\n\nprediction = model(data) # forward pass\n\nloss_fn = torch.nn.MSELoss() # Mean Squared Error loss (we'll discuss torch.nn in more detail later)\nloss = loss_fn(prediction, labels) # this returns a Tensor\n\n\n\nBackward passStep 1: Compute the gradients by calling .backward(). PyTorch’s autograd engine will calculate and store the gradients for each model parameter (in the parameter’s .grad attribute).\n\nThere is a lot going on in the background. In short, every time you perform a forward pass, PyTorch dynamically constructs a computational graph that tracks tensors and operations involved in computing gradients. When you call .backward(), PyTorch traverses this graph in reverse to compute all gradients efficiently using a process called automatic differentiation. To optimize computation, PyTorch stores intermediate values needed for gradient calculation while applying the chain rule. Once backward() completes, the computational graph is discarded to free memory, unless retain_graph=True is specified. This means you can modify the model’s structure (e.g., shape, size, and operations) at every iteration.\n\nloss.backward() # backward pass\n\n\n\nIt’s important to understand that loss is a Tensor, and in PyTorch, every tensor that results from an operation is also a node in the computational graph (at least if it has the property requires_grad=True). Hence, loss is the last node in the computational graph, and by calling backward, PyTorch will traverse the graph backwards and attach all the gradients to all the (learnable) tensors it encounters. We will visualize the computational graph below (ResNet has a huge graph).\n\nloss\n\n\n\nStep 2: Choose an optimizer (e.g. stochastic gradient descent) and its hyperparameters (e.g. learning rate). Then do one .step() with the optimizer to update the model weigths.\n\nThe expression model.parameters() in PyTorch returns an iterator over all learnable parameters (weights and biases) of the model (i.e., tensors with requires_grad=True). When we ran backward() we added the gradients to the parameters, and the optimizer will use these gradients to update the parameters when we run step.\n\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\noptim.step() #gradient descent\n\n\n\n\nRecap. One training cycle thus consists of these simple commands, and now you also understand how they work.\n\nprediction = model(data) # forward pass\nloss = loss_fn(prediction, labels) # compute loss\nloss.backward() # backward pass (compute all gradients)\noptim.step() # update model weigths\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#neural-network-training-backpropagation","position":13},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Building Neural Networks"},"type":"lvl2","url":"/labs/lab-4-tutorial#building-neural-networks","position":14},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Building Neural Networks"},"content":"It’s now time to build a complete NN. This can be done via the torch.nn package, which contains a lot of components (e.g. layers such as nn.Conv2d) and the torch.nn.functional module which contains functions such as relu and max_pool.\n\nDefining a NN works buy defining a Python class:\n\nCreate a subclass of nn.Module.\n\nIn the __init__ method, define the components that you will use and their hyperparameters.\n\nIn the forward method, define the structure of the network. Pass the input to the required functions and layers, store the result in a variable, and pass it to the next function.\n\nFor example, let’s implement the convolutional network (LeNet) in the image below\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels (filters), 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        # 6 input channels, 16 output channels (filters), 5x5 square convolution kernel\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # Fully connected layers. \n        # After flattening the conv2 output, we get a vector of 16 * 5 * 5\n        self.fc1 = nn.Linear(16 * 5 * 5, 120) # 120 hidden nodes\n        self.fc2 = nn.Linear(120, 84) # 84 hidden nodes\n        self.fc3 = nn.Linear(84, 10) # 10 output nodes (classes)\n\n    def forward(self, input):\n        # Convolution layer: conv1 + RELU activation function\n        # Outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n        c1 = F.relu(self.conv1(input))\n        # Maxpooling (subsampling)) layer: 2x2 grid\n        # This layer does not have any parameters, outputs a (N, 6, 14, 14) Tensor\n        s2 = F.max_pool2d(c1, (2, 2))\n        # Convolution layer: conv2 + RELU activation\n        # Outputs a (N, 16, 10, 10) Tensor\n        c3 = F.relu(self.conv2(s2))\n        # Maxpooling layer S4: 2x2 grid\n        # Outputs a (N, 16, 5, 5) Tensor\n        s4 = F.max_pool2d(c3, 2)\n        # Flatten operation: outputs a (N, 400) Tensor (16*5*5)\n        s4 = torch.flatten(s4, 1)\n        # Fully connected layer + RELU activation\n        # (N, 400) Tensor input, outputs a (N, 120) Tensor\n        f5 = F.relu(self.fc1(s4))\n        # Fully connected layer + RELU activation\n        # (N, 120) Tensor input, outputs a (N, 84) Tensor\n        f6 = F.relu(self.fc2(f5))\n        # Output layer:\n        # (N, 84) Tensor input, outputs a (N, 10) Tensor\n        output = self.fc3(f6)\n        return output\n\nnet = Net()\nprint(net)\n\n\n\nInspect the number of learnable parameters\n\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total learnable parameters: {num_params}\")\n\n\n\nToy training data\n\ninput = torch.randn(1, 1, 32, 32) # random input image\ntarget = torch.randn(10)  # a dummy target\ntarget = target.view(1, -1)  # make it the same shape as output\n\n\n\nForward pass: Passing input through the network calls the forward method of the model, computing and returning the output of the network\n\nout = net(input)\nprint(out)\n\n\n\nCompute loss: same as before.\n\nloss_fn = torch.nn.MSELoss() # Mean Squared Error loss\nloss = loss_fn(out, target)\nprint(loss)\n\n\n\nYou can run the code below to visualize the computational graph ending in loss (green box). It will show all the operations and the learnable model parameters in blue boxes (biases and weights in separate boxes).\n\nfrom torchviz import make_dot\ngraph = make_dot(loss, params=dict(model.named_parameters()))\ngraph.render(\"computational_graph\", format=\"png\", view=True)\n\n# Display inside Jupyter Notebook\nfrom IPython.display import Image\nImage(\"computational_graph.png\")\n\n\n\nBackward pass. In PyTorch, you need to zero the gradients of all parameters before performing a new backward pass because gradients accumulate by default. This behavior is designed to support techniques like mini-batch gradient accumulation, but in standard training loops, failing to zero out the gradients can lead to incorrect updates.\n\nnet.zero_grad() # zeroes the gradient buffers of all parameters\n\nprint('Gradients of Conv1 biases, after zero_grad, before backward:', net.conv1.bias.grad)\n\nloss.backward()\n\nprint('Gradients of Conv1 biases, after backward:', net.conv1.bias.grad)\n\n\n\nOptimizer step: Create an SGD optimizer on the model parameters and do a step. Notice that the parameters have changed slightly.\n\noptim = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)\n\nprint('Weights of Conv1 biases, before step:', net.conv1.bias)\noptim.step() #gradient descent\nprint('Weights of Conv1 biases, after step:', net.conv1.bias)\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#building-neural-networks","position":15},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Training a classifier for CIFAR-10"},"type":"lvl2","url":"/labs/lab-4-tutorial#training-a-classifier-for-cifar-10","position":16},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Training a classifier for CIFAR-10"},"content":"We’ll now do a complete training run on the CIFAR10 dataset. It has the classes:\n‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’,\n‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e.\n3-channel color images of 32x32 pixels in size.\n\n","type":"content","url":"/labs/lab-4-tutorial#training-a-classifier-for-cifar-10","position":17},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Loading the data","lvl2":"Training a classifier for CIFAR-10"},"type":"lvl3","url":"/labs/lab-4-tutorial#loading-the-data","position":18},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Loading the data","lvl2":"Training a classifier for CIFAR-10"},"content":"We can het the CIFAR10 datasets from torchvision, but since these are PILImage images, we’ll need to transform them to Tensors first and normalize them. We’ll also create a separate training and test set (using a predefined split). We’ll also create data loaders that will return batches of 4 images at a time.\n\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n\nLet’s peek at some of the images returned by our dataloader.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to show a batch of images with labels underneath\ndef show_images(images, labels, classes):\n    images = images / 2 + 0.5  \n    np_images = images.numpy()\n\n    batch_size = len(images)  \n    fig, axes = plt.subplots(1, batch_size, figsize=(batch_size * 2, 2)) \n\n    if batch_size == 1: \n        axes = [axes]\n\n    for idx, ax in enumerate(axes):\n        ax.imshow(np.transpose(np_images[idx], (1, 2, 0)))  # Convert (C, H, W) to (H, W, C)\n        ax.set_title(classes[labels[idx]], fontsize=10)  # Set label as title\n        ax.axis(\"off\")  # Hide axes\n\n    plt.tight_layout()\n    plt.show()\n\n# Get a batch of training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Show images with class labels above\nshow_images(images, labels, classes)\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#loading-the-data","position":19},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Create the neural network","lvl2":"Training a classifier for CIFAR-10"},"type":"lvl3","url":"/labs/lab-4-tutorial#create-the-neural-network","position":20},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Create the neural network","lvl2":"Training a classifier for CIFAR-10"},"content":"We will re-create the network we had before, but now using 3-channel inputs since these are colored images. We’ll also condense the code, as is often done. Check that it is still the same network.\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\n\n\n\nSidenote: simple networks like this, which are a pure sequence of layers, can also be defined by the Sequential API, which is much simpler:\n\nnet_sequential = nn.Sequential(\n    nn.Conv2d(3, 6, 5),\n    nn.ReLU(),\n    nn.MaxPool2d(2, 2),\n    \n    nn.Conv2d(6, 16, 5),\n    nn.ReLU(),\n    nn.MaxPool2d(2, 2),\n    \n    nn.Flatten(),\n    nn.Linear(16 * 5 * 5, 120),\n    nn.ReLU(),\n    \n    nn.Linear(120, 84),\n    nn.ReLU(),\n    \n    nn.Linear(84, 10)\n)\n\n\n\nPut the network on GPU if possible. Note that when we train the network, we also need to put the data on the GPU.\n\nnet.to(device)\n\n\n\nWe’ll use cross-entropy loss and an SGD optimizer\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#create-the-neural-network","position":21},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Train the network","lvl2":"Training a classifier for CIFAR-10"},"type":"lvl3","url":"/labs/lab-4-tutorial#train-the-network","position":22},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Train the network","lvl2":"Training a classifier for CIFAR-10"},"content":"To train the network, we have to do all the steps we discussed before in a loop. Each iteration, we’ll get a new batch of data from the dataloader, and we’ll also repeat this for 2 epochs.\n\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        # Put the data on GPU if possible\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#train-the-network","position":23},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Test the network","lvl2":"Training a classifier for CIFAR-10"},"type":"lvl3","url":"/labs/lab-4-tutorial#test-the-network","position":24},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Test the network","lvl2":"Training a classifier for CIFAR-10"},"content":"Let’s see whether the model makes good predictions. We’ll take some examples from out test set loader.\n\n# Get a batch of test images\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\nshow_images(images, labels, classes)\n\n\n\nGet the outputs from the trained model\n\noutputs = net(images.to(device))\n\n\n\nSince the model returns probabilities, we’ll take the class with the highest probability.\n\n_, predicted = torch.max(outputs, 1)\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\n\n\n\nNow, let’s evaluate on the whole test set\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        inputs, labels = data[0].to(device), data[1].to(device)\n        # calculate outputs by running images through the network\n        outputs = net(inputs)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#test-the-network","position":25},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Saving and reloading","lvl2":"Training a classifier for CIFAR-10"},"type":"lvl3","url":"/labs/lab-4-tutorial#saving-and-reloading","position":26},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl3":"Saving and reloading","lvl2":"Training a classifier for CIFAR-10"},"content":"Finally, let’s see how we can save the model to disk and load it again.\n\n# Saving the trained model\ntorch.save(net.state_dict(), './cifar_net.pth')\n\n# Load the trained model\nnet = Net()\nnet.load_state_dict(torch.load('./cifar_net.pth', weights_only=True))\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#saving-and-reloading","position":27},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Pytorch Lightning"},"type":"lvl2","url":"/labs/lab-4-tutorial#pytorch-lightning","position":28},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Pytorch Lightning"},"content":"We did have to write quite a bit of code to build and train the model.\n\n\nPyTorch Lightning is a handy library that abstracts away boilerplate code and provides a fit() function similar to sklearn or Keras.\n\nSee the code below for an example. Here we used a resnet, but you can also use your own network.\n\nimport lightning as L\n\n# Define the LightningModule\nclass LitModel(L.LightningModule):\n    def __init__(self, model, criterion):\n        super().__init__()\n        self.model = model\n        self.criterion = criterion\n        self.training_loss_history = []  # Store loss values\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        outputs = self.model(inputs)\n        loss = self.criterion(outputs, labels)\n\n        # Log loss for visualization\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=True)\n\n        # Store loss in history (needed for manual plotting)\n        self.training_loss_history.append(loss.item())\n\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n\n# Load dataset\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n\n# Define model\nnet = torchvision.models.resnet18(num_classes=10)  # Example model\ncriterion = nn.CrossEntropyLoss()\n\n# Wrap the model in Lightning\nlit_model = LitModel(net, criterion)\n\n# Train using PyTorch Lightning Trainer\ntrainer = L.Trainer(max_epochs=2)\ntrainer.fit(lit_model, trainloader)\n\n# Plot loss curve\nplt.plot(lit_model.training_loss_history, label=\"Training Loss\", linestyle=\"-\")\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Curve\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/labs/lab-4-tutorial#pytorch-lightning","position":29},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Tensorboard"},"type":"lvl2","url":"/labs/lab-4-tutorial#tensorboard","position":30},{"hierarchy":{"lvl1":"Lab 4: Deep Learning with PyTorch","lvl2":"Tensorboard"},"content":"TensorBoard is a visualization toolkit for tracking and analyzing deep learning experiments. It can log:\n\nTraining loss curves\n\nValidation accuracy over epochs\n\nModel graphs\n\nHistograms of weights & gradients\n\nImages, embeddings, and more\n\nWe can log our model training as follows:\n\nimport lightning.pytorch as pl \nlogger = pl.loggers.TensorBoardLogger(\"logs/\", name=\"my_experiment\")\ntrainer = pl.Trainer(max_epochs=2, logger=logger)\ntrainer.fit(lit_model, trainloader)\n\n\n\n\n\n\n\nNormally, you would open tensorboard from the command line withtensorboard --logdir logs\n\nBut we can also integrate it here using some magic:\n\n%load_ext tensorboard\n%tensorboard --logdir logs\n\n\n\n\n\n\n\nTODO:\n\nMore on drawing learning curves\n\nRegularization (Early stopping, L1/L2, Dropout, Batchnorm)","type":"content","url":"/labs/lab-4-tutorial#tensorboard","position":31},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets"},"type":"lvl1","url":"/labs/lab-5-convolutional-neural-networks","position":0},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets"},"content":"In this lab we consider the \n\nCIFAR dataset, but model it using convolutional neural networks instead of linear models.\nThere is no separate tutorial, but you can find lots of examples in the lecture notebook on convolutional neural networks. If you are very confident, you can also try to solve these exercises using PyTorch instead of TensorFlow.\n\nTip: You can run these exercises faster on a GPU (but they will also run fine on a CPU). If you do not have a GPU locally, you can upload this notebook to Google Colab. You can enable GPU support at “runtime” -> “change runtime type”.\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nimport tensorflow as tf\n\n\n\n# Uncomment the next line if you run on Colab\n#!pip install --quiet openml\n\n\n\n%matplotlib inline\nimport openml as oml\nimport matplotlib.pyplot as plt\n\n\n\n# Download CIFAR data. Takes a while the first time.\n# This version returns 3x32x32 resolution images. \n# If you feel like it, repeat the exercises with the 96x96x3 resolution version by using ID 41103 \ncifar = oml.datasets.get_dataset(40926) \nX, y, _, _ = cifar.get_data(target=cifar.default_target_attribute, dataset_format='array'); \ncifar_classes = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\",\n                 5: \"dog\", 6: \"frog\", 7: \"horse\", 8: \"ship\", 9: \"truck\"}\n\n\n\n# The dataset (40926) is in a weird 3x32x32 format, we need to reshape and transpose\nXr = X.reshape((len(X),3,32,32)).transpose(0,2,3,1)\n\n\n\n# Take some random examples, reshape to a 32x32 image and plot\nfrom random import randint\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor i in range(5):\n    n = randint(0,len(Xr))\n    # The data is stored in a 3x32x32 format, so we need to transpose it\n    axes[i].imshow(Xr[n]/255)\n    axes[i].set_xlabel((cifar_classes[int(y[n])]))\n    axes[i].set_xticks(()), axes[i].set_yticks(())\nplt.show();\n\n\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks","position":1},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 1: A simple model"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#exercise-1-a-simple-model","position":2},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 1: A simple model"},"content":"Split the data into 80% training and 20% validation sets\n\nNormalize the data to [0,1]\n\nBuild a ConvNet with 3 convolutional layers interspersed with MaxPooling layers, and one dense layer.\n\nUse at least 32 3x3 filters in the first layer and ReLU activation.\n\nOtherwise, make rational design choices or experiment a bit to see what works.\n\nYou should at least get 60% accuracy.\n\nFor training, you can try batch sizes of 64, and 20-50 epochs, but feel free to explore this as well\n\nPlot and interpret the learning curves. Is the model overfitting? How could you improve it further?\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks#exercise-1-a-simple-model","position":3},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 2: VGG-like model"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#exercise-2-vgg-like-model","position":4},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 2: VGG-like model"},"content":"Implement a simplified VGG model by building 3 ‘blocks’ of 2 convolutional layers each\n\nDo MaxPooling after each block\n\nThe first block should use at least 32 filters, later blocks should use more\n\nYou can use 3x3 filters\n\nUse zero-padding to be able to build a deeper model (see the padding attribute)\n\nUse a dense layer with at least 128 hidden nodes.\n\nYou can use ReLU activations everywhere (where it makes sense)\n\nPlot and interpret the learning curves\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks#exercise-2-vgg-like-model","position":5},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 3: Regularization"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#exercise-3-regularization","position":6},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 3: Regularization"},"content":"Explore different ways to regularize your VGG-like model\n\nTry adding some dropout after every MaxPooling and Dense layer.\n\nWhat are good Dropout rates? Try a fixed Dropout rate, or increase the rates in the deeper layers.\n\nTry batch normalization together with Dropout\n\nThink about where batch normalization would make sense\n\nPlot and interpret the learning curves\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks#exercise-3-regularization","position":7},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 4: Data Augmentation"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#exercise-4-data-augmentation","position":8},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 4: Data Augmentation"},"content":"Perform image augmentation (rotation, shift, shear, zoom, flip,...). You can use the ImageDataGenerator for this.\n\nWhat is the effect? What is the effect with and without Dropout?\n\nPlot and interpret the learning curves\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks#exercise-4-data-augmentation","position":9},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 5: Interpret the misclassifications"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#exercise-5-interpret-the-misclassifications","position":10},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 5: Interpret the misclassifications"},"content":"Chances are that even your best model is not yet perfect. It is important to understand what kind of errors it still makes.\n\nRun the test images through the network and detect all misclassified ones\n\nInterpret some of the misclassifications. Are these misclassifications to be expected?\n\nCompute the confusion matrix. Which classes are often confused?\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks#exercise-5-interpret-the-misclassifications","position":11},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 6: Interpret the model"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#exercise-6-interpret-the-model","position":12},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Exercise 6: Interpret the model"},"content":"Retrain your best model on all the data. Next, retrieve and visualize the activations (feature maps) for every filter for every convolutional layer, or at least for a few filters for every layer. Tip: see the course notebooks for examples on how to do this.\n\nInterpret the results. Is your model indeed learning something useful?\n\n","type":"content","url":"/labs/lab-5-convolutional-neural-networks#exercise-6-interpret-the-model","position":13},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Optional: Take it a step further"},"type":"lvl2","url":"/labs/lab-5-convolutional-neural-networks#optional-take-it-a-step-further","position":14},{"hierarchy":{"lvl1":"Lab 7a: Convolutional neural nets","lvl2":"Optional: Take it a step further"},"content":"Repeat the exercises, but now use a \n\nhigher-resolution version of the CIFAR dataset (with OpenML ID 41103), or another \n\nversion with 100 classes (with OpenML ID 41983). Good luck!","type":"content","url":"/labs/lab-5-convolutional-neural-networks#optional-take-it-a-step-further","position":15},{"hierarchy":{"lvl1":"Lab 6: Transformers"},"type":"lvl1","url":"/labs/lab-6-transformers","position":0},{"hierarchy":{"lvl1":"Lab 6: Transformers"},"content":"In this lab, we will apply the transformer architecture from the tutorial to various tasks:\n\nSequence-to-Sequence modelling\n\nSet anomaly detection.\n\n","type":"content","url":"/labs/lab-6-transformers","position":1},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl2":"Question 1: Sequence to Sequence"},"type":"lvl2","url":"/labs/lab-6-transformers#question-1-sequence-to-sequence","position":2},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl2":"Question 1: Sequence to Sequence"},"content":"A Sequence-to-Sequence task represents a task where the input and the output is a sequence, e.g. as in machine translation and summarization. Usually we would use an encoder-decoder architecture for this, but for now we’ll use only the encoder om a simple task: given a sequence of N numbers between 0 and M, the task is to reverse the input sequence. In Numpy notation, if our input is x, the output should be x[::-1].","type":"content","url":"/labs/lab-6-transformers#question-1-sequence-to-sequence","position":3},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The data","lvl2":"Question 1: Sequence to Sequence"},"type":"lvl3","url":"/labs/lab-6-transformers#the-data","position":4},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The data","lvl2":"Question 1: Sequence to Sequence"},"content":"First, let’s create a dataset class below.\n\nclass ReverseDataset(data.Dataset):\n\n    def __init__(self, num_categories, seq_len, size):\n        super().__init__()\n        self.num_categories = num_categories\n        self.seq_len = seq_len\n        self.size = size\n        \n        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n  \n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        inp_data = self.data[idx]\n        labels = torch.flip(inp_data, dims=(0,))\n        return inp_data, labels\n\n\n\nWe create an arbitrary number of random sequences of numbers between 0 and num_categories-1. The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders below.\n\ndataset = partial(ReverseDataset, 10, 16)\ntrain_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\nval_loader   = data.DataLoader(dataset(1000), batch_size=128)\ntest_loader  = data.DataLoader(dataset(10000), batch_size=128)\n\n\n\nLet’s look at an arbitrary sample of the dataset:\n\ninp_data, labels = train_loader.dataset[0]\nprint(\"Input data:\", inp_data)\nprint(\"Labels:    \", labels)\n\n\n\n","type":"content","url":"/labs/lab-6-transformers#the-data","position":5},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The model","lvl2":"Question 1: Sequence to Sequence"},"type":"lvl3","url":"/labs/lab-6-transformers#the-model","position":6},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The model","lvl2":"Question 1: Sequence to Sequence"},"content":"Create a class ReversePredictor, which extends the TransformerPredictor we created in the tutorial. During training, pass the input sequence through the Transformer encoder and predict the output for each input token. You can use standard Cross-Entropy loss. Every number can be represented as a one-hot vector, or a learned embedding vector provided by the PyTorch module nn.Embedding.\n\nCreate a training function for PyTorch Lightning, and also test your model on the test set. Experiment with adding an additional parameter gradient_clip_val that implements \n\ngradient clipping).\n\n","type":"content","url":"/labs/lab-6-transformers#the-model","position":7},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The trainer","lvl2":"Question 1: Sequence to Sequence"},"type":"lvl3","url":"/labs/lab-6-transformers#the-trainer","position":8},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The trainer","lvl2":"Question 1: Sequence to Sequence"},"content":"Train the model, use a single encoder block and a single head in the Multi-Head Attention. This should be ok for this simple task.\n\n","type":"content","url":"/labs/lab-6-transformers#the-trainer","position":9},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"Visualization","lvl2":"Question 1: Sequence to Sequence"},"type":"lvl3","url":"/labs/lab-6-transformers#visualization","position":10},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"Visualization","lvl2":"Question 1: Sequence to Sequence"},"content":"Try to visualize the attention in the Multi-Head Attention block, for an arbitrary input. Next, create a plot where over rows, we have different layers, while over columns, we show the different heads, i.e. a matrix visualizing all the attention values.\n\n","type":"content","url":"/labs/lab-6-transformers#visualization","position":11},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl2":"Set Anomaly Detection"},"type":"lvl2","url":"/labs/lab-6-transformers#set-anomaly-detection","position":12},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl2":"Set Anomaly Detection"},"content":"Transformers offer the perfect architecture for set problems as the Multi-Head Attention is permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs (inputs and outputs are permuted equally). The task we are looking at for sets is Set Anomaly Detection which means that we try to find the element(s) in a set that does not fit the others. A common application of anomaly detection is performed on a set of images, where N-1 images belong to the same category/have the same high-level features while one belongs to another category. Note that category does not necessarily have to relate to a class in a standard classification problem, but could be the combination of multiple features. For instance, on a face dataset, this could be people with glasses, male, beard, etc. An example of distinguishing different animals can be seen below. The first four images show foxes, while the last represents a different animal. We want to recognize that the last image shows a different animal, but it is not relevant which class of animal it is.\n\n","type":"content","url":"/labs/lab-6-transformers#set-anomaly-detection","position":13},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The data","lvl2":"Set Anomaly Detection"},"type":"lvl3","url":"/labs/lab-6-transformers#the-data-1","position":14},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The data","lvl2":"Set Anomaly Detection"},"content":"We will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. The larger amount of classes requires the model to attend to specific features in the images instead of coarse features as in CIFAR10, therefore making the task harder. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.\n\nUsing the raw images directly as input to the Transformer is not a good idea, because it is not translation invariant as a CNN. Use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images. Below, we will load the dataset.\n\n# ImageNet statistics\nDATA_MEANS = np.array([0.485, 0.456, 0.406])\nDATA_STD = np.array([0.229, 0.224, 0.225])\n# As torch tensors for later preprocessing\nTORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1,3,1,1)\nTORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1,3,1,1)\n\n# Resize to 224x224, and normalize to ImageNet statistic\ntransform = transforms.Compose([transforms.Resize((224,224)),\n                                transforms.ToTensor(),\n                                transforms.Normalize(DATA_MEANS, DATA_STD)\n                                ])\n# Loading the training dataset. \ntrain_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n\n# Loading the test set\ntest_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)\n\n\n\nNext, run a pre-trained ResNet model on the images, and extract the features before the classification layer. These are the most high-level features, and should sufficiently describe the images. As we don’t have a large enough dataset and want to train our model efficiently, it’s best to extract the features beforehand.\n\nThis is a validation set to detect when we should stop training. In this case, we will split the training set into 90% training, 10% validation in a balanced way.\n\n## Split train into train+val\n# Get labels from train set\nlabels = train_set.targets\n\n# Get indices of images per class\nlabels = torch.LongTensor(labels)\nnum_labels = labels.max()+1\nsorted_indices = torch.argsort(labels).reshape(num_labels, -1) # [classes, num_imgs per class]\n\n# Determine number of validation images per class\nnum_val_exmps = sorted_indices.shape[1] // 10\n\n# Get image indices for validation and training\nval_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\ntrain_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n\n# Group corresponding image features and labels\ntrain_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\nval_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]\n\n\n\nWe create a dataset class for the set anomaly task. We define an epoch to be the sequence in which each image has been exactly once as an “anomaly”. Hence, the length of the dataset is the number of images in it. For the training set, each time we access an item with __getitem__, we sample a random, different class than the image at the corresponding index idx has. In a second step, we sample N-1 images of this sampled class. The set of 10 images is finally returned. The randomness in the __getitem__ allows us to see a slightly different set during each iteration. However, we can’t use the same strategy for the test set as we want the test dataset to be the same every time we iterate over it. Hence, we sample the sets in the __init__ method, and return those in __getitem__. The code below implements exactly this dynamic.\n\nclass SetAnomalyDataset(data.Dataset):\n    \n    def __init__(self, img_feats, labels, set_size=10, train=True):\n        \"\"\"\n        Inputs:\n            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n            labels - Tensor of shape [num_imgs], containing the class labels for the images\n            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.\n            train - If True, a new set will be sampled every time __getitem__ is called.\n        \"\"\"\n        super().__init__()\n        self.img_feats = img_feats\n        self.labels = labels\n        self.set_size = set_size-1 # The set size is here the size of correct images\n        self.train = train\n        \n        # Tensors with indices of the images per class\n        self.num_labels = labels.max()+1\n        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n        \n        if not train:\n            self.test_sets = self._create_test_sets()\n            \n            \n    def _create_test_sets(self):\n        # Pre-generates the sets for each image for the test set\n        test_sets = []\n        num_imgs = self.img_feats.shape[0]\n        np.random.seed(42)\n        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n        test_sets = torch.stack(test_sets, dim=0)\n        return test_sets\n            \n        \n    def sample_img_set(self, anomaly_label):\n        \"\"\"\n        Samples a new set of images, given the label of the anomaly. \n        The sampled images come from a different class than anomaly_label\n        \"\"\"\n        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n        set_label = np.random.randint(self.num_labels-1)\n        if set_label >= anomaly_label:\n            set_label += 1\n            \n        # Sample images from the class determined above\n        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n        img_indices = self.img_idx_by_label[set_label, img_indices]\n        return img_indices\n        \n        \n    def __len__(self):\n        return self.img_feats.shape[0]\n    \n    \n    def __getitem__(self, idx):\n        anomaly = self.img_feats[idx]\n        if self.train: # If train => sample\n            img_indices = self.sample_img_set(self.labels[idx])\n        else: # If test => use pre-generated ones\n            img_indices = self.test_sets[idx]\n            \n        # Concatenate images. The anomaly is always the last image for simplicity\n        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n        label = img_set.shape[0]-1\n        \n        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n        return img_set, indices, label\n\n\n\nNext, we can setup our datasets and data loaders below. Here, we will use a set size of 10, i.e. 9 images from one category + 1 anomaly. Feel free to change it if you want to experiment with the sizes.\n\nSET_SIZE = 10\ntest_labels = torch.LongTensor(test_set.targets)\n\ntrain_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\nval_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\ntest_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n\ntrain_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=0, pin_memory=True)\nval_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=0)\ntest_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=0)\n\n\n\nTo understand the dataset a little better, we can plot below a few sets from the test dataset. Each row shows a different input set, where the first 9 are from the same class.\n\ndef visualize_exmp(indices, orig_dataset):\n    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n    images = torch.stack(images, dim=0)\n    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n    \n    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n    img_grid = img_grid.permute(1, 2, 0)\n\n    plt.figure(figsize=(12,8))\n    plt.title(\"Anomaly examples on CIFAR100\")\n    plt.imshow(img_grid)\n    plt.axis('off')\n    plt.show()\n    plt.close()\n\n_, indices, _ = next(iter(test_anom_loader))\nvisualize_exmp(indices[:4], test_set)\n\n\n\nWe can already see that for some sets the task might be easier than for others. Difficulties can especially arise if the anomaly is in a different, but yet visually similar class (e.g. train vs bus, flour vs worm, etc.).","type":"content","url":"/labs/lab-6-transformers#the-data-1","position":15},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The model","lvl2":"Set Anomaly Detection"},"type":"lvl3","url":"/labs/lab-6-transformers#the-model-1","position":16},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The model","lvl2":"Set Anomaly Detection"},"content":"Write a model to classify whole set. For the prediction to be permutation-equivariant, output one logit for each image. Over these logits, apply a softmax and train the anomaly image to have the highest score/probability.\n\n","type":"content","url":"/labs/lab-6-transformers#the-model-1","position":17},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The trainer","lvl2":"Set Anomaly Detection"},"type":"lvl3","url":"/labs/lab-6-transformers#the-trainer-1","position":18},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"The trainer","lvl2":"Set Anomaly Detection"},"content":"Finally, write your train function below. It can have the exact same structure as the reverse task one. Train your model.\n\n","type":"content","url":"/labs/lab-6-transformers#the-trainer-1","position":19},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"Visualization","lvl2":"Set Anomaly Detection"},"type":"lvl3","url":"/labs/lab-6-transformers#visualization-1","position":20},{"hierarchy":{"lvl1":"Lab 6: Transformers","lvl3":"Visualization","lvl2":"Set Anomaly Detection"},"content":"To interpret the model a little more, we plot the attention maps inside the model. This should give you an idea of what information the model is sharing/communicating between images, and what each head might represent. Write a plot function which plots the images in the input set, the prediction of the model, and the attention maps of the different heads on layers of the transformer. Feel free to explore the attention maps for different input examples as well.","type":"content","url":"/labs/lab-6-transformers#visualization-1","position":21},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial"},"type":"lvl1","url":"/labs/lab-6-tutorial","position":0},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial"},"content":"In this tutorial, we’ll reproduce, step by step, the model from the paper that first introduced the transformer architecture, \n\nAttention Is All You Need), albeit only the encoder part.\nAfter that, we’ll do a number of small experiments to visualize and better understand the inner workings.\n\n# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np \nimport math\nfrom functools import partial\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torchvision\nfrom torchvision.datasets import CIFAR100\nfrom torchvision import transforms\n\n# PyTorch Lightning\ntry:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n    !pip install --quiet pytorch-lightning>=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nDATASET_PATH = \"data\"\nCHECKPOINT_PATH = \"saved_models\"\n\n# Setting the seed\npl.seed_everything(42)\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = \"cpu\"\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nprint(\"Device:\", device)\n\n\n\n\n\n","type":"content","url":"/labs/lab-6-tutorial","position":1},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Attention mechanism"},"type":"lvl3","url":"/labs/lab-6-tutorial#attention-mechanism","position":2},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Attention mechanism"},"content":"The attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys. In other words, we want to dynamically decide on which inputs we want to “attend” more than others. To implement the attention mechanism, there are four parts we need to specify:\n\nQuery: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n\nKeys: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n\nValues: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n\nThe weights of the average are calculated by a softmax over all score function outputs.\n\nThe dot product attention takes as input a set of queries Q\\in\\mathbb{R}^{T\\times d_k}, keys K\\in\\mathbb{R}^{T\\times d_k} and values V\\in\\mathbb{R}^{T\\times d_v} where T is the sequence length, and d_k and d_v are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element i to j is based on its similarity of the query Q_i and key K_j, using the dot product as the similarity metric.\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\nThe matrix multiplication QK^T performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape T\\times T. Each row represents the attention logits for a specific element i to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - \n\nVaswani et al., 2017).\n\n\n\nThe scaling factor of 1/\\sqrt{d_k} is crucial to maintain an appropriate variance of attention values after initialization. The block Mask (opt.) in the diagram above represents the optional masking of specific entries in the attention matrix. This is usually done by setting the respective attention logits to a very low value.\n\nWe can write a function below which computes the output features given the triple of queries, keys, and values:\n\ndef scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n\n\n\nLet’s generate a few random queries, keys, and value vectors, and calculate the attention outputs:\n\nseq_len, d_k = 3, 2\npl.seed_everything(42)\nq = torch.randn(seq_len, d_k)\nk = torch.randn(seq_len, d_k)\nv = torch.randn(seq_len, d_k)\nvalues, attention = scaled_dot_product(q, k, v)\nprint(\"Q\\n\", q)\nprint(\"K\\n\", k)\nprint(\"V\\n\", v)\nprint(\"Values\\n\", values)\nprint(\"Attention\\n\", attention)\n\n\n\n\n\n","type":"content","url":"/labs/lab-6-tutorial#attention-mechanism","position":3},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Multi-Head Attention"},"type":"lvl3","url":"/labs/lab-6-tutorial#multi-head-attention","position":4},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Multi-Head Attention"},"content":"The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. We refer to this as Multi-Head Attention layer with the learnable parameters W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}, W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}, W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}, and W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}} (D being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - \n\nVaswani et al., 2017).\n\n\n\nHow are we applying a Multi-Head Attention layer in a neural network, where we don’t have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}, as Q, K and V (B being the batch size, T the sequence length, d_{\\text{model}} the hidden dimensionality of X). The consecutive weight matrices W^{Q}, W^{K}, and W^{V} can transform X to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below.\n\n# Helper function to support different mask shapes.\n# Output shape supports (batch_size, number of heads, seq length, seq length)\n# If 2D: broadcasted over batch size and number of heads\n# If 3D: broadcasted over number of heads\n# If 4D: leave as is\ndef expand_mask(mask):\n    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim < 4:\n        mask = mask.unsqueeze(0)\n    return mask\n\n\n\nclass MultiheadAttention(nn.Module):\n    \n    def __init__(self, input_dim, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n        self.o_proj = nn.Linear(embed_dim, input_dim)\n        \n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n        \n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n        \n        if return_attention:\n            return o, attention\n        else:\n            return o\n\n\n\n","type":"content","url":"/labs/lab-6-tutorial#multi-head-attention","position":5},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Transformer Encoder"},"type":"lvl3","url":"/labs/lab-6-tutorial#transformer-encoder","position":6},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Transformer Encoder"},"content":"Next, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner. We will focus here on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - \n\nVaswani et al., 2017).:\n\n\n\nThe encoder consists of N identical blocks that are applied in sequence. Taking as input x, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates \\text{LayerNorm}(x+\\text{Multihead}(x,x,x)) (x being Q, K and V input to the attention layer). The residual connection is crucial for enabling a smooth gradient flow through the model, and to make sure that the information about the original sequence isn’t lost. The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. Finally, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. The full transformation including the residual connection can be expressed as:\\begin{split}\n    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n\\end{split}\n\nFinally, we can start implementing the architecture below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization.\n\nclass EncoderBlock(nn.Module):\n    \n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Dimensionality of the input\n            num_heads - Number of heads to use in the attention block\n            dim_feedforward - Dimensionality of the hidden layer in the MLP\n            dropout - Dropout probability to use in the dropout layers\n        \"\"\"\n        super().__init__()\n        \n        # Attention layer\n        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n        \n        # Two-layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim)\n        )\n        \n        # Layers to apply in between the main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n        \n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n        \n        return x\n\n\n\nBased on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called get_attention_maps. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding the model later. However, the attention probabilities should be interpreted with a grain of salt as it \n\ndoes not necessarily reflect the true interpretation of the model.\n\nclass TransformerEncoder(nn.Module):\n    \n    def __init__(self, num_layers, **block_args):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for l in self.layers:\n            x = l(x, mask=mask)\n        return x\n\n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for l in self.layers:\n            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = l(x)\n        return attention_maps\n\n\n\n","type":"content","url":"/labs/lab-6-tutorial#transformer-encoder","position":7},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Positional encoding"},"type":"lvl3","url":"/labs/lab-6-tutorial#positional-encoding","position":8},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Positional encoding"},"content":"We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. However, we can use patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:PE_{(pos,i)} = \\begin{cases}\n    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n\\end{cases}\n\nPE_{(pos,i)} represents the position encoding at position pos in the sequence, and hidden dimensionality i. The intuition behind this encoding is that you can represent PE_{(pos+k,:)} as a linear function of PE_{(pos,:)}, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from 2\\pi to 10000\\cdot 2\\pi. The positional encoding is implemented below.\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x\n\n\n\nTo understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let’s do it below.\n\nencod_block = PositionalEncoding(d_model=48, max_len=96)\npe = encod_block.pe.squeeze().T.cpu().numpy()\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\npos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\nfig.colorbar(pos, ax=ax)\nax.set_xlabel(\"Position in sequence\")\nax.set_ylabel(\"Hidden dimension\")\nax.set_title(\"Positional encoding over hidden dimensions\")\nax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\nax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\nplt.show()\n\n\n\nYou can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions 1, 2, 3 and 4.\n\nsns.set_theme()\nfig, ax = plt.subplots(2, 2, figsize=(12,4))\nax = [a for a_list in ax for a in a_list]\nfor i in range(len(ax)):\n    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n    ax[i].set_xticks(np.arange(1,17))\n    ax[i].tick_params(axis='both', which='major', labelsize=10)\n    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n    ax[i].set_ylim(-1.2, 1.2)\nfig.subplots_adjust(hspace=0.8)\nsns.reset_orig()\nplt.show()\n\n\n\nAs we can see, the patterns between the hidden dimension 1 and 2 only differ in the starting angle. The wavelength is 2\\pi, hence the repetition after position 6. The hidden dimensions 2 and 3 have about twice the wavelength.\n\n","type":"content","url":"/labs/lab-6-tutorial#positional-encoding","position":9},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Learning rate warm-up"},"type":"lvl3","url":"/labs/lab-6-tutorial#learning-rate-warm-up","position":10},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"Learning rate warm-up"},"content":"One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations.\n\nclass CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n    \n    def __init__(self, optimizer, warmup, max_iters):\n        self.warmup = warmup\n        self.max_num_iters = max_iters\n        super().__init__(optimizer)\n        \n    def get_lr(self):\n        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n    \n    def get_lr_factor(self, epoch):\n        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n        if epoch <= self.warmup:\n            lr_factor *= epoch * 1.0 / self.warmup\n        return lr_factor\n\n\n\n# Needed for initializing the lr scheduler\np = nn.Parameter(torch.empty(4,4))\noptimizer = optim.Adam([p], lr=1e-3)\nlr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n\n# Plotting\nepochs = list(range(2000))\nsns.set()\nplt.figure(figsize=(8,3))\nplt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\nplt.ylabel(\"Learning rate factor\")\nplt.xlabel(\"Iterations (in batches)\")\nplt.title(\"Cosine Warm-up Learning Rate Scheduler\")\nplt.show()\nsns.reset_orig()\n\n\n\n","type":"content","url":"/labs/lab-6-tutorial#learning-rate-warm-up","position":11},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"PyTorch Lightning Module"},"type":"lvl3","url":"/labs/lab-6-tutorial#pytorch-lightning-module","position":12},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl3":"PyTorch Lightning Module"},"content":"Finally, we can embed the Transformer architecture into a PyTorch lightning module. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional [CLS] token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element.\n\nAdditionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). The training, validation, and test step is left empty for now and will be filled for our task-specific models.\n\nclass TransformerPredictor(pl.LightningModule):\n\n    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Hidden dimensionality of the input\n            model_dim - Hidden dimensionality to use inside the Transformer\n            num_classes - Number of classes to predict per sequence element\n            num_heads - Number of heads to use in the Multi-Head Attention blocks\n            num_layers - Number of encoder blocks to use.\n            lr - Learning rate in the optimizer\n            warmup - Number of warmup steps. Usually between 50 and 500\n            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n            dropout - Dropout to apply inside the model\n            input_dropout - Dropout to apply on the input features\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self._create_model()\n\n    def _create_model(self):\n        # Input dim -> Model dim\n        self.input_net = nn.Sequential(\n            nn.Dropout(self.hparams.input_dropout),\n            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n        )\n        # Positional encoding for sequences\n        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n        # Transformer\n        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n                                              input_dim=self.hparams.model_dim,\n                                              dim_feedforward=2*self.hparams.model_dim,\n                                              num_heads=self.hparams.num_heads,\n                                              dropout=self.hparams.dropout)\n        # Output classifier per sequence lement\n        self.output_net = nn.Sequential(\n            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n            nn.LayerNorm(self.hparams.model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n        ) \n\n    def forward(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Inputs:\n            x - Input features of shape [Batch, SeqLen, input_dim]\n            mask - Mask to apply on the attention outputs (optional)\n            add_positional_encoding - If True, we add the positional encoding to the input.\n                                      Might not be desired for some tasks.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)\n        x = self.output_net(x)\n        return x\n\n    @torch.no_grad()\n    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Function for extracting the attention matrices of the whole Transformer for a single batch.\n        Input arguments same as the forward pass.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n        return attention_maps\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n        \n        # Apply lr scheduler per step\n        lr_scheduler = CosineWarmupScheduler(optimizer, \n                                             warmup=self.hparams.warmup, \n                                             max_iters=self.hparams.max_iters)\n        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n\n    def training_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def validation_step(self, batch, batch_idx):\n        raise NotImplementedError    \n\n    def test_step(self, batch, batch_idx):\n        raise NotImplementedError   \n\n\n\nThat’s it for now. You are now ready to start the labs.\n\n","type":"content","url":"/labs/lab-6-tutorial#pytorch-lightning-module","position":13},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl2":"Further reading"},"type":"lvl2","url":"/labs/lab-6-tutorial#further-reading","position":14},{"hierarchy":{"lvl1":"Lab 6: Transformers Tutorial","lvl2":"Further reading"},"content":"Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n\nThe Illustrated Transformer (Jay Alammar, 2018) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n\nAttention? Attention! (Lilian Weng, 2018) - A nice blog post summarizing attention mechanisms in many domains including vision.\n\nIllustrated: Self-Attention (Raimi Karim, 2019) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n\nThe Transformer family (Lilian Weng, 2020) - A very detailed blog post reviewing more variants of Transformers besides the original one.\n\nThis tutorial was greatly inspired by the \n\nUvA Deep Learning tutorials","type":"content","url":"/labs/lab-6-tutorial#further-reading","position":15},{"hierarchy":{"lvl1":"Lecture 1. Introduction"},"type":"lvl1","url":"/notebooks/introduction","position":0},{"hierarchy":{"lvl1":"Lecture 1. Introduction"},"content":"A few useful things to know about machine learning\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots\nif interactive:\n    fig_scale = 1.5\nelse: # For printing\n    fig_scale = 0.3\n    plt.rcParams.update(print_config)\n\n\n\n","type":"content","url":"/notebooks/introduction","position":1},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Why Machine Learning?"},"type":"lvl2","url":"/notebooks/introduction#why-machine-learning","position":2},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Why Machine Learning?"},"content":"Search engines (e.g. Google)\n\nRecommender systems (e.g. Netflix)\n\nAutomatic translation (e.g. Google Translate)\n\nSpeech understanding (e.g. Siri, Alexa)\n\nGame playing (e.g. AlphaGo)\n\nSelf-driving cars\n\nPersonalized medicine\n\nProgress in all sciences: Genetics, astronomy, chemistry, neurology, physics,...\n\n","type":"content","url":"/notebooks/introduction#why-machine-learning","position":3},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"What is Machine Learning?"},"type":"lvl2","url":"/notebooks/introduction#what-is-machine-learning","position":4},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"What is Machine Learning?"},"content":"Learn to perform a task, based on experience (examples) X, minimizing error \\mathcal{E}\n\nE.g. recognizing a person in an image as accurately as possible\n\nOften, we want to learn a function (model) f with some model parameters \\theta that produces the right output yf_{\\theta}(X) = y\\underset{\\theta}{\\operatorname{argmin}} \\mathcal{E}(f_{\\theta}(X))\n\nUsually part of a much larger system that provides the data X in the right form\n\nData needs to be collected, cleaned, normalized, checked for data biases,...\n\n","type":"content","url":"/notebooks/introduction#what-is-machine-learning","position":5},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Inductive bias","lvl2":"What is Machine Learning?"},"type":"lvl3","url":"/notebooks/introduction#inductive-bias","position":6},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Inductive bias","lvl2":"What is Machine Learning?"},"content":"In practice, we have to put assumptions into the model: inductive bias b\n\nWhat should the model look like?\n\nMimick human brain: Neural Networks\n\nLogical combination of inputs: Decision trees, Linear models\n\nRemember similar examples: Nearest Neighbors, SVMs\n\nProbability distribution: Bayesian models\n\nUser-defined settings (hyperparameters)\n\nE.g. depth of tree, network architecture\n\nAssuptions about the data distribution, e.g. X \\sim N(\\mu,\\sigma)\n\nWe can transfer knowledge from previous tasks: f_1, f_2, f_3, ... \\Longrightarrow f_{new}\n\nChoose the right model, hyperparameters\n\nReuse previously learned values for model parameters \\theta\n\nIn short:\\underset{\\theta,b}{\\operatorname{argmin}} \\mathcal{E}(f_{\\theta, b}(X))\n\n","type":"content","url":"/notebooks/introduction#inductive-bias","position":7},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Machine learning vs Statistics","lvl2":"What is Machine Learning?"},"type":"lvl3","url":"/notebooks/introduction#machine-learning-vs-statistics","position":8},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Machine learning vs Statistics","lvl2":"What is Machine Learning?"},"content":"See Breiman (2001): Statistical modelling: The two cultures\n\nBoth aim to make predictions of natural phenomena:\n\n\nStatistics:\n\nHelp humans understand the world\n\nAssume data is generated according to an understandable model\n\n\nMachine learning:\n\nAutomate a task entirely (partially replace the human)\n\nAssume that the data generation process is unknown\n\nEngineering-oriented, less (too little?) mathematical theory\n\n\n","type":"content","url":"/notebooks/introduction#machine-learning-vs-statistics","position":9},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Types of machine learning"},"type":"lvl2","url":"/notebooks/introduction#types-of-machine-learning","position":10},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Types of machine learning"},"content":"Supervised Learning: learn a model f from labeled data (X,y) (ground truth)\n\nGiven a new input X, predict the right output y\n\nGiven examples of stars and galaxies, identify new objects in the sky\n\nUnsupervised Learning: explore the structure of the data (X) to extract meaningful information\n\nGiven inputs X, find which ones are special, similar, anomalous, ...\n\nSemi-Supervised Learning: learn a model from (few) labeled and (many) unlabeled examples\n\nUnlabeled examples add information about which new examples are likely to occur\n\nReinforcement Learning: develop an agent that improves its performance based on interactions with the environment\n\nNote: Practical ML systems can combine many types in one system.\n\n","type":"content","url":"/notebooks/introduction#types-of-machine-learning","position":11},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl3","url":"/notebooks/introduction#supervised-machine-learning","position":12},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"content":"Learn a model from labeled training data, then make predictions\n\nSupervised: we know the correct/desired outcome (label)\n\nSubtypes: classification (predict a class) and regression (predict a numeric value)\n\nMost supervised algorithms that we will see can do both\n\n\n\n","type":"content","url":"/notebooks/introduction#supervised-machine-learning","position":13},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Classification","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl4","url":"/notebooks/introduction#classification","position":14},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Classification","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"content":"Predict a class label (category), discrete and unordered\n\nCan be binary (e.g. spam/not spam) or multi-class (e.g. letter recognition)\n\nMany classifiers can return a confidence per class\n\nThe predictions of the model yield a decision boundary separating the classes\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_moons\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# create a synthetic dataset\nX1, y1 = make_moons(n_samples=70, noise=0.2, random_state=8)\n\n# Train classifiers\nlr = LogisticRegression().fit(X1, y1)\nsvm = SVC(kernel='rbf', gamma=2, probability=True).fit(X1, y1)\nknn = KNeighborsClassifier(n_neighbors=3).fit(X1, y1)\n\n# Plotting\n@interact\ndef plot_classifier(classifier=[lr,svm,knn]):  \n    fig, axes = plt.subplots(1, 2, figsize=(12*fig_scale, 4*fig_scale))\n    mglearn.tools.plot_2d_separator(\n        classifier, X1, ax=axes[0], alpha=.4, cm=mglearn.cm2)\n    scores_image = mglearn.tools.plot_2d_scores(\n        classifier, X1, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n    for ax in axes:\n        mglearn.discrete_scatter(X1[:, 0], X1[:, 1], y1,\n                                 markers='.', ax=ax)\n        ax.set_xlabel(\"Feature 0\")\n        ax.set_ylabel(\"Feature 1\", labelpad=0)\n        ax.tick_params(axis='y', pad=0)\n\n    cbar = plt.colorbar(scores_image, ax=axes.tolist())\n    cbar.set_label('Predicted probability', rotation=270, labelpad=6)\n    cbar.set_alpha(1)\n    axes[0].legend([\"Class 0\", \"Class 1\"], ncol=4, loc=(.1, 1.1));\n    plt.show()\n\n\n\nif not interactive:\n    plot_classifier(classifier=svm)\n\n\n\n","type":"content","url":"/notebooks/introduction#classification","position":15},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl5":"Example: Flower classification","lvl4":"Classification","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl5","url":"/notebooks/introduction#example-flower-classification","position":16},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl5":"Example: Flower classification","lvl4":"Classification","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"content":"Classify types of Iris flowers (setosa, versicolor, or virginica). How would you do it?\n\n","type":"content","url":"/notebooks/introduction#example-flower-classification","position":17},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl5":"Representation: input features and labels","lvl4":"Classification","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl5","url":"/notebooks/introduction#representation-input-features-and-labels","position":18},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl5":"Representation: input features and labels","lvl4":"Classification","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"content":"We could take pictures and use them (pixel values) as inputs (-> Deep Learning)\n\nWe can manually define a number of input features (variables), e.g. length and width of leaves\n\nEvery `example’ is a point in a (possibly high-dimensional) space \n\n","type":"content","url":"/notebooks/introduction#representation-input-features-and-labels","position":19},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Regression","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl4","url":"/notebooks/introduction#regression","position":20},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Regression","lvl3":"Supervised Machine Learning","lvl2":"Types of machine learning"},"content":"Predict a continuous value, e.g. temperature\n\nTarget variable is numeric\n\nSome algorithms can return a confidence interval\n\nFind the relationship between predictors and the target.\n\nfrom mglearn.datasets import make_wave\nfrom mglearn.plot_helpers import cm2\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nX2, y2 = make_wave(n_samples=60)\nx = np.atleast_2d(np.linspace(-3, 3, 100)).T\nlr = LinearRegression().fit(X2, y2)\nridge = BayesianRidge().fit(X2, y2)\ngp = GaussianProcessRegressor(kernel=RBF(10, (1e-2, 1e2)), n_restarts_optimizer=9, alpha=0.1, normalize_y=True).fit(X2, y2)\n\n@interact\ndef plot_regression(regressor=[lr, ridge, gp]):\n    line = np.linspace(-3, 3, 100).reshape(-1, 1)\n    plt.figure(figsize=(5*fig_scale, 5*fig_scale))\n    plt.plot(X2, y2, 'o', c=cm2(0))\n    if(regressor.__class__.__name__ == 'LinearRegression'):\n        y_pred = regressor.predict(x)\n    else:\n        y_pred, sigma = regressor.predict(x, return_std=True)\n        plt.fill(np.concatenate([x, x[::-1]]),\n             np.concatenate([y_pred - 1.9600 * sigma,\n                            (y_pred + 1.9600 * sigma)[::-1]]),\n             alpha=.5, fc='b', ec='None', label='95% confidence interval')\n        \n    plt.plot(line, y_pred, 'b-')\n    plt.xlabel(\"Input feature 1\")\n    plt.ylabel(\"Target\")\n    plt.show()\n\n\n\nif not interactive:\n    plot_regression(regressor=gp)\n\n\n\n","type":"content","url":"/notebooks/introduction#regression","position":21},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Unsupervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl3","url":"/notebooks/introduction#unsupervised-machine-learning","position":22},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Unsupervised Machine Learning","lvl2":"Types of machine learning"},"content":"Unlabeled data, or data with unknown structure\n\nExplore the structure of the data to extract information\n\nMany types, we’ll just discuss two.\n\n","type":"content","url":"/notebooks/introduction#unsupervised-machine-learning","position":23},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Clustering","lvl3":"Unsupervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl4","url":"/notebooks/introduction#clustering","position":24},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Clustering","lvl3":"Unsupervised Machine Learning","lvl2":"Types of machine learning"},"content":"Organize information into meaningful subgroups (clusters)\n\nObjects in cluster share certain degree of similarity (and dissimilarity to other clusters)\n\nExample: distinguish different types of customers\n\n# Note: the most recent versions of numpy seem to cause problems for KMeans\n# Uninstalling and installing the latest version of threadpoolctl fixes this\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nnr_samples = 1500\n\n@interact\ndef plot_clusters(randomize=(1,100,1)):\n    # Generate data\n    X, y = make_blobs(n_samples=nr_samples, cluster_std=[1.0, 1.5, 0.5], random_state=randomize)\n    # Cluster\n    y_pred = KMeans(n_clusters=3, random_state=randomize).fit_predict(X)\n    # PLot\n    plt.figure(figsize=(5*fig_scale, 5*fig_scale))\n    plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n    plt.title(\"KMeans Clusters\")\n    plt.xlabel(\"Feature 0\")\n    plt.ylabel(\"Feature 1\")\n    plt.show()\n\n\n\nif not interactive:\n    plot_clusters(randomize=2)\n\n\n\n","type":"content","url":"/notebooks/introduction#clustering","position":25},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Dimensionality reduction","lvl3":"Unsupervised Machine Learning","lvl2":"Types of machine learning"},"type":"lvl4","url":"/notebooks/introduction#dimensionality-reduction","position":26},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Dimensionality reduction","lvl3":"Unsupervised Machine Learning","lvl2":"Types of machine learning"},"content":"Data can be very high-dimensional and difficult to understand, learn from, store,...\n\nDimensionality reduction can compress the data into fewer dimensions, while retaining most of the information\n\nContrary to feature selection, the new features lose their (original) meaning\n\nThe new representation can be a lot easier to model (and visualize)\n\nfrom sklearn.datasets import make_swiss_roll\nfrom sklearn.manifold import locally_linear_embedding\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\nX, color = make_swiss_roll(n_samples=800, random_state=123)\n\nfig = plt.figure(figsize=plt.figaspect(0.3)*fig_scale*2.5)\nax1 = fig.add_subplot(1, 3, 1, projection='3d')\nax1.xaxis.pane.fill = False\nax1.yaxis.pane.fill = False\nax1.zaxis.pane.fill = False\nax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.rainbow, s=10*fig_scale)\nplt.title('Swiss Roll in 3D')\n\nax2 = fig.add_subplot(1, 3, 2)\nscikit_pca = PCA(n_components=2)\nX_spca = scikit_pca.fit_transform(X)\nplt.scatter(X_spca[:, 0], X_spca[:, 1], c=color, cmap=plt.cm.rainbow)\nplt.title('PCA');\n\nax3 = fig.add_subplot(1, 3, 3)\nX_lle, err = locally_linear_embedding(X, n_neighbors=12, n_components=2)\nplt.scatter(X_lle[:, 0], X_lle[:, 1], c=color, cmap=plt.cm.rainbow)\nplt.title('Locally Linear Embedding');\n\n\n\n","type":"content","url":"/notebooks/introduction#dimensionality-reduction","position":27},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Reinforcement learning","lvl2":"Types of machine learning"},"type":"lvl3","url":"/notebooks/introduction#reinforcement-learning","position":28},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Reinforcement learning","lvl2":"Types of machine learning"},"content":"Develop an agent that improves its performance based on interactions with the environment\n\nExample: games like Chess, Go,...\n\nSearch a (large) space of actions and states\n\nReward function defines how well a (series of) actions works\n\nLearn a series of actions (policy) that maximizes reward through exploration\n\n\n\n","type":"content","url":"/notebooks/introduction#reinforcement-learning","position":29},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Learning = Representation + evaluation + optimization"},"type":"lvl2","url":"/notebooks/introduction#learning-representation-evaluation-optimization","position":30},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Learning = Representation + evaluation + optimization"},"content":"All machine learning algorithms consist of 3 components:\n\nRepresentation: A model f_{\\theta} must be represented in a formal language that the computer can handle\n\nDefines the ‘concepts’ it can learn, the hypothesis space\n\nE.g. a decision tree, neural network, set of annotated data points\n\nEvaluation: An internal way to choose one hypothesis over the other\n\nObjective function, scoring function, loss function \\mathcal{L}(f_{\\theta})\n\nE.g. Difference between correct output and predictions\n\nOptimization: An efficient way to search the hypothesis space\n\nStart from simple hypothesis, extend (relax) if it doesn’t fit the data\n\nStart with initial set of model parameters, gradually refine them\n\nMany methods, differing in speed of learning, number of optima,...\n\nA powerful/flexible model is only useful if it can also be optimized efficiently\n\n","type":"content","url":"/notebooks/introduction#learning-representation-evaluation-optimization","position":31},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Neural networks: representation","lvl2":"Learning = Representation + evaluation + optimization"},"type":"lvl3","url":"/notebooks/introduction#neural-networks-representation","position":32},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Neural networks: representation","lvl2":"Learning = Representation + evaluation + optimization"},"content":"Let’s take neural networks as an example\n\nRepresentation: (layered) neural network\n\nEach connection has a weight \\theta_i (a.k.a. model parameters)\n\nEach node receives weighted inputs, emits new value\n\nModel f returns the output of the last layer\n\nThe architecture, number/type of neurons, etc. are fixed\n\nWe call these hyperparameters (set by user, fixed during training)\n\n","type":"content","url":"/notebooks/introduction#neural-networks-representation","position":33},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Neural networks: evaluation and optimization","lvl2":"Learning = Representation + evaluation + optimization"},"type":"lvl3","url":"/notebooks/introduction#neural-networks-evaluation-and-optimization","position":34},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Neural networks: evaluation and optimization","lvl2":"Learning = Representation + evaluation + optimization"},"content":"Representation: Given the structure, the model is represented by its parameters\n\nImagine a mini-net with two weights (\\theta_0,\\theta_1): a 2-dimensional search space\n\nEvaluation: A loss function \\mathcal{L}(\\theta) computes how good the predictions are\n\nEstimated on a set of training data with the ‘correct’ predictions\n\nWe can’t see the full surface, only evaluate specific sets of parameters\n\nOptimization: Find the optimal set of parameters\n\nUsually a type of search in the hypothesis space\n\nE.g. Gradient descent: \\theta_i^{new} = \\theta_i - \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta_i} \n\n","type":"content","url":"/notebooks/introduction#neural-networks-evaluation-and-optimization","position":35},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Overfitting and Underfitting"},"type":"lvl2","url":"/notebooks/introduction#overfitting-and-underfitting","position":36},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Overfitting and Underfitting"},"content":"It’s easy to build a complex model that is 100% accurate on the training data, but very bad on new data\n\nOverfitting: building a model that is too complex for the amount of data you have\n\nYou model peculiarities in your training data (noise, biases,...)\n\nSolve by making model simpler (regularization), or getting more data\n\nMost algorithms have hyperparameters that allow regularization\n\nUnderfitting: building a model that is too simple given the complexity of the data\n\nUse a more complex model\n\nThere are techniques for detecting overfitting (e.g. bias-variance analysis). More about that later\n\nYou can build ensembles of many models to overcome both underfitting and overfitting\n\nThere is often a sweet spot that you need to find by optimizing the choice of algorithms and hyperparameters, or using more data.\n\nExample: regression using polynomial functions\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\nn_samples = 30\nX3 = np.sort(np.random.rand(n_samples))\ny3 = true_fun(X3) + np.random.randn(n_samples) * 0.1\nX3_test = np.linspace(0, 1, 100)\nscores_x, scores_y = [], []\n\nshow_output = True\n\n@interact\ndef plot_poly(degrees = (1, 16, 1)):\n    polynomial_features = PolynomialFeatures(degree=degrees,\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n                         (\"linear_regression\", linear_regression)])\n    pipeline.fit(X3[:, np.newaxis], y3)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(pipeline, X3[:, np.newaxis], y3,\n                             scoring=\"neg_mean_squared_error\", cv=10)   \n    scores_x.append(degrees)\n    scores_y.append(-scores.mean())\n\n    if show_output:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12*fig_scale, 4*fig_scale))    \n        ax1.plot(X3_test, pipeline.predict(X3_test[:, np.newaxis]), label=\"Model\")\n        ax1.plot(X3_test, true_fun(X3_test), label=\"True function\")\n        ax1.scatter(X3, y3, edgecolor='b', label=\"Samples\")\n        ax1.set_xlabel(\"x\")\n        ax1.set_ylabel(\"y\")\n        ax1.set_xlim((0, 1))\n        ax1.set_ylim((-2, 2))\n        ax1.legend(loc=\"best\")\n        ax1.set_title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n            degrees, -scores.mean(), scores.std()))\n\n        # Plot scores\n        ax2.scatter(scores_x, scores_y, edgecolor='b')\n        order = np.argsort(scores_x)\n        ax2.plot(np.array(scores_x)[order], np.array(scores_y)[order])\n        ax2.set_xlim((0, 16))\n        ax2.set_ylim((10**-2, 10**11))\n        ax2.set_xlabel(\"degree\")\n        ax2.set_ylabel(\"error\", labelpad=0)\n        ax2.set_yscale(\"log\")\n\n        plt.show()\n\n\n\nfrom IPython.display import clear_output\nfrom ipywidgets import IntSlider, Output\n\nif not interactive:\n    show_output = False\n    for i in range(1,15):\n        plot_poly(degrees = i)\n    \n    show_output = True\n    plot_poly(degrees = 15)\n\n\n\n","type":"content","url":"/notebooks/introduction#overfitting-and-underfitting","position":37},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Model selection","lvl2":"Overfitting and Underfitting"},"type":"lvl3","url":"/notebooks/introduction#model-selection","position":38},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Model selection","lvl2":"Overfitting and Underfitting"},"content":"Next to the (internal) loss function, we need an (external) evaluation function\n\nFeedback signal: are we actually learning the right thing?\n\nAre we under/overfitting?\n\nCarefully choose to fit the application.\n\nNeeded to select between models (and hyperparameter settings)\n\n© XKCD\n\n\nData needs to be split into training and test sets\n\nOptimize model parameters on the training set, evaluate on independent test set\n\nAvoid data leakage:\n\nNever optimize hyperparameter settings on the test data\n\nNever choose preprocessing techniques based on the test data\n\nTo optimize hyperparameters and preprocessing as well, set aside part of training set as a validation set\n\nKeep test set hidden during all training\n\nimport mglearn\nmglearn.plots.plot_threefold_split()\n\n\n\nFor a given hyperparameter setting, learn the model parameters on training set\n\nMinize the loss\n\nEvaluate the trained model on the validation set\n\nTune the hyperparameters to maximize a certain metric (e.g. accuracy)\n\n","type":"content","url":"/notebooks/introduction#model-selection","position":39},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Only generalization counts!","lvl2":"Overfitting and Underfitting"},"type":"lvl3","url":"/notebooks/introduction#only-generalization-counts","position":40},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Only generalization counts!","lvl2":"Overfitting and Underfitting"},"content":"Never evaluate your final models on the training data, except for:\n\nTracking whether the optimizer converges (learning curves)\n\nDiagnosing under/overfitting:\n\nLow training and test score: underfitting\n\nHigh training score, low test score: overfitting\n\nAlways keep a completely independent test set\n\nOn small datasets, use multiple train-test splits to avoid sampling bias\n\nYou could sample an ‘easy’ test set by accident\n\nE.g. Use cross-validation (see later)\n\n","type":"content","url":"/notebooks/introduction#only-generalization-counts","position":41},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Better data representations, better models"},"type":"lvl2","url":"/notebooks/introduction#better-data-representations-better-models","position":42},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Better data representations, better models"},"content":"Algorithm needs to correctly transform the inputs to the right outputs\n\nA lot depends on how we present the data to the algorithm\n\nTransform data to better representation (a.k.a. encoding or embedding)\n\nCan be done end-to-end (e.g. deep learning) or by first ‘preprocessing’ the data (e.g. feature selection/generation)\n\n","type":"content","url":"/notebooks/introduction#better-data-representations-better-models","position":43},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Feature engineering","lvl2":"Better data representations, better models"},"type":"lvl3","url":"/notebooks/introduction#feature-engineering","position":44},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Feature engineering","lvl2":"Better data representations, better models"},"content":"Most machine learning techniques require humans to build a good representation of the data\n\nEspecially when data is naturally structured (e.g. table with meaningful columns)\n\nFeature engineering is often still necessary to get the best results\n\nFeature selection, dimensionality reduction, scaling, ...\n\nApplied machine learning is basically feature engineering (Andrew Ng)\n\nNothing beats domain knowledge (when available) to get a good representation\n\nE.g. Iris data: leaf length/width separate the classes well\n\nBuild prototypes early-on\n\n","type":"content","url":"/notebooks/introduction#feature-engineering","position":45},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Learning data transformations end-to-end","lvl2":"Better data representations, better models"},"type":"lvl3","url":"/notebooks/introduction#learning-data-transformations-end-to-end","position":46},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Learning data transformations end-to-end","lvl2":"Better data representations, better models"},"content":"For unstructured data (e.g. images, text), it’s hard to extract good features\n\nDeep learning: learn your own representation (embedding) of the data\n\nThrough multiple layers of representation (e.g. layers of neurons)\n\nEach layer transforms the data a bit, based on what reduces the error\n\n","type":"content","url":"/notebooks/introduction#learning-data-transformations-end-to-end","position":47},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Example: digit classification","lvl3":"Learning data transformations end-to-end","lvl2":"Better data representations, better models"},"type":"lvl4","url":"/notebooks/introduction#example-digit-classification","position":48},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Example: digit classification","lvl3":"Learning data transformations end-to-end","lvl2":"Better data representations, better models"},"content":"Input pixels go in, each layer transforms them to an increasingly informative representation for the given task\n\nOften less intuitive for humans\n\n","type":"content","url":"/notebooks/introduction#example-digit-classification","position":49},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Curse of dimensionality","lvl2":"Better data representations, better models"},"type":"lvl3","url":"/notebooks/introduction#curse-of-dimensionality","position":50},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl3":"Curse of dimensionality","lvl2":"Better data representations, better models"},"content":"Just adding lots of features and letting the model figure it out doesn’t work\n\nOur assumptions (inductive biases) often fail in high dimensions:\n\nRandomly sample points in an n-dimensional space (e.g. a unit hypercube)\n\nAlmost all points become outliers at the edge of the space\n\nDistances between any two points will become almost identical\n\n# Code originally by Peter Norvig \ndef sample(d=2, N=100):\n    return [[np.random.uniform(0., 1.) for i in range(d)] for _ in range(N)]\n\ndef corner_count(points):\n    return np.mean([any([(d < .01 or d > .99) for d in p]) for p in points])\n\ndef go(Ds=range(1,200)):\n    plt.figure(figsize=(5*fig_scale, 4*fig_scale))\n    plt.plot(Ds, [corner_count(sample(d)) for d in Ds])\n    plt.xlabel(\"Number of dimensions\")\n    plt.ylabel(\"Proportion of point that are 1% outliers\")\n    \ngo()\n\n\n\n","type":"content","url":"/notebooks/introduction#curse-of-dimensionality","position":51},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Practical consequences","lvl3":"Curse of dimensionality","lvl2":"Better data representations, better models"},"type":"lvl4","url":"/notebooks/introduction#practical-consequences","position":52},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl4":"Practical consequences","lvl3":"Curse of dimensionality","lvl2":"Better data representations, better models"},"content":"For every dimension (feature) you add, you need exponentially more data to avoid sparseness\n\nAffects any algorithm that is based on distances (e.g. kNN, SVM, kernel-based methods, tree-based methods,...)\n\nBlessing of non-uniformity: on many applications, the data lives in a very small subspace\n\nYou can drastically improve performance by selecting features or using lower-dimensional data representations\n\n","type":"content","url":"/notebooks/introduction#practical-consequences","position":53},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"“More data can beat a cleverer algorithm”"},"type":"lvl2","url":"/notebooks/introduction#more-data-can-beat-a-cleverer-algorithm","position":54},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"“More data can beat a cleverer algorithm”"},"content":"(but you need both)\n\nMore data reduces the chance of overfitting\n\nLess sparse data reduces the curse of dimensionality\n\nNon-parametric models: number of model parameters grows with amount of data\n\nTree-based techniques, k-Nearest neighbors, SVM,...\n\nThey can learn any model given sufficient data (but can get stuck in local minima)\n\nParametric (fixed size) models: fixed number of model parameters\n\nLinear models, Neural networks,...\n\nCan be given a huge number of parameters to benefit from more data\n\nDeep learning models can have millions of weights, learn almost any function.\n\nThe bottleneck is moving from data to compute/scalability\n\n","type":"content","url":"/notebooks/introduction#more-data-can-beat-a-cleverer-algorithm","position":55},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Building machine learning systems"},"type":"lvl2","url":"/notebooks/introduction#building-machine-learning-systems","position":56},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Building machine learning systems"},"content":"A typical machine learning system has multiple components, which we will cover in upcoming lectures:\n\nPreprocessing: Raw data is rarely ideal for learning\n\nFeature scaling: bring values in same range\n\nEncoding: make categorical features numeric\n\nDiscretization: make numeric features categorical\n\nLabel imbalance correction (e.g. downsampling)\n\nFeature selection: remove uninteresting/correlated features\n\nDimensionality reduction can also make data easier to learn\n\nUsing pre-learned embeddings (e.g. word-to-vector, image-to-vector)\n\nLearning and evaluation\n\nEvery algorithm has its own biases\n\nNo single algorithm is always best\n\nModel selection compares and selects the best models\n\nDifferent algorithms, different hyperparameter settings\n\nSplit data in training, validation, and test sets\n\nPrediction\n\nFinal optimized model can be used for prediction\n\nExpected performance is performance measured on independent test set\n\nTogether they form a workflow of pipeline\n\nThere exist machine learning methods to automatically build and tune these pipelines\n\nYou need to optimize pipelines continuously\n\nConcept drift: the phenomenon you are modelling can change over time\n\nFeedback: your model’s predictions may change future data\n\n","type":"content","url":"/notebooks/introduction#building-machine-learning-systems","position":57},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/introduction#summary","position":58},{"hierarchy":{"lvl1":"Lecture 1. Introduction","lvl2":"Summary"},"content":"Learning algorithms contain 3 components:\n\nRepresentation: a model f that maps input data X to desired output y\n\nContains model parameters \\theta that can be made to fit the data X\n\nLoss function \\mathcal{L}(f_{\\theta}(X)): measures how well the model fits the data\n\nOptimization technique to find the optimal \\theta: \\underset{\\theta}{\\operatorname{argmin}} \\mathcal{L}(f_{\\theta}(X))\n\nSelect the right model, then fit it to the data to minimize a task-specific error \\mathcal{E}\n\nInductive bias b: assumptions about model and hyperparameters\\underset{\\theta,b}{\\operatorname{argmin}} \\mathcal{E}(f_{\\theta, b}(X))\n\nOverfitting: model fits the training data well but not new (test) data\n\nSplit the data into (multiple) train-validation-test splits\n\nRegularization: tune hyperparameters (on validation set) to simplify model\n\nGather more data, or build ensembles of models\n\nMachine learning pipelines: preprocessing + learning + deployment","type":"content","url":"/notebooks/introduction#summary","position":59},{"hierarchy":{"lvl1":"Lecture 2. Linear models"},"type":"lvl1","url":"/notebooks/linear-models","position":0},{"hierarchy":{"lvl1":"Lecture 2. Linear models"},"content":"Basics of modeling, optimization, and regularization\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = False # Set to True for interactive plots\nif interactive:\n    fig_scale = 0.5\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.3\n    plt.rcParams.update(print_config)\n\n\n\n","type":"content","url":"/notebooks/linear-models","position":1},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Notation and Definitions"},"type":"lvl3","url":"/notebooks/linear-models#notation-and-definitions","position":2},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Notation and Definitions"},"content":"A scalar is a simple numeric value, denoted by an italic letter: x=3.24\n\nA vector is a 1D ordered array of n scalars, denoted by a bold letter: \\mathbf{x}=[3.24, 1.2]\n\nx_i denotes the ith element of a vector, thus x_0 = 3.24.\n\nNote: some other courses use x^{(i)} notation\n\nA set is an unordered collection of unique elements, denote by caligraphic capital: \\mathcal{S}=\\{3.24, 1.2\\}\n\nA matrix is a 2D array of scalars, denoted by bold capital: \\mathbf{X}=\\begin{bmatrix}\n3.24 & 1.2 \\\\\n2.24 & 0.2 \n\\end{bmatrix}\n\n\\textbf{X}_{i} denotes the ith row of the matrix\n\n\\textbf{X}_{:,j} denotes the jth column\n\n\\textbf{X}_{i,j} denotes the element in the ith row, jth column, thus \\mathbf{X}_{1,0} = 2.24\n\n\\mathbf{X}^{n \\times p}, an n \\times p matrix, can represent n data points in a p-dimensional space\n\nEvery row is a vector that can represent a point in an p-dimensional space, given a basis.\n\nThe standard basis for a Euclidean space is the set of unit vectors\n\nE.g. if \\mathbf{X}=\\begin{bmatrix}\n3.24 & 1.2 \\\\\n2.24 & 0.2 \\\\\n3.0 & 0.6 \n\\end{bmatrix}\n\nX = np.array([[3.24 , 1.2 ],[2.24, 0.2],[3.0 , 0.6 ]]) \nfig = plt.figure(figsize=(5*fig_scale,4*fig_scale))\nplt.scatter(X[:,0],X[:,1]);\nfor i in range(3):\n    plt.annotate(i, (X[i,0]+0.02, X[i,1]))\n\n\n\nA tensor is an k-dimensional array of data, denoted by an italic capital: T\n\nk is also called the order, degree, or rank\n\nT_{i,j,k,...} denotes the element or sub-tensor in the corresponding position\n\nA set of color images can be represented by:\n\na 4D tensor (sample x height x width x color channel)\n\na 2D tensor (sample x flattened vector of pixel values)\n\n","type":"content","url":"/notebooks/linear-models#notation-and-definitions","position":3},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Basic operations","lvl3":"Notation and Definitions"},"type":"lvl4","url":"/notebooks/linear-models#basic-operations","position":4},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Basic operations","lvl3":"Notation and Definitions"},"content":"Sums and products are denoted by capital Sigma and capital Pi:\\sum_{i=0}^{p} = x_0 + x_1 + ... + x_p \\quad \\prod_{i=0}^{p} = x_0 \\cdot x_1 \\cdot ... \\cdot x_p\n\nOperations on vectors are element-wise: e.g. \\mathbf{x}+\\mathbf{z} = [x_0+z_0,x_1+z_1, ... , x_p+z_p]\n\nDot product \\mathbf{w}\\mathbf{x} = \\mathbf{w} \\cdot \\mathbf{x} = \\mathbf{w}^{T} \\mathbf{x} = \\sum_{i=0}^{p} w_i \\cdot x_i = w_0 \\cdot x_0 + w_1 \\cdot x_1 + ... + w_p \\cdot x_p\n\nMatrix product \\mathbf{W}\\mathbf{x} = \\begin{bmatrix}\n\\mathbf{w_0} \\cdot \\mathbf{x} \\\\\n... \\\\\n\\mathbf{w_p} \\cdot \\mathbf{x} \\end{bmatrix}\n\nA function f(x) = y relates an input element x to an output y\n\nIt has a local minimum at x=c if f(x) \\geq f(c) in interval (c-\\epsilon, c+\\epsilon)\n\nIt has a global minimum at x=c if f(x) \\geq f(c) for any value for x\n\nA vector function consumes an input and produces a vector: \\mathbf{f}(\\mathbf{x}) = \\mathbf{y}\n\n\\underset{x\\in X}{\\operatorname{max}}f(x) returns the largest value f(x) for any x\n\n\\underset{x\\in X}{\\operatorname{argmax}}f(x) returns the element x that maximizes f(x)\n\n","type":"content","url":"/notebooks/linear-models#basic-operations","position":5},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Gradients","lvl3":"Notation and Definitions"},"type":"lvl4","url":"/notebooks/linear-models#gradients","position":6},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Gradients","lvl3":"Notation and Definitions"},"content":"A derivative f' of a function f describes how fast f grows or decreases\n\nThe process of finding a derivative is called differentiation\n\nDerivatives for basic functions are known\n\nFor non-basic functions we use the chain rule: F(x) = f(g(x)) \\rightarrow F'(x)=f'(g(x))g'(x)\n\nA function is differentiable if it has a derivative in any point of it’s domain\n\nIt’s continuously differentiable if f' is a continuous function\n\nWe say f is smooth if it is infinitely differentiable, i.e., f', f'', f''', ... all exist\n\nA gradient \\nabla f is the derivative of a function in multiple dimensions\n\nIt is a vector of partial derivatives: \\nabla f = \\left[ \\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1},... \\right]\n\nE.g. f=2x_0+3x_1^{2}-\\sin(x_2) \\rightarrow \\nabla f= [2, 6x_1, -cos(x_2)]\n\nExample: f = -(x_0^2+x_1^2)\n\n\\nabla f = \\left[\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1}\\right] = \\left[-2x_0,-2x_1\\right]\n\nEvaluated at point (-4,1): \\nabla f(-4,1) = [8,-2]\n\nThese are the slopes at point (-4,1) in the direction of x_0 and x_1 respectively\n\nfrom mpl_toolkits import mplot3d\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# f = -(x0^2 + x1^2)\ndef g_f(x0, x1):\n    return -(x0 ** 2 + x1 ** 2)\ndef g_dfx0(x0):\n    return -2 * x0\ndef g_dfx1(x1):\n    return -2 * x1\n\n@interact\ndef plot_gradient(rotation=(0,240,10)):\n    # plot surface of f\n    fig = plt.figure(figsize=(12*fig_scale,5*fig_scale))\n    ax = plt.axes(projection=\"3d\")\n    x0 = np.linspace(-6, 6, 30)\n    x1 = np.linspace(-6, 6, 30)\n    X0, X1 = np.meshgrid(x0, x1)\n    ax.plot_surface(X0, X1, g_f(X0, X1), rstride=1, cstride=1,\n                    cmap='winter', edgecolor='none',alpha=0.3)\n\n    # choose point to evaluate: (-4,1)\n    i0 = -4\n    i1 = 1\n    iz = np.linspace(g_f(i0,i1), -82, 30)\n    ax.scatter3D(i0, i1, g_f(i0,i1), c=\"k\", s=20*fig_scale,label='($i_0$,$i_1$) = (-4,1)')\n    ax.plot3D([i0]*30, [i1]*30, iz, linewidth=1*fig_scale, c='silver', linestyle='-')\n    ax.set_zlim(-80,0)\n\n    # plot intersects\n    ax.plot3D(x0,[1]*30,g_f(x0, 1),linewidth=3*fig_scale,alpha=0.9,label='$f(x_0,i_1)$',c='r',linestyle=':')\n    ax.plot3D([-4]*30,x1,g_f(-4, x1),linewidth=3*fig_scale,alpha=0.9,label='$f(i_0,x_1)$',c='b',linestyle=':')\n\n    # df/dx0 is slope of line at the intersect point\n    x0 = np.linspace(-8, 0, 30)\n    ax.plot3D(x0,[1]*30,g_dfx0(i0)*x0-g_f(i0,i1),linewidth=3*fig_scale,label=r'$\\frac{\\partial f}{\\partial x_0}(i_0,i_1) x_0 + f(i_0,i_1)$',c='r',linestyle='-')\n    ax.plot3D([-4]*30,x1,g_dfx1(i1)*x1+g_f(i0,i1),linewidth=3*fig_scale,label=r'$\\frac{\\partial f}{\\partial x_1}(i_0,i_1) x_1 + f(i_0,i_1)$',c='b',linestyle='-')\n\n    ax.set_xlabel('x0', labelpad=-4/fig_scale)\n    ax.set_ylabel('x1', labelpad=-4/fig_scale)\n    ax.get_zaxis().set_ticks([])\n    ax.view_init(30, rotation) # Use this to rotate the figure\n    ax.legend()\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    ax.tick_params(axis='both', width=0, labelsize=10*fig_scale, pad=-6)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\nif not interactive:\n    plot_gradient(rotation=120)\n\n\n\n","type":"content","url":"/notebooks/linear-models#gradients","position":7},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Distributions and Probabilities","lvl3":"Notation and Definitions"},"type":"lvl4","url":"/notebooks/linear-models#distributions-and-probabilities","position":8},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Distributions and Probabilities","lvl3":"Notation and Definitions"},"content":"The normal (Gaussian) distribution with mean \\mu and standard deviation \\sigma is noted as N(\\mu,\\sigma)\n\nA random variable X can be continuous or discrete\n\nA probability distribution f_X of a continuous variable X: probability density function (pdf)\n\nThe expectation is given by \\mathbb{E}[X] = \\int x f_{X}(x) dx\n\nA probability distribution of a discrete variable: probability mass function (pmf)\n\nThe expectation (or mean) \\mu_X = \\mathbb{E}[X] = \\sum_{i=1}^k[x_i \\cdot Pr(X=x_i)]\n\n","type":"content","url":"/notebooks/linear-models#distributions-and-probabilities","position":9},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl2":"Linear models"},"type":"lvl2","url":"/notebooks/linear-models#linear-models","position":10},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl2":"Linear models"},"content":"Linear models make a prediction using a linear function of the input features Xf_{\\mathbf{w}}(\\mathbf{x}) = \\sum_{i=1}^{p} w_i \\cdot x_i + w_{0}\n\nLearn w from X, given a loss function \\mathcal{L}:\\underset{\\mathbf{w}}{\\operatorname{argmin}} \\mathcal{L}(f_\\mathbf{w}(X))\n\nMany algorithms with different \\mathcal{L}: Least squares, Ridge, Lasso, Logistic Regression, Linear SVMs,...\n\nCan be very powerful (and fast), especially for large datasets with many features.\n\nCan be generalized to learn non-linear patterns: Generalized Linear Models\n\nFeatures can be augmentented with polynomials of the original features\n\nFeatures can be transformed according to a distribution (Poisson, Tweedie, Gamma,...)\n\nSome linear models (e.g. SVMs) can be kernelized to learn non-linear functions\n\n","type":"content","url":"/notebooks/linear-models#linear-models","position":11},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl3","url":"/notebooks/linear-models#linear-models-for-regression","position":12},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Prediction formula for input features x:\n\nw_1 ... w_p usually called weights or coefficients , w_0 the bias or intercept\n\nAssumes that errors are N(0,\\sigma)\\hat{y} = \\mathbf{w}\\mathbf{x} + w_0 = \\sum_{i=1}^{p} w_i \\cdot x_i + w_0 = w_1 \\cdot x_1 + w_2 \\cdot x_2 + ... + w_p \\cdot x_p + w_0\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom mglearn.datasets import make_wave\n\nXw, yw = make_wave(n_samples=60)\nXw_train, Xw_test, yw_train, yw_test = train_test_split(Xw, yw, random_state=42)\n\nline = np.linspace(-3, 3, 100).reshape(-1, 1)\n\nlr = LinearRegression().fit(Xw_train, yw_train)\nprint(\"w_1: %f  w_0: %f\" % (lr.coef_[0], lr.intercept_))\n\nplt.figure(figsize=(6*fig_scale, 3*fig_scale))\nplt.plot(line, lr.predict(line), lw=fig_scale)\nplt.plot(Xw_train, yw_train, 'o', c='b')\n#plt.plot(X_test, y_test, '.', c='r')\nax = plt.gca()\nax.grid(True)\nax.set_ylim(-2, 2)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend([\"model\", \"training data\"], loc=\"best\");\n\n\n\n\n\n","type":"content","url":"/notebooks/linear-models#linear-models-for-regression","position":13},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#linear-regression-aka-ordinary-least-squares","position":14},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Loss function is the sum of squared errors (SSE) (or residuals) between predictions \\hat{y}_i (red) and the true regression targets y_i (blue) on the training set.\\mathcal{L}_{SSE} = \\sum_{n=1}^{N} (y_n-\\hat{y}_n)^2 = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2\n\n","type":"content","url":"/notebooks/linear-models#linear-regression-aka-ordinary-least-squares","position":15},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Solving ordinary least squares","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#solving-ordinary-least-squares","position":16},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Solving ordinary least squares","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Convex optimization problem with unique closed-form solution:w^{*} = (X^{T}X)^{-1} X^T Y\n\nAdd a column of 1’s to the front of X to get w_0\n\nSlow. Time complexity is quadratic in number of features: \\mathcal{O}(p^2n)\n\nX has n rows, p features, hence X^{T}X has dimensionality p \\cdot p\n\nOnly works if n>p\n\nGradient Descent\n\nFaster for large and/or high-dimensional datasets\n\nWhen X^{T}X cannot be computed or takes too long (p or n is too large)\n\nWhen you want more control over the learning process\n\nVery easily overfits.\n\ncoefficients w become very large (steep incline/decline)\n\nsmall change in the input x results in a very different output y\n\nNo hyperparameters that control model complexity\n\n","type":"content","url":"/notebooks/linear-models#solving-ordinary-least-squares","position":17},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Gradient Descent","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#gradient-descent","position":18},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Gradient Descent","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Start with an initial, random set of weights: \\mathbf{w}^0\n\nGiven a differentiable loss function \\mathcal{L} (e.g. \\mathcal{L}_{SSE}), compute \\nabla \\mathcal{L}\n\nFor least squares: \\frac{\\partial \\mathcal{L}_{SSE}}{\\partial w_i}(\\mathbf{w}) = -2\\sum_{n=1}^{N} (y_n-\\hat{y}_n) x_{n,i}\n\nIf feature X_{:,i} is associated with big errors, the gradient wrt w_i will be large\n\nUpdate all weights slightly (by step size or learning rate \\eta) in ‘downhill’ direction.\n\nBasic update rule (step s):\\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}^s)\n\nImportant hyperparameters\n\nLearning rate\n\nToo small: slow convergence. Too large: possible divergence\n\nMaximum number of iterations\n\nToo small: no convergence. Too large: wastes resources\n\nLearning rate decay with decay rate k\n\nE.g. exponential (\\eta^{s+1} = \\eta^{0}  e^{-ks}), inverse-time (\\eta^{s+1} = \\frac{\\eta^{s}}{1+ks}),...\n\nMany more advanced ways to control learning rate (see later)\n\nAdaptive techniques: depend on how much loss improved in previous step\n\nimport math\n# Some convex function to represent the loss\ndef l_fx(x):\n    return (x * 4)**2 \n# Derivative to compute the gradient\ndef l_dfx0(x0):\n    return 8 * x0\n\n@interact\ndef plot_learning_rate(learn_rate=(0.01,0.4,0.01), exp_decay=False):\n    w = np.linspace(-1,1,101)\n    f = [l_fx(i) for i in w]\n    w_current = -0.75\n    learn_rate_current = learn_rate\n    fw = [] # weight values\n    fl = [] # loss values\n    for i in range(10):\n        fw.append(w_current)\n        fl.append(l_fx(w_current))\n        # Decay\n        if exp_decay:\n            learn_rate_current = learn_rate * math.exp(-0.3*i)\n        # Update rule\n        w_current = w_current - learn_rate_current * l_dfx0(w_current)\n    fig, ax = plt.subplots(figsize=(5*fig_scale,3*fig_scale))\n    ax.set_xlabel('w')\n    ax.set_xticks([])\n    ax.set_ylabel('loss')\n    ax.plot(w, f, lw=2*fig_scale, ls='-', c='k', label='Loss')\n    ax.plot(fw, fl, '--bo', lw=2*fig_scale, markersize=3)\n    plt.ylim(-1,16)\n    plt.xlim(-1,1)\n    plt.show()\n    \n\n\n\nif not interactive:\n    plot_learning_rate(learn_rate=0.21, exp_decay=False)\n\n\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Toy surface\ndef f(x, y):\n    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n\n# TensorFlow optimizers\nsgd = tf.optimizers.SGD(learning_rate=0.01)\nlr_schedule = tf.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=0.02,\n    decay_steps=100,\n    decay_rate=0.96\n)\nsgd_decay = tf.optimizers.SGD(learning_rate=lr_schedule)\n\noptimizers = [sgd, sgd_decay]\nopt_names = ['sgd', 'sgd_decay']\ncmap = plt.cm.get_cmap('tab10')\ncolors = [cmap(x/10) for x in range(10)]\n\n# Training\nall_paths = []\nfor opt, name in zip(optimizers, opt_names):\n    x = tf.Variable(0.8, dtype=tf.float32)\n    y = tf.Variable(1.6, dtype=tf.float32)\n\n    x_history = []\n    y_history = []\n    loss_prev = 0.0\n    max_steps = 100\n    \n    for step in range(max_steps):\n        with tf.GradientTape() as g:\n            loss = f(x, y)\n        \n        x_history.append(x.numpy())\n        y_history.append(y.numpy())\n        grads = g.gradient(loss, [x, y])\n        opt.apply_gradients(zip(grads, [x, y]))\n        \n        if np.abs(loss_prev - loss.numpy()) < 1e-6:\n            break\n        loss_prev = loss.numpy()\n    \n    x_history = np.array(x_history)\n    y_history = np.array(y_history)\n    path = np.vstack((x_history, y_history))\n    all_paths.append(path)\n\n\n\nfrom matplotlib.colors import LogNorm\n\n# Toy surface\ndef f(x, y):\n    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n\n# Tensorflow optimizers\nsgd = tf.optimizers.SGD(0.01)\nlr_schedule = tf.optimizers.schedules.ExponentialDecay(0.02,decay_steps=100,decay_rate=0.96)\nsgd_decay = tf.optimizers.SGD(learning_rate=lr_schedule)\n\noptimizers = [sgd, sgd_decay]\nopt_names = ['sgd', 'sgd_decay']\ncmap = plt.cm.get_cmap('tab10')\ncolors = [cmap(x/10) for x in range(10)]\n\n# Training\nall_paths = []\nfor opt, name in zip(optimizers, opt_names):\n    x_init = 0.8\n    x = tf.Variable(x_init)\n    y_init = 1.6\n    y = tf.Variable(y_init)\n\n    x_history = []\n    y_history = []\n    z_prev = 0.0\n    max_steps = 100\n    for step in range(max_steps):\n        with tf.GradientTape() as g:\n            z = f(x, y)\n            x_history.append(x.numpy())\n            y_history.append(y.numpy())\n            dz_dx, dz_dy = g.gradient(z, [x, y])\n            opt.apply_gradients(zip([dz_dx, dz_dy], [x, y]))\n\n    if np.abs(z_prev - z.numpy()) < 1e-6:\n        break\n    z_prev = z.numpy()\n    x_history = np.array(x_history)\n    y_history = np.array(y_history)\n    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n    all_paths.append(path)\n        \n# Plotting\nnumber_of_points = 50\nmargin = 4.5\nminima = np.array([3., .5])\nminima_ = minima.reshape(-1, 1)\nx_min = 0. - 2\nx_max = 0. + 3.5\ny_min = 0. - 3.5\ny_max = 0. + 2\nx_points = np.linspace(x_min, x_max, number_of_points) \ny_points = np.linspace(y_min, y_max, number_of_points)\nx_mesh, y_mesh = np.meshgrid(x_points, y_points)\nz = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])\n\ndef plot_optimizers(ax, iterations, optimizers):\n    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=fig_scale, zorder=-1)\n    ax.plot(*minima, 'r*', markersize=20*fig_scale)\n    for name, path, color in zip(opt_names, all_paths, colors):\n        if name in optimizers:\n            p = path[:,:iterations]\n            ax.plot([], [], color=color, label=name, lw=3*fig_scale, linestyle='-')\n            ax.quiver(p[0,:-1], p[1,:-1], p[0,1:]-p[0,:-1], p[1,1:]-p[1,:-1], scale_units='xy', angles='xy', scale=1, color=color, lw=4)\n\n\n    ax.set_xlim((x_min, x_max))\n    ax.set_ylim((y_min, y_max))\n    ax.legend(loc='lower left', prop={'size': 15*fig_scale}) \n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.tight_layout()\n\n\n\n# Training for momentum\nall_lr_paths = []\nlr_range = [0.005 * i for i in range(10)]\n\nfor lr in lr_range:\n    opt = tf.optimizers.SGD(learning_rate=lr, nesterov=False)\n    \n    x = tf.Variable(0.8, dtype=tf.float32)\n    y = tf.Variable(1.6, dtype=tf.float32)\n    \n    x_history = []\n    y_history = []\n    z_prev = 0.0\n    max_steps = 100\n    \n    for step in range(max_steps):\n        with tf.GradientTape() as g:\n            z = f(x, y)\n        \n        x_history.append(x.numpy())\n        y_history.append(y.numpy())\n        dz_dx, dz_dy = g.gradient(z, [x, y])\n        opt.apply_gradients(zip([dz_dx, dz_dy], [x, y]))\n        \n        if np.abs(z_prev - z.numpy()) < 1e-6:\n            break\n        z_prev = z.numpy()\n    \n    x_history = np.array(x_history)\n    y_history = np.array(y_history)\n    path = np.vstack((x_history, y_history))\n    all_lr_paths.append(path)\n    \n# Plotting\nnumber_of_points = 50\nmargin = 4.5\nminima = np.array([3., 0.5])\nminima_ = minima.reshape(-1, 1)\nx_min = -2\nx_max = 3.5\ny_min = -3.5\ny_max = 2\nx_points = np.linspace(x_min, x_max, number_of_points) \ny_points = np.linspace(y_min, y_max, number_of_points)\nx_mesh, y_mesh = np.meshgrid(x_points, y_points)\nz = np.array([[f(xps, yps) for xps, yps in zip(row_x, row_y)] for row_x, row_y in zip(x_mesh, y_mesh)])\n\ndef plot_learning_rate_optimizers(ax, iterations, lr):\n    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=1, zorder=-1)\n    ax.plot(*minima, 'r*', markersize=20)\n    \n    for path, lrate in zip(all_lr_paths, lr_range):\n        if round(lrate, 3) == round(lr, 3):\n            p = path[:, :iterations]\n            ax.plot([], [], color='b', label=f\"Learning rate {round(lr, 3)}\", lw=3, linestyle='-')\n            ax.quiver(p[0, :-1], p[1, :-1], p[0, 1:] - p[0, :-1], p[1, 1:] - p[1, :-1], scale_units='xy', angles='xy', scale=1, color='b', lw=4)\n    \n    ax.set_xlim((x_min, x_max))\n    ax.set_ylim((y_min, y_max))\n    ax.legend(loc='lower left', prop={'size': 8})\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.tight_layout()\n\n\n\nEffect of learning rate\n\n@interact\ndef plot_lr(iterations=(1, 100, 1), learning_rate=(0.005, 0.045, 0.005)):  # Fixed range\n    fig, ax = plt.subplots(figsize=(6 * fig_scale, 4 * fig_scale))\n    plot_learning_rate_optimizers(ax, iterations, learning_rate)\n    plt.show()\n    \nif not interactive:\n    plot_lr(iterations=50, learning_rate=0.02)\n\n\n\n\n\nEffect of learning rate decay\n\n@interact\ndef compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n    plot_optimizers(ax,iterations,[optimizer1,optimizer2])\n    plt.show()\n    \nif not interactive:\n    compare_optimizers(iterations=50, optimizer1=\"sgd\", optimizer2=\"sgd_decay\")\n\n\n\n\n\nIn two dimensions:\n\n\nYou can get stuck in local minima (if the loss is not fully convex)\n\nIf you have many model parameters, this is less likely\n\nYou always find a way down in some direction\n\nModels with many parameters typically find good local minima\n\nIntuition: walking downhill using only the slope you “feel” nearby\n\n(Image by A. Karpathy)\n\n","type":"content","url":"/notebooks/linear-models#gradient-descent","position":19},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Stochastic Gradient Descent (SGD)","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#stochastic-gradient-descent-sgd","position":20},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Stochastic Gradient Descent (SGD)","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Compute gradients not on the entire dataset, but on a single data point i at a time\n\nGradient descent: \\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}^s) = \\mathbf{w}^s-\\frac{\\eta}{n} \\sum_{i=1}^{n} \\nabla \\mathcal{L_i}(\\mathbf{w}^s)\n\nStochastic Gradient Descent: \\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta\\nabla \\mathcal{L_i}(\\mathbf{w}^s)\n\nMany smoother variants, e.g.\n\nMinibatch SGD: compute gradient on batches of data: \\mathbf{w}^{s+1} = \\mathbf{w}^s-\\frac{\\eta}{B} \\sum_{i=1}^{B} \\nabla \\mathcal{L_i}(\\mathbf{w}^s)\n\nStochastic Average Gradient Descent (\n\nSAG, \n\nSAGA). With i_s \\in [1,n] randomly chosen per iteration:\n\nIncremental gradient: \\mathbf{w}^{s+1} = \\mathbf{w}^s-\\frac{\\eta}{n} \\sum_{i=1}^{n} v_i^s with v_i^s = \\begin{cases}\\nabla \\mathcal{L_i}(\\mathbf{w}^s) & i = i_s \\\\ v_i^{s-1} & \\text{otherwise} \\end{cases}\n\n","type":"content","url":"/notebooks/linear-models#stochastic-gradient-descent-sgd","position":21},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"In practice","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#in-practice","position":22},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"In practice","lvl4":"Linear Regression (aka Ordinary Least Squares)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Linear regression can be found in sklearn.linear_model. We’ll evaluate it on the Boston Housing dataset.\n\nLinearRegression uses closed form solution, SGDRegressor with loss='squared_loss' uses Stochastic Gradient Descent\n\nLarge coefficients signal overfitting\n\nTest score is much lower than training scorefrom sklearn.linear_model import LinearRegression\nlr = LinearRegression().fit(X_train, y_train)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX_B, y_B = mglearn.datasets.load_extended_boston()\nX_B_train, X_B_test, y_B_train, y_B_test = train_test_split(X_B, y_B, random_state=0)\n\nlr = LinearRegression().fit(X_B_train, y_B_train)\n\n\n\nprint(\"Weights (coefficients): {}\".format(lr.coef_[0:40]))\nprint(\"Bias (intercept): {}\".format(lr.intercept_))\n\n\n\nprint(\"Training set score (R^2): {:.2f}\".format(lr.score(X_B_train, y_B_train)))\nprint(\"Test set score (R^2): {:.2f}\".format(lr.score(X_B_test, y_B_test)))\n\n\n\n","type":"content","url":"/notebooks/linear-models#in-practice","position":23},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Ridge regression","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#ridge-regression","position":24},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Ridge regression","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Adds a penalty term to the least squares loss function:\\mathcal{L}_{Ridge} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} w_i^2\n\nModel is penalized if it uses large coefficients (w)\n\nEach feature should have as little effect on the outcome as possible\n\nWe don’t want to penalize w_0, so we leave it out\n\nRegularization: explicitly restrict a model to avoid overfitting.\n\nCalled L2 regularization because it uses the L2 norm: \\sum w_i^2\n\nThe strength of the regularization can be controlled with the \\alpha hyperparameter.\n\nIncreasing \\alpha causes more regularization (or shrinkage). Default is 1.0.\n\nStill convex. Can be optimized in different ways:\n\nClosed form solution (a.k.a. Cholesky): w^{*} = (X^{T}X + \\alpha I)^{-1} X^T Y\n\nGradient descent and variants, e.g. Stochastic Average Gradient (SAG,SAGA)\n\nConjugate gradient (CG): each new gradient is influenced by previous ones\n\nUse Cholesky for smaller datasets, Gradient descent for larger ones\n\n","type":"content","url":"/notebooks/linear-models#ridge-regression","position":25},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"In practice","lvl4":"Ridge regression","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#in-practice-1","position":26},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"In practice","lvl4":"Ridge regression","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"from sklearn.linear_model import Ridge\nlr = Ridge().fit(X_train, y_train)\n\nfrom sklearn.linear_model import Ridge\nridge = Ridge().fit(X_B_train, y_B_train)\nprint(\"Weights (coefficients): {}\".format(ridge.coef_[0:40]))\nprint(\"Bias (intercept): {}\".format(ridge.intercept_))\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_B_train, y_B_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_B_test, y_B_test)))\n\n\n\nTest set score is higher and training set score lower: less overfitting!\n\nWe can plot the weight values for differents levels of regularization to explore the effect of \\alpha.\n\nIncreasing regularization decreases the values of the coefficients, but never to 0.\n\nfrom __future__ import print_function\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\nfrom sklearn.linear_model import Ridge\n\n@interact\ndef plot_ridge(alpha=(0,10.0,0.05)):\n    r = Ridge(alpha=alpha).fit(X_B_train, y_B_train)\n    fig, ax = plt.subplots(figsize=(8*fig_scale,1.5*fig_scale))\n    ax.plot(r.coef_, 'o', markersize=3)\n    ax.set_title(\"alpha {}, test score {:.2f} (training score {:.2f})\".format(alpha, r.score(X_B_test, y_B_test), r.score(X_B_train, y_B_train)))\n    ax.set_xlabel(\"Coefficient index\")\n    ax.set_ylabel(\"Coefficient magnitude\")\n    ax.hlines(0, 0, len(r.coef_))\n    ax.set_ylim(-25, 25)\n    ax.set_xlim(0, 50);\n    plt.show()\n\n\n\nif not interactive:\n    for alpha in [0.1, 10]:\n        plot_ridge(alpha)\n\n\n\n\n\nWhen we plot the train and test scores for every \\alpha value, we see a sweet spot around \\alpha=0.2\n\nModels with smaller \\alpha are overfitting\n\nModels with larger \\alpha are underfitting\n\nalpha=np.logspace(-3,2,num=20)\nai = list(range(len(alpha)))\ntest_score=[]\ntrain_score=[]\nfor a in alpha:\n    r = Ridge(alpha=a).fit(X_B_train, y_B_train)\n    test_score.append(r.score(X_B_test, y_B_test))\n    train_score.append(r.score(X_B_train, y_B_train))\nfig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\nax.set_xticks(range(20))\nax.set_xticklabels(np.round(alpha,3))\nax.set_xlabel('alpha')\nax.plot(test_score, lw=2*fig_scale, label='test score')\nax.plot(train_score, lw=2*fig_scale, label='train score')\nax.legend()\nplt.xticks(rotation=45);\n\n\n\n","type":"content","url":"/notebooks/linear-models#in-practice-1","position":27},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Other ways to reduce overfitting","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#other-ways-to-reduce-overfitting","position":28},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Other ways to reduce overfitting","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Add more training data: with enough training data, regularization becomes less important\n\nRidge and ordinary least squares will have the same performance\n\nUse fewer features: remove unimportant ones or find a low-dimensional embedding (e.g. PCA)\n\nFewer coefficients to learn, reduces the flexibility of the model\n\nScaling the data typically helps (and changes the optimal \\alpha value)\n\nfig, ax = plt.subplots(figsize=(10*fig_scale,4*fig_scale))\nmglearn.plots.plot_ridge_n_samples(ax)\n\n\n\n","type":"content","url":"/notebooks/linear-models#other-ways-to-reduce-overfitting","position":29},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Lasso (Least Absolute Shrinkage and Selection Operator)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#lasso-least-absolute-shrinkage-and-selection-operator","position":30},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Lasso (Least Absolute Shrinkage and Selection Operator)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Adds a different penalty term to the least squares sum:\\mathcal{L}_{Lasso} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} |w_i|\n\nCalled L1 regularization because it uses the L1 norm\n\nWill cause many weights to be exactly 0\n\nSame parameter \\alpha to control the strength of regularization.\n\nWill again have a ‘sweet spot’ depending on the data\n\nNo closed-form solution\n\nConvex, but no longer strictly convex, and not differentiable\n\nWeights can be optimized using coordinate descent\n\nAnalyze what happens to the weights:\n\nL1 prefers coefficients to be exactly zero (sparse models)\n\nSome features are ignored entirely: automatic feature selection\n\nHow can we explain this?\n\nfrom sklearn.linear_model import Lasso\n\n@interact\ndef plot_lasso(alpha=(0,0.5,0.005)):\n    r = Lasso(alpha=alpha).fit(X_B_train, y_B_train)\n    fig, ax = plt.subplots(figsize=(8*fig_scale,1.5*fig_scale))\n    ax.plot(r.coef_, 'o', markersize=6*fig_scale)\n    ax.set_title(\"alpha {}, score {:.2f} (training score {:.2f})\".format(alpha, r.score(X_B_test, y_B_test), r.score(X_B_train, y_B_train)), pad=0.5)\n    ax.set_xlabel(\"Coefficient index\", labelpad=0)\n    ax.set_ylabel(\"Coefficient magnitude\")\n    ax.hlines(0, 0, len(r.coef_))\n    ax.set_ylim(-25, 25);\n    ax.set_xlim(0, 50);\n    plt.show()\n\n\n\nif not interactive:\n    for alpha in [0.00001, 0.01]:\n        plot_lasso(alpha)\n\n\n\n\n\n","type":"content","url":"/notebooks/linear-models#lasso-least-absolute-shrinkage-and-selection-operator","position":31},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Coordinate descent","lvl4":"Lasso (Least Absolute Shrinkage and Selection Operator)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#coordinate-descent","position":32},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Coordinate descent","lvl4":"Lasso (Least Absolute Shrinkage and Selection Operator)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Alternative for gradient descent, supports non-differentiable convex loss functions (e.g. \\mathcal{L}_{Lasso})\n\nIn every iteration, optimize a single coordinate w_i (find minimum in direction of x_i)\n\nContinue with another coordinate, using a selection rule (e.g. round robin)\n\nFaster iterations. No need to choose a step size (learning rate).\n\nMay converge more slowly. Can’t be parallellized.\n\n","type":"content","url":"/notebooks/linear-models#coordinate-descent","position":33},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Coordinate descent with Lasso","lvl4":"Lasso (Least Absolute Shrinkage and Selection Operator)","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#coordinate-descent-with-lasso","position":34},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Coordinate descent with Lasso","lvl4":"Lasso (Least Absolute Shrinkage and Selection Operator)","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Remember that \\mathcal{L}_{Lasso} = \\mathcal{L}_{SSE} + \\alpha \\sum_{i=1}^{p} |w_i|\n\nFor one w_i: \\mathcal{L}_{Lasso}(w_i) = \\mathcal{L}_{SSE}(w_i) + \\alpha |w_i|\n\nThe L1 term is not differentiable but convex: we can compute the \n\nsubgradient\n\nUnique at points where \\mathcal{L} is differentiable, a range of all possible slopes [a,b] where it is not\n\nFor |w_i|, the subgradient \\partial_{w_i} |w_i| =  \\begin{cases}-1 & w_i<0\\\\ [-1,1] & w_i=0 \\\\ 1 & w_i>0 \\\\ \\end{cases}\n\nSubdifferential \\partial(f+g) = \\partial f + \\partial g if f and g are both convex\n\nTo find the optimum for Lasso w_i^{*}, solve\\begin{aligned} \\partial_{w_i} \\mathcal{L}_{Lasso}(w_i) &= \\partial_{w_i} \\mathcal{L}_{SSE}(w_i) + \\partial_{w_i} \\alpha |w_i| \\\\ 0 &= (w_i - \\rho_i) + \\alpha \\cdot \\partial_{w_i} |w_i| \\\\ w_i &= \\rho_i - \\alpha \\cdot \\partial_{w_i} |w_i| \\end{aligned}\n\nIn which \\rho_i is the part of \\partial_{w_i} \\mathcal{L}_{SSE}(w_i) excluding w_i (assume z_i=1 for now)\n\n\\rho_i can be seen as the \\mathcal{L}_{SSE} ‘solution’: w_i = \\rho_i if \\partial_{w_i} \\mathcal{L}_{SSE}(w_i) = 0\n\\partial_{w_i} \\mathcal{L}_{SSE}(w_i) = \\partial_{w_i} \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 = z_i w_i -\\rho_i\n\nWe found: w_i = \\rho_i - \\alpha \\cdot \\partial_{w_i} |w_i|\n\nThe Lasso solution has the form of a soft thresholding function Sw_i^* = S(\\rho_i,\\alpha) = \\begin{cases} \\rho_i + \\alpha, & \\rho_i < -\\alpha \\\\  0, & -\\alpha < \\rho_i < \\alpha \\\\ \\rho_i - \\alpha, & \\rho_i > \\alpha \\\\ \\end{cases}\n\nSmall weights (all weights between -\\alpha and \\alpha) become 0: sparseness!\n\nIf the data is not normalized, w_i^* = \\frac{1}{z_i}S(\\rho_i,\\alpha) with constant z_i = \\sum_{n=1}^{N} x_{ni}^2\n\nRidge solution: w_i = \\rho_i - \\alpha \\cdot \\partial_{w_i} w_i^2 = \\rho_i - 2\\alpha \\cdot w_i, thus w_i^* = \\frac{\\rho_i}{1 + 2\\alpha}\n\n@interact\ndef plot_rho(alpha=(0,2.0,0.05)):\n    w = np.linspace(-2,2,101)\n    r = w/(1+2*alpha)\n    l = [x+alpha if x <= -alpha else (x-alpha if x > alpha else 0) for x in w]\n    fig, ax = plt.subplots(figsize=(6*fig_scale,3*fig_scale))\n    ax.set_xlabel(r'$\\rho$')\n    ax.set_ylabel(r'$w^{*}$')\n    ax.plot(w, w, lw=2*fig_scale, c='g', label='Ordinary Least Squares (SSE)')\n    ax.plot(w, r, lw=2*fig_scale, c='b', label='Ridge with alpha={}'.format(alpha))\n    ax.plot(w, l, lw=2*fig_scale, c='r', label='Lasso with alpha={}'.format(alpha))\n    ax.legend()\n    plt.grid()\n    plt.show()\n\n\n\nif not interactive:\n    plot_rho(alpha=1)\n\n\n\n","type":"content","url":"/notebooks/linear-models#coordinate-descent-with-lasso","position":35},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Interpreting L1 and L2 loss","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#interpreting-l1-and-l2-loss","position":36},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Interpreting L1 and L2 loss","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"L1 and L2 in function of the weights\n\nLeast Squares Loss + L1 or L2\n\nThe Lasso curve has 3 parts (w<0,w=0,w>0) corresponding to the thresholding function\n\nFor any minimum of least squares, L2 will be smaller, and L1 is more likely be exactly 0\n\ndef c_fx(x):\n    fX = ((x * 2 - 1)**2) # Some convex function to represent the loss\n    return fX/9 # Scaling\ndef c_fl2(x,alpha):\n    return c_fx(x) + alpha * x**2\ndef c_fl1(x,alpha):\n    return c_fx(x) + alpha * abs(x)\ndef l2(x,alpha):\n    return alpha * x**2\ndef l1(x,alpha):\n    return alpha * abs(x)\n\n@interact\ndef plot_losses(alpha=(0,1.0,0.05)):\n    w = np.linspace(-1,1,101)\n    f = [c_fx(i) for i in w]\n    r = [c_fl2(i,alpha) for i in w]\n    l = [c_fl1(i,alpha) for i in w]\n    rp = [l2(i,alpha) for i in w]\n    lp = [l1(i,alpha) for i in w]\n    fig, ax = plt.subplots(figsize=(8*fig_scale,4*fig_scale))\n    ax.set_xlabel('w')\n    ax.set_ylabel('loss')\n    ax.plot(w, rp, lw=1.5*fig_scale, ls=':', c='b', label='L2 with alpha={}'.format(alpha))\n    ax.plot(w, lp, lw=1.5*fig_scale, ls=':', c='r', label='L1 with alpha={}'.format(alpha))\n    ax.plot(w, f, lw=2*fig_scale, ls='-', c='k', label='Least Squares loss')\n    ax.plot(w, r, lw=2*fig_scale, ls='-', c='b', label='Loss + L2 (Ridge)'.format(alpha))\n    ax.plot(w, l, lw=2*fig_scale, ls='-', c='r', label='Loss + L1 (Lasso)'.format(alpha))\n    opt_f = np.argmin(f)\n    ax.scatter(w[opt_f], f[opt_f], c=\"k\", s=50*fig_scale)\n    opt_r = np.argmin(r)\n    ax.scatter(w[opt_r], r[opt_r], c=\"b\", s=50*fig_scale)\n    opt_l = np.argmin(l)\n    ax.scatter(w[opt_l], l[opt_l], c=\"r\", s=50*fig_scale)\n    ax.legend()\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    plt.ylim(-0.1,1)\n    plt.grid()\n    plt.show()\n\n\n\nif not interactive:\n    plot_losses(alpha=0.5)\n\n\n\nIn 2D (for 2 model weights w_1 and w_2)\n\nThe least squared loss is a 2D convex function in this space (ellipses on the right)\n\nFor illustration, assume that L1 loss = L2 loss = 1\n\nL1 loss (\\Sigma |w_i|): the optimal {w_1, w_2} (blue dot) falls on the diamond\n\nL2 loss (\\Sigma w_i^2): the optimal {w_1, w_2} (cyan dot) falls on the circle\n\nFor L1, the loss is minimized if w_1 or w_2 is 0 (rarely so for L2)\n\ndef plot_loss_interpretation():\n    line = np.linspace(-1.5, 1.5, 1001)\n    xx, yy = np.meshgrid(line, line)\n\n    l2 = xx ** 2 + yy ** 2\n    l1 = np.abs(xx) + np.abs(yy)\n    rho = 0.7\n    elastic_net = rho * l1 + (1 - rho) * l2\n\n    plt.figure(figsize=(5*fig_scale, 4*fig_scale))\n    ax = plt.gca()\n\n    elastic_net_contour = plt.contour(xx, yy, elastic_net, levels=[1], linewidths=2*fig_scale, colors=\"darkorange\")\n    l2_contour = plt.contour(xx, yy, l2, levels=[1], linewidths=2*fig_scale, colors=\"c\")\n    l1_contour = plt.contour(xx, yy, l1, levels=[1], linewidths=2*fig_scale, colors=\"navy\")\n    ax.set_aspect(\"equal\")\n    ax.spines['left'].set_position('center')\n    ax.spines['right'].set_color('none')\n    ax.spines['bottom'].set_position('center')\n    ax.spines['top'].set_color('none')\n\n    plt.clabel(elastic_net_contour, inline=1, fontsize=12*fig_scale,\n               fmt={1.0: 'elastic-net'}, manual=[(-0.6, -0.6)])\n    plt.clabel(l2_contour, inline=1, fontsize=12*fig_scale,\n               fmt={1.0: 'L2'}, manual=[(-0.5, -0.5)])\n    plt.clabel(l1_contour, inline=1, fontsize=12*fig_scale,\n               fmt={1.0: 'L1'}, manual=[(-0.5, -0.5)])\n\n    x1 = np.linspace(0.5, 1.5, 100)\n    x2 = np.linspace(-1.0, 1.5, 100)\n    X1, X2 = np.meshgrid(x1, x2)\n    Y = np.sqrt(np.square(X1/2-0.7) + np.square(X2/4-0.28))\n    cp = plt.contour(X1, X2, Y)\n    plt.clabel(cp, inline=1, fontsize=3)\n    ax.tick_params(axis='both', pad=0)\n    ax.scatter(1, 0, c=\"navy\", s=50*fig_scale)\n    ax.scatter(0.89, 0.42, c=\"c\", s=50*fig_scale)\n\n    plt.tight_layout()\n    plt.show()\nplot_loss_interpretation()\n\n\n\n","type":"content","url":"/notebooks/linear-models#interpreting-l1-and-l2-loss","position":37},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Elastic-Net","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#elastic-net","position":38},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Elastic-Net","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Adds both L1 and L2 regularization:\\mathcal{L}_{Elastic} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\rho \\sum_{i=1}^{p} |w_i| + \\alpha (1 -  \\rho) \\sum_{i=1}^{p} w_i^2\n\n\\rho is the L1 ratio\n\nWith \\rho=1, \\mathcal{L}_{Elastic} = \\mathcal{L}_{Lasso}\n\nWith \\rho=0, \\mathcal{L}_{Elastic} = \\mathcal{L}_{Ridge}\n\n0 < \\rho < 1 sets a trade-off between L1 and L2.\n\nAllows learning sparse models (like Lasso) while maintaining L2 regularization benefits\n\nE.g. if 2 features are correlated, Lasso likely picks one randomly, Elastic-Net keeps both\n\nWeights can be optimized using coordinate descent (similar to Lasso)\n\n","type":"content","url":"/notebooks/linear-models#elastic-net","position":39},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Other loss functions for regression","lvl3":"Linear models for regression","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#other-loss-functions-for-regression","position":40},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Other loss functions for regression","lvl3":"Linear models for regression","lvl2":"Linear models"},"content":"Huber loss: switches from squared loss to linear loss past a value \\epsilon\n\nMore robust against outliers\n\nEpsilon insensitive: ignores errors smaller than \\epsilon, and linear past that\n\nAims to fit function so that residuals are at most \\epsilon\n\nAlso known as Support Vector Regression (SVR in sklearn)\n\nSquared Epsilon insensitive: ignores errors smaller than \\epsilon, and squared past that\n\nThese can all be solved with stochastic gradient descent\n\nSGDRegressor in sklearn\n\n","type":"content","url":"/notebooks/linear-models#other-loss-functions-for-regression","position":41},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl3","url":"/notebooks/linear-models#linear-models-for-classification","position":42},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Aims to find a hyperplane that separates the examples of each class.For binary classification (2 classes), we aim to fit the following function:\n\n\\hat{y} = w_1 * x_1 + w_2 * x_2 +... + w_p * x_p + w_0 > 0\n\nWhen \\hat{y}<0, predict class -1, otherwise predict class +1\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nXf, yf = mglearn.datasets.make_forge()\nfig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\nclf = LogisticRegression().fit(Xf, yf)\nmglearn.tools.plot_2d_separator(clf, Xf,\n                                ax=ax, alpha=.7, cm=mglearn.cm2)\nmglearn.discrete_scatter(Xf[:, 0], Xf[:, 1], yf, ax=ax, s=10*fig_scale)\nax.set_xlabel(\"Feature 1\")\nax.set_ylabel(\"Feature 2\")\nax.legend(['Class -1','Class 1']);\n\n\n\nThere are many algorithms for linear classification, differing in loss function, regularization techniques, and optimization method\n\nMost common techniques:\n\nConvert target classes {neg,pos} to {0,1} and treat as a regression task\n\nLogistic regression (Log loss)\n\nRidge Classification (Least Squares + L2 loss)\n\nFind hyperplane that maximizes the margin between classes\n\nLinear Support Vector Machines (Hinge loss)\n\nNeural networks without activation functions\n\nPerceptron (Perceptron loss)\n\nSGDClassifier: can act like any of these by choosing loss function\n\nHinge, Log, Modified_huber, Squared_hinge, Perceptron\n\n","type":"content","url":"/notebooks/linear-models#linear-models-for-classification","position":43},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#logistic-regression","position":44},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Aims to predict the probability that a point belongs to the positive class\n\nConverts target values {negative (blue), positive (red)} to {0,1}\n\nFits a logistic (or sigmoid or S curve) function through these points\n\nMaps (-Inf,Inf) to a probability [0,1]\\hat{y} = \\textrm{logistic}(f_{\\theta}(\\mathbf{x})) = \\frac{1}{1+e^{-f_{\\theta}(\\mathbf{x})}}\n\nE.g. in 1D:  \\textrm{logistic}(x_1w_1+w_0) = \\frac{1}{1+e^{-x_1w_1-w_0}} \n\ndef sigmoid(x,w1,w0):\n    return 1 / (1 + np.exp(-(x*w1+w0)))\n\n@interact\ndef plot_logreg(w0=(-10.0,5.0,1),w1=(-1.0,3.0,0.3)):\n    fig, ax = plt.subplots(figsize=(8*fig_scale,3*fig_scale))\n    red = [Xf[i, 1] for i in range(len(yf)) if yf[i]==1]\n    blue = [Xf[i, 1] for i in range(len(yf)) if yf[i]==0]\n    ax.scatter(red, [1]*len(red), c='r', label='Positive class')\n    ax.scatter(blue, [0]*len(blue), c='b', label='Negative class')\n    x = np.linspace(min(-1, -w0/w1),max(6, -w0/w1))\n    ax.plot(x,sigmoid(x,w1,w0),lw=2*fig_scale,c='g', label='logistic(x*w1+w0)'.format(np.round(w0,2),np.round(w1,2)))\n    ax.axvline(x=(-w0/w1), ymin=0, ymax=1, label='Decision boundary')\n    ax.plot(x,x*w1+w0,lw=2*fig_scale,c='k',linestyle=':', label='y=x*w1+w0')\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"y\")\n    ax.set_ylim(-0.05,1.05)\n    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    box = ax.get_position()\n    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height]);\n    plt.show()\n\n\n\nif not interactive:\n    # fitted solution\n    clf2 = LogisticRegression(C=100).fit(Xf[:, 1].reshape(-1, 1), yf)\n    w0 = clf2.intercept_\n    w1 = clf2.coef_[0][0]\n    plot_logreg(w0=w0,w1=w1)\n\n\n\nFitted solution to our 2D example:\n\nTo get a binary prediction, choose a probability threshold (e.g. 0.5)\n\nlr_clf = LogisticRegression(C=100).fit(Xf, yf)\n\ndef sigmoid2d(x1,x2,w0,w1,w2):\n    return 1 / (1 + np.exp(-(x2*w2+x1*w1+w0)))\n\n@interact\ndef plot_logistic_fit(rotation=(0,360,10)):\n    w0 = lr_clf.intercept_\n    w1 = lr_clf.coef_[0][0]\n    w2 = lr_clf.coef_[0][1]\n\n    # plot surface of f\n    fig = plt.figure(figsize=(7*fig_scale,5*fig_scale))\n    ax = plt.axes(projection=\"3d\")\n    x0 = np.linspace(8, 16, 30)\n    x1 = np.linspace(-1, 6, 30)\n    X0, X1 = np.meshgrid(x0, x1)\n    \n    # Surface\n    ax.plot_surface(X0, X1, sigmoid2d(X0, X1, w0, w1, w2), rstride=1, cstride=1,\n                    cmap='bwr', edgecolor='none',alpha=0.5,label='sigmoid')\n    # Points\n    c=['b','r']\n    ax.scatter3D(Xf[:, 0], Xf[:, 1], yf, c=[c[i] for i in yf], s=10*fig_scale)\n    \n    # Decision boundary\n    # x2 = -(x1*w1 + w0)/w2\n    ax.plot3D(x0,-(x0*w1 + w0)/w2,[0.5]*len(x0), lw=1*fig_scale, c='k', linestyle=':')\n    z = np.linspace(0, 1, 31)\n    XZ, Z = np.meshgrid(x0, z)\n    YZ = -(XZ*w1 + w0)/w2    \n    ax.plot_wireframe(XZ, YZ, Z, rstride=5, lw=1*fig_scale, cstride=5, alpha=0.3, color='k',label='decision boundary')\n    ax.tick_params(axis='both', width=0, labelsize=10*fig_scale, pad=-4)\n\n    ax.set_xlabel('x0', labelpad=-6)\n    ax.set_ylabel('x1', labelpad=-6)\n    ax.get_zaxis().set_ticks([])\n    ax.view_init(30, rotation) # Use this to rotate the figure\n    plt.tight_layout()\n    #plt.legend() # Doesn't work yet, bug in matplotlib\n    plt.show()\n\n\n\nif not interactive:\n    plot_logistic_fit(rotation=150)\n\n\n\n","type":"content","url":"/notebooks/linear-models#logistic-regression","position":45},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Loss function: Cross-entropy","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#loss-function-cross-entropy","position":46},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Loss function: Cross-entropy","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Models that return class probabilities can use cross-entropy loss\\mathcal{L_{log}}(\\mathbf{w}) = \\sum_{n=1}^{N} H(p_n,q_n) = - \\sum_{n=1}^{N} \\sum_{c=1}^{C} p_{n,c} log(q_{n,c})\n\nAlso known as log loss, logistic loss, or maximum likelihood\n\nBased on true probabilities p (0 or 1) and predicted probabilities q over N instances and C classes\n\nBinary case (C=2): \\mathcal{L_{log}}(\\mathbf{w}) = - \\sum_{n=1}^{N} \\big[ y_n log(\\hat{y}_n) + (1-y_n) log(1-\\hat{y}_n) \\big]\n\nPenalty (a.k.a. ‘surprise’) grows exponentially as difference between p and q increases\n\nIf you are sure of an answer (high q) and it’s wrong (low p), you definitely want to learn\n\nOften used together with L2 (or L1) loss: \\mathcal{L_{log}}'(\\mathbf{w}) = \\mathcal{L_{log}}(\\mathbf{w}) + \\alpha \\sum_{i} w_i^2 \n\ndef cross_entropy(yHat, y):\n    if y == 1:\n        return -np.log(yHat)\n    else:\n        return -np.log(1 - yHat)\n\nfig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\nx = np.linspace(0,1,100)\n\nax.plot(x,cross_entropy(x, 1),lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\nax.plot(x,cross_entropy(x, 0),lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\nax.set_xlabel(r\"Predicted probability $\\hat{y}$\")\nax.set_ylabel(\"Log loss\")\nplt.grid()\nplt.legend();\n\n\n\n","type":"content","url":"/notebooks/linear-models#loss-function-cross-entropy","position":47},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Optimization methods (solvers) for cross-entropy loss","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#optimization-methods-solvers-for-cross-entropy-loss","position":48},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Optimization methods (solvers) for cross-entropy loss","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Gradient descent (only supports L2 regularization)\n\nLog loss is differentiable, so we can use (stochastic) gradient descent\n\nVariants thereof, e.g. Stochastic Average Gradient (SAG, SAGA)\n\nCoordinate descent (supports both L1 and L2 regularization)\n\nFaster iteration, but may converge more slowly, has issues with saddlepoints\n\nCalled liblinear in sklearn. Can’t run in parallel.\n\nNewton-Rhapson or Newton Conjugate Gradient (only L2):\n\nUses the Hessian H = \\big[\\frac{\\partial^2 \\mathcal{L}}{\\partial x_i \\partial x_j} \\big]: \\mathbf{w}^{s+1} = \\mathbf{w}^s-\\eta H^{-1}(\\mathbf{w}^s) \\nabla \\mathcal{L}(\\mathbf{w}^s)\n\nSlow for large datasets. Works well if solution space is (near) convex\n\nQuasi-Newton methods (only L2)\n\nApproximate, faster to compute\n\nE.g. Limited-memory Broyden–Fletcher–Goldfarb–Shanno (lbfgs)\n\nDefault in sklearn for Logistic Regression\n\nSome hints on choosing solvers\n\nData scaling helps convergence, minimizes differences between solvers\n\n","type":"content","url":"/notebooks/linear-models#optimization-methods-solvers-for-cross-entropy-loss","position":49},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"In practice","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#in-practice-2","position":50},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"In practice","lvl4":"Logistic regression","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Logistic regression can also be found in sklearn.linear_model.\n\nC hyperparameter is the inverse regularization strength: C=\\alpha^{-1}\n\npenalty: type of regularization: L1, L2 (default), Elastic-Net, or None\n\nsolver: newton-cg, lbfgs (default), liblinear, sag, saga\n\nIncreasing C: less regularization, tries to overfit individual pointsfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C=1).fit(X_train, y_train)\n\nfrom sklearn.linear_model import LogisticRegression\n\n@interact\ndef plot_lr(C_log=(-3,4,0.1)):\n    # Still using artificial data\n    fig, ax = plt.subplots(figsize=(6*fig_scale,3*fig_scale))\n    mglearn.discrete_scatter(Xf[:, 0], Xf[:, 1], yf, ax=ax, s=10*fig_scale)\n    lr = LogisticRegression(C=10**C_log).fit(Xf, yf)\n    w = lr.coef_[0]\n    xx = np.linspace(7, 13)\n    yy = (-w[0] * xx - lr.intercept_[0]) / w[1]\n    ax.plot(xx, yy, c='k')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(\"C = {:.3f}, w1={:.3f}, w2={:.3f}\".format(10**C_log,w[0],w[1]))\n    ax.legend(loc=\"best\");\n    plt.show()\n\n\n\nif not interactive:\n    plot_lr(C_log=(4))\n\n\n\nAnalyze behavior on the breast cancer dataset\n\nUnderfitting if C is too small, some overfitting if C is too large\n\nWe use cross-validation because the dataset is small\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import cross_validate\n\nspam_data = fetch_openml(name=\"qsar-biodeg\", as_frame=True)\nX_C, y_C = spam_data.data, spam_data.target\n\nC=np.logspace(-3,6,num=19)\ntest_score=[]\ntrain_score=[]\nfor c in C:\n    lr = LogisticRegression(C=c)\n    scores = cross_validate(lr,X_C,y_C,cv=10, return_train_score=True)\n    test_score.append(np.mean(scores['test_score']))\n    train_score.append(np.mean(scores['train_score']))\nfig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\nax.set_xticks(range(19))\nax.set_xticklabels(np.round(C,3))\nax.set_xlabel('C')\nax.plot(test_score, lw=2*fig_scale, label='test score')\nax.plot(train_score, lw=2*fig_scale, label='train score')\nax.legend()\nplt.xticks(rotation=45);\n\n\n\nAgain, choose between L1 or L2 regularization (or elastic-net)\n\nSmall C overfits, L1 leads to sparse models\n\nX_C_train, X_C_test, y_C_train, y_C_test = train_test_split(X_C, y_C, random_state=0)\n\n@interact\ndef plot_logreg(C=(0.01,1000.0,0.1), penalty=['l1','l2']):\n    r = LogisticRegression(C=C, penalty=penalty, solver='liblinear').fit(X_C_train, y_C_train)\n    fig, ax = plt.subplots(figsize=(8*fig_scale,1.9*fig_scale))\n    ax.plot(r.coef_.T, 'o', markersize=6*fig_scale)\n    ax.set_title(\"C: {:.3f}, penalty: {}, score {:.2f} (training score {:.2f})\".format(C, penalty, r.score(X_C_test, y_C_test), r.score(X_C_train, y_C_train)),pad=0)\n    ax.set_xlabel(\"Coefficient index\", labelpad=0)\n    ax.set_ylabel(\"Coeff. magnitude\", labelpad=0, fontsize=10*fig_scale)\n    ax.tick_params(axis='both', pad=0)\n    ax.hlines(0, 40, len(r.coef_)-1)\n    ax.set_ylim(-10, 10)\n    ax.set_xlim(0, 40);\n    plt.tight_layout();\n    plt.show();\n\n\n\nif not interactive:\n    plot_logreg(0.001, 'l2')\n    plot_logreg(100, 'l2')\n    plot_logreg(100, 'l1')\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/linear-models#in-practice-2","position":51},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Ridge Classification","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#ridge-classification","position":52},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Ridge Classification","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Instead of log loss, we can also use ridge loss:\\mathcal{L}_{Ridge} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=1}^{p} w_i^2\n\nIn this case, target values {negative, positive} are converted to {-1,1}\n\nCan be solved similarly to Ridge regression:\n\nClosed form solution (a.k.a. Cholesky)\n\nGradient descent and variants\n\nE.g. Conjugate Gradient (CG) or Stochastic Average Gradient (SAG,SAGA)\n\nUse Cholesky for smaller datasets, Gradient descent for larger ones\n\n","type":"content","url":"/notebooks/linear-models#ridge-classification","position":53},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#support-vector-machines","position":54},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Decision boundaries close to training points may generalize badly\n\nVery similar (nearby) test point are classified as the other class\n\nChoose a boundary that is as far away from training points as possible\n\nThe support vectors are the training samples closest to the hyperplane\n\nThe margin is the distance between the separating hyperplane and the support vectors\n\nHence, our objective is to maximize the margin\n\n\n","type":"content","url":"/notebooks/linear-models#support-vector-machines","position":55},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Solving SVMs with Lagrange Multipliers","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#solving-svms-with-lagrange-multipliers","position":56},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Solving SVMs with Lagrange Multipliers","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Imagine a hyperplane (green) y= \\sum_1^p \\mathbf{w}_i * \\mathbf{x}_i + w_0 that has slope \\mathbf{w}, value ‘+1’ for the positive (red) support vectors, and ‘-1’ for the negative (blue) ones\n\nMargin between the boundary and support vectors is \\frac{y-w_0}{||\\mathbf{w}||}, with ||\\mathbf{w}|| = \\sum_i^p w_i^2\n\nWe want to find the weights that maximize \\frac{1}{||\\mathbf{w}||}. We can also do that by maximizing \\frac{1}{||\\mathbf{w}||^2}\n\nfrom sklearn.svm import SVC\n\n# we create 40 separable points\nnp.random.seed(0)\nsX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nsY = [0] * 20 + [1] * 20\n\n# fit the model\ns_clf = SVC(kernel='linear')\ns_clf.fit(sX, sY)\n\n@interact\ndef plot_svc_fit(rotationX=(0,20,1),rotationY=(90,180,1)):\n    # get the separating hyperplane\n    w = s_clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (s_clf.intercept_[0]) / w[1]\n    zz = np.linspace(-2, 2, 30)\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors\n    b = s_clf.support_vectors_[0]\n    yy_down = a * xx + (b[1] - a * b[0])\n    b = s_clf.support_vectors_[-1]\n    yy_up = a * xx + (b[1] - a * b[0])\n\n    # plot the line, the points, and the nearest vectors to the plane\n    fig = plt.figure(figsize=(7*fig_scale,4.5*fig_scale))\n    ax = plt.axes(projection=\"3d\")\n    ax.plot3D(xx, yy, [0]*len(xx), 'k-')\n    ax.plot3D(xx, yy_down, [0]*len(xx), 'k--')\n    ax.plot3D(xx, yy_up, [0]*len(xx), 'k--')\n\n    ax.scatter3D(s_clf.support_vectors_[:, 0], s_clf.support_vectors_[:, 1], [0]*len(s_clf.support_vectors_[:, 0]),\n                s=85*fig_scale, edgecolors='k', c='w')\n    ax.scatter3D(sX[:, 0], sX[:, 1], [0]*len(sX[:, 0]), c=sY, cmap=plt.cm.bwr, s=10*fig_scale )\n\n\n    # Planes\n    XX, YY = np.meshgrid(xx, yy)\n    if interactive:\n        ZZ = w[0]*XX+w[1]*YY+clf.intercept_[0]\n    else: # rescaling (for prints) messes up the Z values\n        ZZ = w[0]*XX/fig_scale+w[1]*YY/fig_scale+clf.intercept_[0]*fig_scale/2\n    ax.plot_wireframe(XX, YY, XX*0, rstride=5, cstride=5, alpha=0.3, color='k', label='XY plane')\n    ax.plot_wireframe(XX, YY, ZZ, rstride=5, cstride=5, alpha=0.3, color='g', label='hyperplane')\n\n    ax.set_axis_off()\n    ax.view_init(rotationX, rotationY) # Use this to rotate the figure\n    ax.dist = 6\n    plt.tight_layout()\n    plt.show()\n\n\n\nif not interactive:\n    plot_svc_fit(9,135)\n\n\n\n","type":"content","url":"/notebooks/linear-models#solving-svms-with-lagrange-multipliers","position":57},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl6":"Geometric interpretation","lvl5":"Solving SVMs with Lagrange Multipliers","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl6","url":"/notebooks/linear-models#geometric-interpretation","position":58},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl6":"Geometric interpretation","lvl5":"Solving SVMs with Lagrange Multipliers","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"We want to maximize f = \\frac{1}{||w||^2} (blue contours)\n\nThe hyperplane (red) must be > 1 for all positive examples:g(\\mathbf{w}) = \\mathbf{w} \\mathbf{x_i} + w_0 > 1 \\,\\,\\, \\forall{i}, y(i)=1\n\nFind the weights \\mathbf{w} that satify g but maximize f\n\n","type":"content","url":"/notebooks/linear-models#geometric-interpretation","position":59},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl6":"Solution","lvl5":"Solving SVMs with Lagrange Multipliers","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl6","url":"/notebooks/linear-models#solution","position":60},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl6":"Solution","lvl5":"Solving SVMs with Lagrange Multipliers","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"A quadratic loss function with linear constraints can be solved with Lagrangian multipliers\n\nThis works by assigning a weight a_i (called a dual coefficient) to every data point x_i\n\nThey reflect how much individual points influence the weights \\mathbf{w}\n\nThe points with non-zero a_i are the support vectors\n\nNext, solve the following Primal objective:\n\ny_i=\\pm1 is the correct class for example x_i\\mathcal{L}_{Primal} = \\frac{1}{2} ||\\mathbf{w}||^2 - \\sum_{i=1}^{n} a_i y_i (\\mathbf{w} \\mathbf{x_i}  + w_0) + \\sum_{i=1}^{n} a_i\n\nso that\\mathbf{w} = \\sum_{i=1}^{n} a_i y_i \\mathbf{x_i}a_i \\geq 0 \\quad \\text{and} \\quad \\sum_{i=1}^{l} a_i y_i = 0\n\nIt has a Dual formulation as well (See ‘Elements of Statistical Learning’ for the full derivation):\\mathcal{L}_{Dual} = \\sum_{i=1}^{l} a_i - \\frac{1}{2} \\sum_{i,j=1}^{l} a_i a_j y_i y_j (\\mathbf{x_i} \\mathbf{x_j})\n\nso thata_i \\geq 0 \\quad \\text{and} \\quad \\sum_{i=1}^{l} a_i y_i = 0\n\nComputes the dual coefficients directly. A number l of these are non-zero (sparseness).\n\nDot product \\mathbf{x_i} \\mathbf{x_j} can be interpreted as the closeness between points \\mathbf{x_i} and \\mathbf{x_j}\n\nWe can replace the dot product with other similarity functions (kernels)\n\n\\mathcal{L}_{Dual} increases if nearby support vectors \\mathbf{x_i} with high weights a_i have different class y_i\n\n\\mathcal{L}_{Dual} also increases with the number of support vectors l and their weights a_i\n\nCan be solved with quadratic programming, e.g. Sequential Minimal Optimization (SMO)\n\nExample result. The circled samples are support vectors, together with their coefficients.\n\nfrom sklearn.svm import SVC\n\n# Plot SVM support vectors\ndef plot_linear_svm(X,y,C,ax):\n\n    clf = SVC(kernel='linear', C=C)\n    clf.fit(X, y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane\n    yy_down = (-1-w[0]*xx-clf.intercept_[0])/w[1]\n    yy_up = (1-w[0]*xx-clf.intercept_[0])/w[1]\n\n    # plot the line, the points, and the nearest vectors to the plane\n    ax.set_title('C = %s' % C)\n    ax.plot(xx, yy, 'k-')\n    ax.plot(xx, yy_down, 'k--')\n    ax.plot(xx, yy_up, 'k--')\n    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n                s=85*fig_scale, edgecolors='gray', c='w', zorder=10, lw=1*fig_scale)\n    ax.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.bwr)\n    ax.axis('tight')\n\n    # Add coefficients\n    for i, coef in enumerate(clf.dual_coef_[0]):\n        ax.annotate(\"%0.2f\" % (coef), (clf.support_vectors_[i, 0]+0.1,clf.support_vectors_[i, 1]+0.35), fontsize=10*fig_scale, zorder=11)\n\n    ax.set_xlim(np.min(X[:, 0])-0.5, np.max(X[:, 0])+0.5)\n    ax.set_ylim(np.min(X[:, 1])-0.5, np.max(X[:, 1])+0.5)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\n# we create 40 separable points\nnp.random.seed(0)\nsvm_X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nsvm_Y = [0] * 20 + [1] * 20\nsvm_fig, svm_ax = plt.subplots(figsize=(8*fig_scale,5*fig_scale))\nplot_linear_svm(svm_X,svm_Y,1,svm_ax)\n\n\n\n","type":"content","url":"/notebooks/linear-models#solution","position":61},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Making predictions","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#making-predictions","position":62},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Making predictions","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"a_i will be 0 if the training point lies on the right side of the decision boundary and outside the margin\n\nThe training samples for which a_i is not 0 are the support vectors\n\nHence, the SVM model is completely defined by the support vectors and their dual coefficients (weights)\n\nKnowing the dual coefficients a_i, we can find the weights w for the maximal margin separating hyperplane\n\nAnd we could classify a new sample \\mathbf{u} by looking at the sign of \\mathbf{w}\\mathbf{u}+w_0\\mathbf{w} = \\sum_{i=1}^{l} a_i y_i \\mathbf{x_i}\n\nHowever, we don’t need to compute \\mathbf{w} to make predictions. We only need to look at the support vectors.\n\n","type":"content","url":"/notebooks/linear-models#making-predictions","position":63},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl6":"SVMs and kNN","lvl5":"Making predictions","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl6","url":"/notebooks/linear-models#svms-and-knn","position":64},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl6":"SVMs and kNN","lvl5":"Making predictions","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Remember, we will classify a new point \\mathbf{u} by looking at the sign of:f(x) = \\mathbf{w}\\mathbf{u}+w_0 = \\sum_{i=1}^{l} a_i y_i \\mathbf{x_i}\\mathbf{u}+w_0\n\nWeighted k-nearest neighbor is a generalization of the k-nearest neighbor classifier. It classifies points by evaluating:f(x) = \\sum_{i=1}^{k} a_i y_i dist(x_i, u)^{-1}\n\nHence: SVM’s predict much the same way as k-NN, only:\n\nThey only consider the truly important points (the support vectors): much faster\n\nThe number of neighbors is the number of support vectors\n\nThe distance function is an inner product of the inputs (or another kernel)\n\nGiven \\mathbf{u}, we predict by looking at the classes of the support vectors, weighted by their distance.\n\n","type":"content","url":"/notebooks/linear-models#svms-and-knn","position":65},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Regularized (soft margin) SVMs","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#regularized-soft-margin-svms","position":66},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Regularized (soft margin) SVMs","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"If the data is not linearly separable, (hard) margin maximization becomes meaningless\n\nRelax the contraint by allowing an error \\xi_{i}: y_i (\\mathbf{w}\\mathbf{x_i} + w_0) \\geq 1 - \\xi_{i}\n\nOr (since \\xi_{i} \\geq 0): \\xi_{i} =  max(0,1-y_i\\cdot(\\mathbf{w}\\mathbf{x_i} + w_0))\n\nThe sum over all points is called hinge loss: \\sum_i^n \\xi_{i}\n\nAttenuating the error component with a hyperparameter C, we get the objective\\mathcal{L}(\\mathbf{w}) = ||\\mathbf{w}||^2 + C \\sum_i^n \\xi_{i}\n\nCan still be solved with quadratic programming\n\ndef hinge_loss(yHat, y):\n    if y == 1:\n        return np.maximum(0,1-yHat)\n    else:\n        return np.maximum(0,1+yHat)\n\nfig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\nx = np.linspace(-2,2,100)\n\nax.plot(x,hinge_loss(x, 1),lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\nax.plot(x,hinge_loss(x, 0),lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\nax.set_xlabel(r\"Prediction value $\\hat{y}$\")\nax.set_ylabel(\"Hinge loss\")\nplt.grid()\nplt.legend();\n\n\n\n","type":"content","url":"/notebooks/linear-models#regularized-soft-margin-svms","position":67},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Sidenote: Least Squares SVMs","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#sidenote-least-squares-svms","position":68},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Sidenote: Least Squares SVMs","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"We can also use the squares of all the errors, or squared hinge loss: \\sum_i^n \\xi_{i}^2\n\nThis yields the Least Squares SVM objective\\mathcal{L}(\\mathbf{w}) = ||\\mathbf{w}||^2 + C \\sum_i^n \\xi_{i}^2\n\nCan be solved with Lagrangian Multipliers and a set of linear equations\n\nStill yields support vectors and still allows kernelization\n\nSupport vectors are not sparse, but pruning techniques exist\n\nfig, ax = plt.subplots(figsize=(6*fig_scale,2*fig_scale))\nx = np.linspace(-2,2,100)\n\nax.plot(x,hinge_loss(x, 1)** 2,lw=2*fig_scale,c='b',label='true label = 1', linestyle='-')\nax.plot(x,hinge_loss(x, 0)** 2,lw=2*fig_scale,c='r',label='true label = 0', linestyle='-')\nax.set_xlabel(r\"Prediction value $\\hat{y}$\")\nax.set_ylabel(\"Squared hinge loss\")\nplt.grid()\nplt.legend();\n\n\n\n","type":"content","url":"/notebooks/linear-models#sidenote-least-squares-svms","position":69},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Effect of regularization on margin and support vectors","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#effect-of-regularization-on-margin-and-support-vectors","position":70},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Effect of regularization on margin and support vectors","lvl4":"Support vector machines","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"SVM’s Hinge loss acts like L1 regularization, yields sparse models\n\nC is the inverse regularization strength (inverse of \\alpha in Lasso)\n\nLarger C: fewer support vectors, smaller margin, more overfitting\n\nSmaller C: more support vectors, wider margin, less overfitting\n\nNeeds to be tuned carefully to the data\n\nfig, svm_axes = plt.subplots(nrows=1, ncols=2, figsize=(12*fig_scale, 4*fig_scale))\nplot_linear_svm(svm_X,svm_Y,1,svm_axes[0])\nplot_linear_svm(svm_X,svm_Y,0.05,svm_axes[1])\n\n\n\nSame for non-linearly separable data\n\nsvm_X = np.r_[np.random.randn(20, 2) - [1, 1], np.random.randn(20, 2) + [1, 1]]\nfig, svm_axes = plt.subplots(nrows=1, ncols=2, figsize=(12*fig_scale, 5*fig_scale))\nplot_linear_svm(svm_X,svm_Y,1,svm_axes[0])\nplot_linear_svm(svm_X,svm_Y,0.05,svm_axes[1])\n\n\n\nLarge C values can lead to overfitting (e.g. fitting noise), small values can lead to underfitting\n\nmglearn.plots.plot_linear_svc_regularization()\n\n\n\n","type":"content","url":"/notebooks/linear-models#effect-of-regularization-on-margin-and-support-vectors","position":71},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Kernelization","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#kernelization","position":72},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Kernelization","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Sometimes we can separate the data better by first transforming it to a higher dimensional space \\Phi(x)\n\nThis transformation \\Phi is called a feature map (but can be expensive)\n\nFor certain \\Phi, we know the function k that computes the dot product in \\Phi(x):  k(\\mathbf{x_i},\\mathbf{x_j}) = \\Phi(\\mathbf{x_i}) \\cdot \\Phi(\\mathbf{x_j})\n\nThis kernel function k(\\mathbf{x_i},\\mathbf{x_j}) computes the dot product without having to construct (reproduce) \\Phi(x)\n\nKernel trick: if your loss function has a dot product, you can simply replace it with a kernel!\n\nFor SVMs (in dual form), replacing (\\mathbf{x_i}\\mathbf{x_j}) \\rightarrow k(\\mathbf{x_i},\\mathbf{x_j}) yields a kernelized SVM:\n\\mathcal{L}_{Dual} (a_i, k) = \\sum_{i=1}^{l} a_i - \\frac{1}{2} \\sum_{i,j=1}^{l} a_i a_j y_i y_j k(\\mathbf{x_i},\\mathbf{x_j})\n\n","type":"content","url":"/notebooks/linear-models#kernelization","position":73},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Polynomial kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#polynomial-kernel","position":74},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Polynomial kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"The polynomial kernel (for degree d \\in \\mathbb{N}) reproduces the polynomial feature map[1, x_1, ..., x_p] \\xrightarrow{\\phi} [1, x_1, ..., x_p, x_1^2, ..., x_p^2, ..., x_p^d, x_1 x_2, ..., x_{p-1} x_p]\n\nIt can be easily computed from the original dot product:k_{poly}(\\mathbf{x_1},\\mathbf{x_2}) = (\\gamma (\\mathbf{x_1} \\cdot \\mathbf{x_2}) + c_0)^d\n\nIt has two more hyperparameters, but you can usually leave them at default\n\n\\gamma is a scaling hyperparameter (default \\frac{1}{p})\n\nc_0 is a hyperparameter (default 1) to trade off influence of higher-order terms\n\nBy simply replacing the dot product with a kernel we can learn non-linear SVMs!\n\nIt is technically still linear in \\Phi(x), but in our original space the boundary becomes a polynomial curve\n\nPrediction still happens as before, but the influence of each support vector drops of polynomially (with degree d)\n\nfrom sklearn import svm\n\ndef plot_svm_kernels(kernels, poly_degree=3, gamma=2, C=1, size=4):\n    # Our dataset and targets\n    X = np.c_[(.4, -.7),\n              (-1.5, -1),\n              (-1.4, -.9),\n              (-1.3, -1.2),\n              (-1.1, -.2),\n              (-1.2, -.4),\n              (-.5, 1.2),\n              (-1.5, 2.1),\n              (1, 1),\n              # --\n              (1.3, .8),\n              (1.2, .5),\n              (.2, -2),\n              (.5, -2.4),\n              (.2, -2.3),\n              (0, -2.7),\n              (1.3, 2.1)].T\n    Y = [0] * 8 + [1] * 8\n\n    # figure number\n    fig, axes = plt.subplots(-(-len(kernels)//3), min(len(kernels),3), figsize=(min(len(kernels),3)*size*1.2*fig_scale, -(-len(kernels)//3)*size*fig_scale), tight_layout=True)\n    if len(kernels) == 1:\n        axes = np.array([axes])\n    if not isinstance(gamma,list):\n        gamma = [gamma]*len(kernels)\n    if not isinstance(C,list):\n        C = [C]*len(kernels)\n    # fit the model\n    for kernel, ax, g, c in zip(kernels,axes.reshape(-1),gamma,C):\n        clf = svm.SVC(kernel=kernel, gamma=g, C=c, degree=poly_degree)\n        clf.fit(X, Y)\n\n        # plot the line, the points, and the nearest vectors to the plane\n        if kernel == 'rbf':\n            ax.set_title(r\"kernel = {}, $\\gamma$={}, C={}\".format(kernel, g, c), pad=0.1)\n        else:\n            ax.set_title('kernel = %s' % kernel,pad=0.1)\n\n        ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n                    s=25, edgecolors='grey', c='w', zorder=10, linewidths=0.5)\n        ax.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.bwr, s=10*fig_scale)\n\n        for i, coef in enumerate(clf.dual_coef_[0]):\n            ax.annotate(\"%0.2f\" % (coef), (clf.support_vectors_[i, 0]+0.1,clf.support_vectors_[i, 1]+0.25), zorder=11, fontsize=3)\n\n        ax.axis('tight')\n        x_min = -3\n        x_max = 3\n        y_min = -3\n        y_max = 3\n\n        XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n        Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n        # Put the result into a color plot\n        Z = Z.reshape(XX.shape)\n        #plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.bwr, alpha=0.1)\n        ax.contour(XX, YY, Z, colors=['k', 'k', 'k', 'k', 'k'], linestyles=['--', ':', '-', ':', '--'],\n                    levels=[-1, -0.5, 0, 0.5, 1])\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\nplot_svm_kernels(['linear', 'poly'],poly_degree=3,size=3.5)\n\n\n\n","type":"content","url":"/notebooks/linear-models#polynomial-kernel","position":75},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Radial Basis Function (RBF) kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#radial-basis-function-rbf-kernel","position":76},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Radial Basis Function (RBF) kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"The RBF or Gaussian kernel (of width \\gamma > 0) is related to the Taylor series expansion of e^x\\Phi(x) = e^{-x^2/2\\gamma^2} \\Big[ 1, \\sqrt{\\frac{1}{1!\\gamma^2}}x,\\sqrt{\\frac{1}{2!\\gamma^4}}x^2,\\sqrt{\\frac{1}{3!\\gamma^6}}x^3,\\ldots\\Big]^T\n\nIt is a function of how closely together two data points are:k_{RBF}(\\mathbf{x_1},\\mathbf{x_2}) = exp(-\\gamma ||\\mathbf{x_1} - \\mathbf{x_2}||^2)\n\nThe influence of a point \\mathbf{x_2} on point \\mathbf{x_1} drops off exponentially with its distance to \\mathbf{x_1}\n\nThe influence of each support vector now drops of exponentially\n\nHence, predictions are only affected by very nearby support vectors\n\nRBF kernels are therefore called local kernels\n\nplot_svm_kernels(['linear', 'rbf'],poly_degree=3,gamma=2,size=4)\n\n\n\nThe kernel width (\\gamma) defines how sharply the local influence decays\n\nActs as a regularizer: low \\gamma causes underfitting and high \\gamma causes overfitting\n\nSVM’s C parameter (inverse regularizer) is still at play and thus interacts with \\gamma\n\n@interact\ndef plot_rbf_data(gamma=(0.1,10,0.5),C=(0.01,5,0.1)):\n    plot_svm_kernels(['rbf'],gamma=gamma,C=C,size=5)\n\n\n\nif not interactive:\n    plot_svm_kernels(['rbf'],gamma=0.5,C=1,size=5)\n\n\n\n","type":"content","url":"/notebooks/linear-models#radial-basis-function-rbf-kernel","position":77},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Kernelization sidenotes (optional)","lvl4":"Radial Basis Function (RBF) kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#kernelization-sidenotes-optional","position":78},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"Kernelization sidenotes (optional)","lvl4":"Radial Basis Function (RBF) kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"You can invent many more feature maps and corresponding kernels (eg. for text, graphs,...)\n\nHowever, learning deep learning embeddings from lots of data often works better\n\nYou can also kernelize Ridge regression, Logistic regression, Perceptrons, Support Vector Regression,...\n\nThe Representer theorem will give you the corresponding loss function\n\nFor more detail see the Kernelization lecture under extra materials.\n\n","type":"content","url":"/notebooks/linear-models#kernelization-sidenotes-optional","position":79},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"SVMs in scikit-learn","lvl4":"Radial Basis Function (RBF) kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl5","url":"/notebooks/linear-models#svms-in-scikit-learn","position":80},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl5":"SVMs in scikit-learn","lvl4":"Radial Basis Function (RBF) kernel","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"svm.LinearSVC: faster for large datasets\n\nAllows choosing between the primal or dual. Primal recommended when n >> p\n\nReturns coef_ (\\mathbf{w}) and intercept_ (w_0)\n\nsvm.SVC allows different kernels to be used\n\nAlso returns support_vectors_ (the support vectors) and the dual_coef_ a_i\n\nScales at least quadratically with the number of samples n\n\nsvm.LinearSVR and svm.SVR are variants for regressionclf = svm.SVC(kernel='linear') # or 'RBF' or 'Poly'\nclf.fit(X, Y)\nprint(\"Support vectors:\", clf.support_vectors_[:])\nprint(\"Coefficients:\", clf.dual_coef_[:])\n\nfrom sklearn import svm\n\n# Linearly separable dat\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# Fit the model\nclf = svm.SVC(kernel='linear')\nclf.fit(X, Y)\n\n# Get the support vectors and weights\nprint(\"Support vectors:\")\nprint(clf.support_vectors_[:])\nprint(\"Coefficients:\")\nprint(clf.dual_coef_[:])\n\n\n\n","type":"content","url":"/notebooks/linear-models#svms-in-scikit-learn","position":81},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Solving SVMs with Gradient Descent","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#solving-svms-with-gradient-descent","position":82},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Solving SVMs with Gradient Descent","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"SVMs can, alternatively, be solved using gradient decent\n\nGood for large datasets, but does not yield support vectors or kernelization\n\nHinge loss is not differentiable but convex, and has a subgradient:\\mathcal{L_{Hinge}}(\\mathbf{w}) =  max(0,1-y_i (\\mathbf{w}\\mathbf{x_i} + w_0))\\frac{\\partial \\mathcal{L_{Hinge}}}{\\partial w_i} =  \\begin{cases}-y_i x_i & y_i (\\mathbf{w}\\mathbf{x_i} + w_0) < 1\\\\ 0 & \\text{otherwise} \\\\ \\end{cases}\n\nCan be solved with (stochastic) gradient descent\n\n","type":"content","url":"/notebooks/linear-models#solving-svms-with-gradient-descent","position":83},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Generalized SVMs","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#generalized-svms","position":84},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Generalized SVMs","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"There are many smoothed versions of hinge loss:\n\nSquared hinge loss:\n\nAlso known as Ridge classification\n\nLeast Squares SVM: allows kernelization (using a linear equation solver)\n\nModified Huber loss: squared hinge, but linear after -1. Robust against outliers\n\nLog loss: equivalent to logistic regression\n\nIn sklearn, SGDClassifier can be used with any of these. Good for large datasets.\n\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n    loss[z >= 1.] = 0\n    return loss\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nlw = 2*fig_scale\nfig, ax = plt.subplots(figsize=(8*fig_scale,4*fig_scale))\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], 'k-', lw=lw,\n         label=\"Zero-one loss\")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0), 'b-', lw=lw,\n         label=\"Hinge loss\")\nplt.plot(xx, -np.minimum(xx, 0), color='yellowgreen', lw=lw,\n         label=\"Perceptron loss\")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), 'r-', lw=lw,\n         label=\"Log loss\")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, 'c-', lw=lw,\n         label=\"Squared hinge loss\")\nplt.plot(xx, modified_huber_loss(xx, 1), color='darkorchid', lw=lw,\n         linestyle='--', label=\"Modified Huber loss\")\nplt.ylim((0, 7))\nplt.legend(loc=\"upper right\")\nplt.xlabel(r\"Decision function $f(x)$\")\nplt.ylabel(\"$Loss(y=1, f(x))$\")\nplt.grid()\nplt.legend();\n\n\n\n","type":"content","url":"/notebooks/linear-models#generalized-svms","position":85},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Perceptron","lvl3":"Linear models for Classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#perceptron","position":86},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"Perceptron","lvl3":"Linear models for Classification","lvl2":"Linear models"},"content":"Represents a single neuron (node) with inputs x_i, a bias w_0, and output y\n\nEach connection has a (synaptic) weight w_i. The node outputs \\hat{y} = \\sum_{i}^n x_{i}w_i + w_0\n\nThe activation function (neuron output) is 1 if \\mathbf{xw} + w_0 > 0, -1 otherwise\n\nIdea: Update synapses only on misclassification, correct output by exactly \\pm1\n\nWeights can be learned with (stochastic) gradient descent and Hinge(0) loss\\mathcal{L}_{Perceptron} = max(0,-y_i (\\mathbf{w}\\mathbf{x_i} + w_0))\\frac{\\partial \\mathcal{L_{Perceptron}}}{\\partial w_i} =  \\begin{cases}-y_i x_i & y_i (\\mathbf{w}\\mathbf{x_i} + w_0) < 0\\\\ 0 & \\text{otherwise} \\\\ \\end{cases}\n\n","type":"content","url":"/notebooks/linear-models#perceptron","position":87},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear Models for multiclass classification","lvl2":"Linear models"},"type":"lvl3","url":"/notebooks/linear-models#linear-models-for-multiclass-classification","position":88},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear Models for multiclass classification","lvl2":"Linear models"},"content":"","type":"content","url":"/notebooks/linear-models#linear-models-for-multiclass-classification","position":89},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"one-vs-rest (aka one-vs-all)","lvl3":"Linear Models for multiclass classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#one-vs-rest-aka-one-vs-all","position":90},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"one-vs-rest (aka one-vs-all)","lvl3":"Linear Models for multiclass classification","lvl2":"Linear models"},"content":"Learn a binary model for each class vs. all other classes\n\nCreate as many binary models as there are classes\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(random_state=42)\nlinear_svm = LinearSVC().fit(X, y)\n\nplt.rcParams[\"figure.figsize\"] = (7*fig_scale,5*fig_scale)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y, s=10*fig_scale)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  mglearn.cm3.colors):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color, lw=2*fig_scale)\nplt.ylim(-10, 15)\nplt.xlim(-10, 8)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3));\n\n\n\nEvery binary classifiers makes a prediction, the one with the highest score (>0) wins\n\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=0.3)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y, s=10*fig_scale)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  mglearn.cm3.colors):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color, lw=2*fig_scale)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\");\n\n\n\n","type":"content","url":"/notebooks/linear-models#one-vs-rest-aka-one-vs-all","position":91},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"one-vs-one","lvl3":"Linear Models for multiclass classification","lvl2":"Linear models"},"type":"lvl4","url":"/notebooks/linear-models#one-vs-one","position":92},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl4":"one-vs-one","lvl3":"Linear Models for multiclass classification","lvl2":"Linear models"},"content":"An alternative is to learn a binary model for every combination of two classes\n\nFor C classes, this results in \\frac{C(C-1)}{2} binary models\n\nEach point is classified according to a majority vote amongst all models\n\nCan also be a ‘soft vote’: sum up the probabilities (or decision values) for all models. The class with the highest sum wins.\n\nRequires more models than one-vs-rest, but training each one is faster\n\nOnly the examples of 2 classes are included in the training data\n\nRecommended for algorithms than learn well on small datasets\n\nEspecially SVMs and Gaussian Processes\n\n%%HTML\n<style>\ntd {font-size: 16px}\nth {font-size: 16px}\n.rendered_html table, .rendered_html td, .rendered_html th {\n    font-size: 16px;\n}\n</style>\n\n\n\n","type":"content","url":"/notebooks/linear-models#one-vs-one","position":93},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear models overview","lvl2":"Linear models"},"type":"lvl3","url":"/notebooks/linear-models#linear-models-overview","position":94},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Linear models overview","lvl2":"Linear models"},"content":"Name\n\nRepresentation\n\nLoss function\n\nOptimization\n\nRegularization\n\nLeast squares\n\nLinear function (R)\n\nSSE\n\nCFS or SGD\n\nNone\n\nRidge\n\nLinear function (R)\n\nSSE + L2\n\nCFS or SGD\n\nL2 strength (\\alpha)\n\nLasso\n\nLinear function (R)\n\nSSE + L1\n\nCoordinate descent\n\nL1 strength (\\alpha)\n\nElastic-Net\n\nLinear function (R)\n\nSSE + L1 + L2\n\nCoordinate descent\n\n\\alpha, L1 ratio (\\rho)\n\nSGDRegressor\n\nLinear function (R)\n\nSSE, Huber, \\epsilon-ins,... + L1/L2\n\nSGD\n\nL1/L2, \\alpha\n\nLogistic regression\n\nLinear function (C)\n\nLog + L1/L2\n\nSGD, coordinate descent,...\n\nL1/L2, \\alpha\n\nRidge classification\n\nLinear function (C)\n\nSSE + L2\n\nCFS or SGD\n\nL2 strength (\\alpha)\n\nLinear SVM\n\nSupport Vectors\n\nHinge(1)\n\nQuadratic programming or SGD\n\nCost (C)\n\nKernelized SVM\n\nSupport Vectors\n\nHinge(1)\n\nQuadratic programming or SGD\n\nCost (C), \\gamma,...\n\nLeast Squares SVM\n\nSupport Vectors\n\nSquared Hinge\n\nLinear equations or SGD\n\nCost (C)\n\nPerceptron\n\nLinear function (C)\n\nHinge(0)\n\nSGD\n\nNone\n\nSGDClassifier\n\nLinear function (C)\n\nLog, (Sq.) Hinge, Mod. Huber,...\n\nSGD\n\nL1/L2, \\alpha\n\nSSE: Sum of Squared Errors\n\nCFS: Closed-form solution\n\nSGD: (Stochastic) Gradient Descent and variants\n\n(R)egression, (C)lassification\n\n","type":"content","url":"/notebooks/linear-models#linear-models-overview","position":95},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Summary","lvl2":"Linear models"},"type":"lvl3","url":"/notebooks/linear-models#summary","position":96},{"hierarchy":{"lvl1":"Lecture 2. Linear models","lvl3":"Summary","lvl2":"Linear models"},"content":"Linear models\n\nGood for very large datasets (scalable)\n\nGood for very high-dimensional data (not for low-dimensional data)\n\nCan be used to fit non-linear or low-dim patterns as well (see later)\n\nPreprocessing: e.g. Polynomial or Poisson transformations\n\nGeneralized linear models (kernelization)\n\nRegularization is important. Tune the regularization strength (\\alpha)\n\nRidge (L2): Good fit, sometimes sensitive to outliers\n\nLasso (L1): Sparse models: fewer features, more interpretable, faster\n\nElastic-Net: Trade-off between both, e.g. for correlated features\n\nMost can be solved by different optimizers (solvers)\n\nClosed form solutions or quadratic/linear solvers for smaller datasets\n\nGradient descent variants (SGD,CD,SAG,CG,...) for larger ones\n\nMulti-class classification can be done using a one-vs-all approach","type":"content","url":"/notebooks/linear-models#summary","position":97},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation"},"type":"lvl1","url":"/notebooks/model-evaluation","position":0},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation"},"content":"Can I trust you?\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots\nif interactive:\n    fig_scale = 0.9\n    plt.rcParams.update(print_config)\nelse:\n    fig_scale = 1.25\n\n\n\n","type":"content","url":"/notebooks/model-evaluation","position":1},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Evaluation"},"type":"lvl2","url":"/notebooks/model-evaluation#evaluation","position":2},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Evaluation"},"content":"To know whether we can trust our method or system, we need to evaluate it.\n\nModel selection: choose between different models in a data-driven way.\n\nIf you cannot measure it, you cannot improve it.\n\nConvince others that your work is meaningful\n\nPeers, leadership, clients, yourself(!)\n\nWhen possible, try to interpret what your model has learned\n\nThe signal your model found may just be an artifact of your biased data\n\nSee ‘Why Should I Trust You?’ by Marco Ribeiro et al.\n\n","type":"content","url":"/notebooks/model-evaluation#evaluation","position":3},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Designing Machine Learning systems","lvl2":"Evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#designing-machine-learning-systems","position":4},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Designing Machine Learning systems","lvl2":"Evaluation"},"content":"Just running your favourite algorithm is usually not a great way to start\n\nConsider the problem: How to measure success? Are there costs involved?\n\nDo you want to understand phenomena or do black box modelling?\n\nAnalyze your model’s mistakes. Don’t just finetune endlessly.\n\nBuild early prototypes. Should you collect more, or additional data?\n\nShould the task be reformulated?\n\nOverly complex machine learning systems are hard to maintain\n\nSee ‘Machine Learning: The High Interest Credit Card of Technical Debt’\n\n","type":"content","url":"/notebooks/model-evaluation#designing-machine-learning-systems","position":5},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Real world evaluations","lvl2":"Evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#real-world-evaluations","position":6},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Real world evaluations","lvl2":"Evaluation"},"content":"Evaluate predictions, but also how outcomes improve because of them\n\nBeware of feedback loops: predictions can influence future input data\n\nMedical recommendations, spam filtering, trading algorithms,...\n\nEvaluate algorithms in the wild.\n\nA/B testing: split users in groups, test different models in parallel\n\nBandit testing: gradually direct more users to the winning system\n\n","type":"content","url":"/notebooks/model-evaluation#real-world-evaluations","position":7},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Performance estimation techniques"},"type":"lvl2","url":"/notebooks/model-evaluation#performance-estimation-techniques","position":8},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Performance estimation techniques"},"content":"Always evaluate models as if they are predicting future data\n\nWe do not have access to future data, so we pretend that some data is hidden\n\nSimplest way: the holdout (simple train-test split)\n\nRandomly split data (and corresponding labels) into training and test set (e.g. 75%-25%)\n\nTrain (fit) a model on the training data, score on the test data\n\nfrom sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit, train_test_split,\n                                     StratifiedKFold, GroupShuffleSplit,\n                                     GroupKFold, StratifiedShuffleSplit)\nfrom matplotlib.patches import Patch\nnp.random.seed(1338)\ncmap_data = plt.cm.brg\ncmap_group = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nn_splits = 4\n\n# Generate the class/group data\nn_points = 100\nX = np.random.randn(100, 10)\n\npercentiles_classes = [.1, .3, .6]\ny = np.hstack([[ii] * int(100 * perc)\n               for ii, perc in enumerate(percentiles_classes)])\n\n# Evenly spaced groups repeated once\nrng = np.random.RandomState(42)\ngroup_prior = rng.dirichlet([2]*10)\nrng.multinomial(100, group_prior)\ngroups = np.repeat(np.arange(10), rng.multinomial(100, group_prior))\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nfig, ax = plt.subplots(figsize=(8*fig_scale, 3*fig_scale))\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                         markers='o', ax=ax)\nmglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                         markers='^', ax=ax)\nax.legend([\"Train class 0\", \"Train class 1\", \"Train class 2\", \"Test class 0\",\n                \"Test class 1\", \"Test class 2\"], ncol=6,  loc=(-0.1, 1.1));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#performance-estimation-techniques","position":9},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"type":"lvl3","url":"/notebooks/model-evaluation#k-fold-cross-validation","position":10},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"content":"Each random split can yield very different models (and scores)\n\ne.g. all easy (of hard) examples could end up in the test set\n\nSplit data into k equal-sized parts, called folds\n\nCreate k splits, each time using a different fold as the test set\n\nCompute k evaluation scores, aggregate afterwards (e.g. take the mean)\n\nExamine the score variance to see how sensitive (unstable) models are\n\nLarge k gives better estimates (more training data), but is expensive\n\ndef plot_cv_indices(cv, X, y, group, ax, lw=2, show_groups=False, s=700, legend=True):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n    n_splits = cv.get_n_splits(X, y, group)\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter([n_splits - ii - 1] * len(indices), range(len(indices)),\n                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2, s=s)\n\n    # Plot the data classes and groups at the end\n    ax.scatter([-1] * len(X), range(len(X)), \n               c=y, marker='_', lw=lw, cmap=cmap_data, s=s)\n    yticklabels = ['class'] + list(range(1, n_splits + 1))\n    \n    if show_groups:\n        ax.scatter([-2] * len(X), range(len(X)), \n                   c=group, marker='_', lw=lw, cmap=cmap_group, s=s)\n        yticklabels.insert(0, 'group')\n\n    # Formatting\n    ax.set(xticks=np.arange(-1 - show_groups, n_splits), xticklabels=yticklabels,\n            ylabel='Sample index', xlabel=\"CV iteration\",\n            xlim=[-1.5 - show_groups, n_splits+.2], ylim=[-6, 100])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    if legend:\n        ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.2))],\n                  ['Testing set', 'Training set'], loc=(1.02, .8))\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.set_yticks(())\n    return ax\n\n\n\nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\ncv = KFold(5)\nplot_cv_indices(cv, X, y, groups, ax, s=700);\n\n\n\nCan you explain this result?kfold = KFold(n_splits=3)\ncross_val_score(logistic_regression, iris.data, iris.target, cv=kfold)\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\niris = load_iris()\nlogreg = LogisticRegression()\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores KFold(n_splits=3):\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n\n\n\nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\nplot_cv_indices(kfold, iris.data, iris.target, iris.target, ax, s=700)\nax.set_ylim((-6, 150));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#k-fold-cross-validation","position":11},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Stratified K-Fold cross-validation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"type":"lvl4","url":"/notebooks/model-evaluation#stratified-k-fold-cross-validation","position":12},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Stratified K-Fold cross-validation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"content":"If the data is unbalanced, some classes have only few samples\n\nLikely that some classes are not present in the test set\n\nStratification: proportions between classes are conserved in each fold\n\nOrder examples per class\n\nSeparate the samples of each class in k sets (strata)\n\nCombine corresponding strata into folds\n\nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\ncv = StratifiedKFold(5)\nplot_cv_indices(cv, X, y, groups, ax, s=700)\nax.set_ylim((-6, 100));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#stratified-k-fold-cross-validation","position":13},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Leave-One-Out cross-validation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"type":"lvl4","url":"/notebooks/model-evaluation#leave-one-out-cross-validation","position":14},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Leave-One-Out cross-validation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"content":"k fold cross-validation with k equal to the number of samples\n\nCompletely unbiased (in terms of data splits), but computationally expensive\n\nActually generalizes less well towards unseen data\n\nThe training sets are correlated (overlap heavily)\n\nOverfits on the data used for (the entire) evaluation\n\nA different sample of the data can yield different results\n\nRecommended only for small datasets\n\nfig, ax = plt.subplots(figsize=(20*fig_scale, 4*fig_scale))\ncv = KFold(33) # There are more than 33 classes, but this visualizes better.\nplot_cv_indices(cv, X, y, groups, ax, s=700)\nax.set_ylim((-6, 100));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#leave-one-out-cross-validation","position":15},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Shuffle-Split cross-validation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"type":"lvl4","url":"/notebooks/model-evaluation#shuffle-split-cross-validation","position":16},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Shuffle-Split cross-validation","lvl3":"K-fold Cross-validation","lvl2":"Performance estimation techniques"},"content":"Shuffles the data, samples (train_size) points randomly as the training set\n\nCan also use a smaller (test_size), handy with very large datasets\n\nNever use if the data is ordered (e.g. time series)\n\nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\ncv = ShuffleSplit(8, test_size=.2)\nplot_cv_indices(cv, X, y, groups, ax, n_splits, s=700)\nax.set_ylim((-6, 100))\nax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.2))],\n          ['Testing set', 'Training set'], loc=(.95, .8));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#shuffle-split-cross-validation","position":17},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"The Bootstrap","lvl2":"Performance estimation techniques"},"type":"lvl3","url":"/notebooks/model-evaluation#the-bootstrap","position":18},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"The Bootstrap","lvl2":"Performance estimation techniques"},"content":"Sample n (dataset size) data points, with replacement, as training set (the bootstrap)\n\nOn average, bootstraps include 66% of all data points (some are duplicates)\n\nUse the unsampled (out-of-bootstrap) samples as the test set\n\nRepeat k times to obtain k scores\n\nSimilar to Shuffle-Split with train_size=0.66, test_size=0.34 but without duplicates\n\nfrom sklearn.utils import resample\n\n# Toy implementation of bootstrapping\nclass Bootstrap:\n    def __init__(self, nr):\n        self.nr = nr\n    \n    def get_n_splits(self, X, y, groups=None):\n        return self.nr\n    \n    def split(self, X, y, groups=None):\n        indices = range(len(X))\n        splits = []\n        for i in range(self.nr):\n            train = resample(indices, replace=True, n_samples=len(X), random_state=i)\n            test = list(set(indices) - set(train))\n            splits.append((train, test))\n        return splits\n            \nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\ncv = Bootstrap(8)\nplot_cv_indices(cv, X, y, groups, ax, n_splits, s=700)\nax.set_ylim((-6, 100))\nax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.2))],\n          ['Testing set', 'Training set'], loc=(.95, .8));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#the-bootstrap","position":19},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Repeated cross-validation","lvl2":"Performance estimation techniques"},"type":"lvl3","url":"/notebooks/model-evaluation#repeated-cross-validation","position":20},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Repeated cross-validation","lvl2":"Performance estimation techniques"},"content":"Cross-validation is still biased in that the initial split can be made in many ways\n\nRepeated, or n-times-k-fold cross-validation:\n\nShuffle data randomly, do k-fold cross-validation\n\nRepeat n times, yields n times k scores\n\nUnbiased, very robust, but n times more expensive\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom matplotlib.patches import Rectangle\nfig, ax = plt.subplots(figsize=(10*fig_scale, 3*fig_scale))\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3)\nplot_cv_indices(cv, X, y, groups, ax, lw=2, s=400, legend=False)\nax.set_ylim((-6, 102))\nxticklabels = [\"class\"] + [f\"{repeat}x{split}\" for repeat in range(1, 4) for split in range(1, 6)]\nax.set_xticklabels(xticklabels)\nfor i in range(3):\n    rect = Rectangle((-.5 + i * 5, -2.), 5, 103, edgecolor='k', facecolor='none')\n    ax.add_artist(rect)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#repeated-cross-validation","position":21},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Cross-validation with groups","lvl2":"Performance estimation techniques"},"type":"lvl3","url":"/notebooks/model-evaluation#cross-validation-with-groups","position":22},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Cross-validation with groups","lvl2":"Performance estimation techniques"},"content":"Sometimes the data contains inherent groups:\n\nMultiple samples from same patient, images from same person,...\n\nData from the same person may end up in the training and test set\n\nWe want to measure how well the model generalizes to other people\n\nMake sure that data from one person are in either the train or test set\n\nThis is called grouping or blocking\n\nLeave-one-subject-out cross-validation: test set for each subject/group\n\nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\ncv = GroupKFold(5)\nplot_cv_indices(cv, X, y, groups, ax, s=700, show_groups=True)\nax.set_ylim((-6, 100));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#cross-validation-with-groups","position":23},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Time series","lvl2":"Performance estimation techniques"},"type":"lvl3","url":"/notebooks/model-evaluation#time-series","position":24},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Time series","lvl2":"Performance estimation techniques"},"content":"When the data is ordered, random test sets are not a good idea\n\nimport pandas as pd\napproval = pd.read_csv(\"https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv\")\nadults = approval.groupby(\"subgroup\").get_group('Adults')\nadults = adults.set_index('modeldate')[::-1]\nadults.approve_estimate.plot()\nax = plt.gca()\nplt.rcParams[\"figure.figsize\"] = (12*fig_scale,6*fig_scale)\nax.set_xlabel(\"\")\nxlim, ylim = ax.get_xlim(), ax.get_ylim()\nfor i in range(20):\n    rect = Rectangle((np.random.randint(0, xlim[1]), ylim[0]), 10, ylim[1]-ylim[0], facecolor='#FFAAAA')\n    ax.add_artist(rect)\nplt.title(\"Presidential approval estimates by fivethirtyeight\")\nplt.legend([rect], ['Random Test Set'] );\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#time-series","position":25},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Test-then-train (prequential evaluation)","lvl3":"Time series","lvl2":"Performance estimation techniques"},"type":"lvl4","url":"/notebooks/model-evaluation#test-then-train-prequential-evaluation","position":26},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Test-then-train (prequential evaluation)","lvl3":"Time series","lvl2":"Performance estimation techniques"},"content":"Every new sample is evaluated only once, then added to the training set\n\nCan also be done in batches (of n samples at a time)\n\nTimeSeriesSplit\n\nIn the kth split, the first k folds are the train set and the (k+1)th fold as the test set\n\nOften, a maximum training set size (or window) is used\n\nmore robust against concept drift (change in data over time)\n\nfrom sklearn.utils import shuffle\nfig, ax = plt.subplots(figsize=(6*fig_scale, 3*fig_scale))\ncv = TimeSeriesSplit(5, max_train_size=20)\nplot_cv_indices(cv, X, shuffle(y), groups, ax, s=700, lw=2)\nax.set_ylim((-6, 100))\nax.set_title(\"TimeSeriesSplit(5, max_train_size=20)\");\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#test-then-train-prequential-evaluation","position":27},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Choosing a performance estimation procedure","lvl2":"Performance estimation techniques"},"type":"lvl3","url":"/notebooks/model-evaluation#choosing-a-performance-estimation-procedure","position":28},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Choosing a performance estimation procedure","lvl2":"Performance estimation techniques"},"content":"No strict rules, only guidelines:\n\nAlways use stratification for classification (sklearn does this by default)\n\nUse holdout for very large datasets (e.g. >1.000.000 examples)\n\nOr when learners don’t always converge (e.g. deep learning)\n\nChoose k depending on dataset size and resources\n\nUse leave-one-out for very small datasets (e.g. <100 examples)\n\nUse cross-validation otherwise\n\nMost popular (and theoretically sound): 10-fold CV\n\nLiterature suggests 5x2-fold CV is better\n\nUse grouping or leave-one-subject-out for grouped data\n\nUse train-then-test for time series\n\n","type":"content","url":"/notebooks/model-evaluation#choosing-a-performance-estimation-procedure","position":29},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Evaluation Metrics for Classification"},"type":"lvl2","url":"/notebooks/model-evaluation#evaluation-metrics-for-classification","position":30},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Evaluation Metrics for Classification"},"content":"\n\n","type":"content","url":"/notebooks/model-evaluation#evaluation-metrics-for-classification","position":31},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Evaluation vs Optimization","lvl2":"Evaluation Metrics for Classification"},"type":"lvl3","url":"/notebooks/model-evaluation#evaluation-vs-optimization","position":32},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Evaluation vs Optimization","lvl2":"Evaluation Metrics for Classification"},"content":"Each algorithm optimizes a given objective function (on the training data)\n\nE.g. remember L2 loss in Ridge regression\n\\mathcal{L}_{Ridge} = \\sum_{n=1}^{N} (y_n-(\\mathbf{w}\\mathbf{x_n} + w_0))^2 + \\alpha \\sum_{i=0}^{p} w_i^2\n\nThe choice of function is limited by what can be efficiently optimized\n\nHowever, we evaluate the resulting model with a score that makes sense in the real world\n\nPercentage of correct predictions (on a test set)\n\nThe actual cost of mistakes (e.g. in money, time, lives,...)\n\nWe also tune the algorithm’s hyperparameters to maximize that score\n\n","type":"content","url":"/notebooks/model-evaluation#evaluation-vs-optimization","position":33},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl3","url":"/notebooks/model-evaluation#binary-classification","position":34},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"content":"We have a positive and a negative class\n\n2 different kind of errors:\n\nFalse Positive (type I error): model predicts positive while true label is negative\n\nFalse Negative (type II error): model predicts negative while true label is positive\n\nThey are not always equally important\n\nWhich side do you want to err on for a medical test?\n\n","type":"content","url":"/notebooks/model-evaluation#binary-classification","position":35},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Confusion matrices","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl4","url":"/notebooks/model-evaluation#confusion-matrices","position":36},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Confusion matrices","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"content":"We can represent all predictions (correct and incorrect) in a confusion matrix\n\nn by n array (n is the number of classes)\n\nRows correspond to true classes, columns to predicted classes\n\nCount how often samples belonging to a class C are classified as C or any other class.\n\nFor binary classification, we label these true negative (TN), true positive (TP), false negative (FN), false positive (FP)\n\n\n\nPredicted Neg\n\nPredicted Pos\n\nActual Neg\n\nTN\n\nFP\n\nActual Pos\n\nFN\n\nTP\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\ndata = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    data.data, data.target, stratify=data.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint(\"confusion_matrix(y_test, y_pred): \\n\", confusion_matrix(y_test, y_pred))\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#confusion-matrices","position":37},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Predictive accuracy","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl4","url":"/notebooks/model-evaluation#predictive-accuracy","position":38},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Predictive accuracy","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"content":"Accuracy can be computed based on the confusion matrix\n\nNot useful if the dataset is very imbalanced\n\nE.g. credit card fraud: is 99.99% accuracy good enough?\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\n3 models: very different predictions, same accuracy:\n\ndef plot_confusion_matrix(values, xlabel=\"predicted labels\", ylabel=\"true labels\", xticklabels=None,\n                          yticklabels=None, cmap=None, vmin=None, vmax=None, ax=None,\n                          fmt=\"{:.2f}\", xtickrotation=45, norm=None, fsize=8):\n    \n    if ax is None:\n        ax = plt.gca()\n    ax.figure.set_size_inches(6*fig_scale, 6*fig_scale)\n    values = np.array(values)  # Ensure 'values' is a numpy array for consistent handling\n    img = ax.pcolormesh(values, cmap=cmap, vmin=vmin, vmax=vmax, norm=norm)\n    ax.set_xlabel(xlabel, fontsize=8)\n    ax.set_ylabel(ylabel, fontsize=8)\n\n    # Setting the tick labels\n    ax.set_xticks(np.arange(values.shape[1]) + 0.5, minor=False)\n    ax.set_yticks(np.arange(values.shape[0]) + 0.5, minor=False)\n    ax.set_xticklabels(xticklabels or [], minor=False, rotation=xtickrotation, fontsize=8)\n    ax.set_yticklabels(yticklabels or [], minor=False, fontsize=8)\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(values.shape[0]):\n        for j in range(values.shape[1]):\n            ax.text(j + 0.5, i + 0.5, fmt.format(values[i, j]),\n                    ha=\"center\", va=\"center\", color=\"white\" if values[i, j] > vmax/2 else \"black\", fontsize=8)\n\n    ax.set_aspect('equal')  # Optional: set aspect ratio to be equal\n    ax.invert_yaxis()\n    return ax\n\n\n\n# Artificial 90-10 imbalanced target\ny_true = np.zeros(100, dtype=int)\ny_true[:10] = 1\ny_pred_1 = np.zeros(100, dtype=int)\ny_pred_2 = y_true.copy()\ny_pred_2[10:20] = 1\ny_pred_3 = y_true.copy()\ny_pred_3[5:15] = 1 - y_pred_3[5:15]\n\ndef plot_measure(measure):\n    fig, axes = plt.subplots(1, 3)\n    for i, (ax, y_pred) in enumerate(zip(axes, [y_pred_1, y_pred_2, y_pred_3])):\n        plot_confusion_matrix(confusion_matrix(y_true, y_pred), cmap='gray_r', ax=ax,\n                              xticklabels=[\"N\", \"P\"], yticklabels=[\"N\", \"P\"], xtickrotation=0, vmin=0, vmax=100)\n        ax.set_title(\"{}: {:.2f}\".format(measure.__name__,measure(y_true, y_pred)), fontsize=8*fig_scale)\n\n    plt.tight_layout()\n\n\n\nplot_measure(accuracy_score)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#predictive-accuracy","position":39},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Precision","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl4","url":"/notebooks/model-evaluation#precision","position":40},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Precision","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"content":"Use when the goal is to limit FPs\n\nClinical trails: you only want to test drugs that really work\n\nSearch engines: you want to avoid bad search results\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\nfrom sklearn.metrics import precision_score\nplot_measure(precision_score)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#precision","position":41},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Recall","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl4","url":"/notebooks/model-evaluation#recall","position":42},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Recall","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"content":"Use when the goal is to limit FNs\n\nCancer diagnosis: you don’t want to miss a serious disease\n\nSearch engines: You don’t want to omit important hits\n\nAlso know as sensitivity, hit rate, true positive rate (TPR)\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\nfrom sklearn.metrics import recall_score\nplot_measure(recall_score)\n\n\n\nComparison\n\n","type":"content","url":"/notebooks/model-evaluation#recall","position":43},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"F1-score","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl4","url":"/notebooks/model-evaluation#f1-score","position":44},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"F1-score","lvl3":"Binary classification","lvl2":"Evaluation Metrics for Classification"},"content":"Trades off precision and recall:\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n\nfrom sklearn.metrics import f1_score\nplot_measure(f1_score)\n\n\n\nClassification measure Zoo\n\nPrecision and recall\n\n","type":"content","url":"/notebooks/model-evaluation#f1-score","position":45},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Multi-class classification","lvl2":"Evaluation Metrics for Classification"},"type":"lvl3","url":"/notebooks/model-evaluation#multi-class-classification","position":46},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Multi-class classification","lvl2":"Evaluation Metrics for Classification"},"content":"Train models per class : one class viewed as positive, other(s) als negative, then average\n\nmicro-averaging: count total TP, FP, TN, FN (every sample equally important)\n\nmicro-precision, micro-recall, micro-F1, accuracy are all the same\n\\text{Precision:} \\frac{\\sum_{c=1}^C\\text{TP}_c}{\\sum_{c=1}^C\\text{TP}_c + \\sum_{c=1}^C\\text{FP}_c} \\xrightarrow{c=2} \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\nmacro-averaging: average of scores R(y_c,\\hat{y_c}) obtained on each class\n\nPreferable for imbalanced classes (if all classes are equally important)\n\nmacro-averaged recall is also called balanced accuracy\n\\frac{1}{C} \\sum_{c=1}^C R(y_c,\\hat{y_c})\n\nweighted averaging (w_c: ratio of examples of class c, aka support): \\sum_{c=1}^C w_c R(y_c,\\hat{y_c})\n\nfrom sklearn.metrics import classification_report\n\ndef report(y_pred):\n    fig = plt.figure()\n    ax = plt.subplot(111)\n    plot_confusion_matrix(confusion_matrix(y_true, y_pred), cmap='gray_r', ax=ax,\n                          xticklabels=[\"N\", \"P\"], yticklabels=[\"N\", \"P\"], xtickrotation=0, vmin=0, vmax=100, fsize=2.5)\n    ax.figure.set_size_inches(2*fig_scale, 2*fig_scale)\n    plt.gcf().text(1.1, 0.2, classification_report(y_true, y_pred), fontsize=8, fontname=\"Courier\")\n\n    plt.tight_layout()\nreport(y_pred_1)\nreport(y_pred_2)\nreport(y_pred_3)\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#multi-class-classification","position":47},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Other useful classification metrics","lvl2":"Evaluation Metrics for Classification"},"type":"lvl3","url":"/notebooks/model-evaluation#other-useful-classification-metrics","position":48},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Other useful classification metrics","lvl2":"Evaluation Metrics for Classification"},"content":"Cohen’s Kappa\n\nMeasures ‘agreement’ between different models (aka inter-rater agreement)\n\nTo evaluate a single model, compare it against a model that does random guessing\n\nSimilar to accuracy, but taking into account the possibility of predicting the right class by chance\n\nCan be weighted: different misclassifications given different weights\n\n1: perfect prediction, 0: random prediction, negative: worse than random\n\nWith p_0 = accuracy, and p_e = accuracy of random classifier:\n\\kappa = \\frac{p_o - p_e}{1 - p_e}\n\nMatthews correlation coefficient\n\nCorrects for imbalanced data, alternative for balanced accuracy or AUROC\n\n1: perfect prediction, 0: random prediction, -1: inverse prediction\nMCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}\n\n","type":"content","url":"/notebooks/model-evaluation#other-useful-classification-metrics","position":49},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Probabilistic evaluation"},"type":"lvl2","url":"/notebooks/model-evaluation#probabilistic-evaluation","position":50},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Probabilistic evaluation"},"content":"Classifiers can often provide uncertainty estimates of predictions.\n\nRemember that linear models actually return a numeric value.\n\nWhen \\hat{y}<0, predict class -1, otherwise predict class +1\n\\hat{y} = w_0 * x_0 + w_1 * x_1 + ... + w_p * x_p + b\n\nIn practice, you are often interested in how certain a classifier is about each class prediction (e.g. cancer treatments).\n\nMost learning methods can return at least one measure of confidence in their predicions.\n\nDecision function: floating point value for each sample (higher: more confident)\n\nProbability: estimated probability for each class\n\n","type":"content","url":"/notebooks/model-evaluation#probabilistic-evaluation","position":51},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"The decision function","lvl2":"Probabilistic evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#the-decision-function","position":52},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"The decision function","lvl2":"Probabilistic evaluation"},"content":"In the binary classification case, the return value of the decision function encodes how strongly the model believes a data point\nbelongs to the “positive” class.\n\nPositive values indicate preference for the positive class.\n\nThe range can be arbitrary, and can be affected by hyperparameters. Hard to interpret.\n\n# create and split a synthetic dataset\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_blobs\nXs, ys = make_blobs(centers=2, cluster_std=2.5, random_state=8)\n\n# we rename the classes \"blue\" and \"red\"\nys_named = np.array([\"blue\", \"red\"])[ys]\n\n# we can call train test split with arbitrary many arrays\n# all will be split in a consistent manner\nXs_train, Xs_test, ys_train_named, ys_test_named, ys_train, ys_test = \\\n    train_test_split(Xs, ys_named, ys, random_state=0)\n\n# build the logistic regression model\nlr = LogisticRegression()\nlr.fit(Xs_train, ys_train_named)\n\nfig, axes = plt.subplots(1, 2, figsize=(10*fig_scale, 3.5*fig_scale))\n    \nmglearn.tools.plot_2d_separator(lr, Xs, ax=axes[0], alpha=.4,\n                                fill=False, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(lr, Xs, ax=axes[1],\n                                            alpha=.4, cm=mglearn.ReBl)\n\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(Xs_test[:, 0], Xs_test[:, 1], ys_test,\n                             markers='^', ax=ax, s=7*fig_scale)\n    mglearn.discrete_scatter(Xs_train[:, 0], Xs_train[:, 1], ys_train,\n                             markers='o', ax=ax, s=7*fig_scale)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\ncbar.set_alpha(1)\ncbar.ax.tick_params(labelsize=8*fig_scale)\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1));  \n\n\n\n","type":"content","url":"/notebooks/model-evaluation#the-decision-function","position":53},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Predicting probabilities","lvl2":"Probabilistic evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#predicting-probabilities","position":54},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Predicting probabilities","lvl2":"Probabilistic evaluation"},"content":"Some models can also return a probability for each class with every prediction. These sum up to 1.\nWe can visualize them again. Note that the gradient looks different now.\n\nfig, axes = plt.subplots(1, 2, figsize=(10*fig_scale, 3.5*fig_scale))\n    \nmglearn.tools.plot_2d_separator(\n    lr, Xs, ax=axes[0], alpha=.4, fill=False, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(\n    lr, Xs, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\n\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(Xs_test[:, 0], Xs_test[:, 1], ys_test,\n                             markers='^', ax=ax, s=7*fig_scale)\n    mglearn.discrete_scatter(Xs_train[:, 0], Xs_train[:, 1], ys_train,\n                             markers='o', ax=ax, s=7*fig_scale)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\n# don't want a transparent colorbar\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\ncbar.set_alpha(1)\ncbar.ax.tick_params(labelsize=8*fig_scale)\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1));\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#predicting-probabilities","position":55},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Threshold calibration","lvl2":"Probabilistic evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#threshold-calibration","position":56},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Threshold calibration","lvl2":"Probabilistic evaluation"},"content":"By default, we threshold at 0 for  decision_function and 0.5 for predict_proba\n\nDepending on the application, you may want to threshold differently\n\nLower threshold yields fewer FN (better recall), more FP (worse precision), and vice-versa\n\nfrom mglearn.datasets import make_blobs\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom mglearn.tools import plot_2d_separator, plot_2d_scores, cm, discrete_scatter\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\nXs, ys = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\n                    random_state=22)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, ys, stratify=ys, random_state=0)\nsvc1 = SVC(gamma=.04).fit(Xs_train, ys_train)\n    \n    \n@interact\ndef plot_decision_threshold(threshold=(-1.2,1.3,0.1)):\n    fig, axes = plt.subplots(1, 2, figsize=(8*fig_scale, 2.2*fig_scale), subplot_kw={'xticks': (), 'yticks': ()})    \n    line = np.linspace(Xs_train.min(), Xs_train.max(), 100)\n\n    axes[0].set_title(\"decision with threshold {:.2f}\".format(threshold))\n    discrete_scatter(Xs_train[:, 0], Xs_train[:, 1], ys_train, ax=axes[0], s=7*fig_scale)\n    discrete_scatter(Xs_test[:, 0], Xs_test[:, 1], ys_test, ax=axes[0], markers='^', s=7*fig_scale)\n\n    plot_2d_scores(svc1, Xs_train, function=\"decision_function\", alpha=.7,\n                   ax=axes[0], cm=mglearn.ReBl)\n    plot_2d_separator(svc1, Xs_train, linewidth=3, ax=axes[0], threshold=threshold)\n    axes[0].set_xlim(Xs_train[:, 0].min(), Xs_train[:, 0].max())\n    axes[0].plot(line, np.array([10 for i in range(len(line))]), 'k:', linewidth=2)\n\n    axes[1].set_title(\"cross-section with threshold {:.2f}\".format(threshold))\n    axes[1].plot(line, svc1.decision_function(np.c_[line, 10 * np.ones(100)]), c='k')\n    dec = svc1.decision_function(np.c_[line, 10 * np.ones(100)])\n    contour = (dec > threshold).reshape(1, -1).repeat(10, axis=0)\n    axes[1].contourf(line, np.linspace(-1.5, 1.5, 10), contour, linewidth=2, alpha=0.4, cmap=cm)\n    discrete_scatter(Xs_test[:, 0], Xs_test[:, 1]*0, ys_test, ax=axes[1], markers='^', s=7*fig_scale)\n\n    axes[1].plot(line, np.array([threshold for i in range(len(line))]), 'r:', linewidth=3)\n    axes[1].tick_params(labelsize=8*fig_scale)\n    \n    axes[0].set_xlim(Xs_train[:, 0].min(), Xs_train[:, 0].max())\n    axes[1].set_xlim(Xs_train[:, 0].min(), Xs_train[:, 0].max())\n    axes[1].set_ylim(-1.5, 1.5)\n    axes[1].set_xticks(())\n    axes[1].set_yticks(np.arange(-1.5, 1.5, 0.5))\n    axes[1].yaxis.tick_right()\n    axes[1].set_ylabel(\"Decision value\")\n    \n    y_pred = svc1.decision_function(Xs_test)\n    y_pred = y_pred > threshold\n    axes[1].text(Xs_train.min()+1,1.2,\"Precision: {:.4f}\".format(precision_score(ys_test,y_pred)), size=7*fig_scale)\n    axes[1].text(Xs_train.min()+1,0.9,\"Recall: {:.4f}\".format(recall_score(ys_test,y_pred)), size=7*fig_scale)\n    plt.tight_layout();\n    plt.show()\n\n\n\nif not interactive:\n    plot_decision_threshold(0)\n    plot_decision_threshold(-0.9)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#threshold-calibration","position":57},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Precision-Recall curve","lvl2":"Probabilistic evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#precision-recall-curve","position":58},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Precision-Recall curve","lvl2":"Probabilistic evaluation"},"content":"The best trade-off between precision and recall depends on your application\n\nYou can have arbitrary high recall, but you often want reasonable precision, too.\n\nPlotting precision against recall for all possible thresholds yields a precision-recall curve\n\nChange the treshold until you find a sweet spot in the precision-recall trade-off\n\nOften jagged at high thresholds, when there are few positive examples left\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_curve\n\n# create a similar dataset as before, but with more samples\n# to get a smoother curve\nXp, yp = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2], random_state=22)\nXp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp, random_state=0)\nsvc2 = SVC(gamma=.05).fit(Xp_train, yp_train)\nrf2 = RandomForestClassifier(n_estimators=100).fit(Xp_train, yp_train)\n\n@interact\ndef plot_PR_curve(threshold=(-3.19,1.4,0.1), model=[svc2, rf2]):\n    if hasattr(model, \"predict_proba\"):\n        precision, recall, thresholds = precision_recall_curve(\n            yp_test, model.predict_proba(Xp_test)[:, 1])\n    else:\n        precision, recall, thresholds = precision_recall_curve(\n            yp_test, model.decision_function(Xp_test))\n    # find existing threshold closest to zero\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.figure(figsize=(10*fig_scale,4*fig_scale))\n    plt.plot(recall[close_zero], precision[close_zero], 'o', markersize=10,\n             label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n    plt.plot(recall, precision, lw=2, label=\"precision recall curve\")\n\n    if hasattr(model, \"predict_proba\"):\n        yp_pred = model.predict_proba(Xp_test)[:, 1] > threshold\n    else:\n        yp_pred = model.decision_function(Xp_test) > threshold\n    plt.plot(recall_score(yp_test,yp_pred), precision_score(yp_test,yp_pred), 'o', markersize=10, label=\"threshold {:.2f}\".format(threshold))\n\n    plt.ylabel(\"Precision\")\n    plt.xlabel(\"Recall\")\n    plt.legend(loc=\"best\", prop={\"size\":10});\n    plt.show()\n\n\n\nif not interactive:\n    plot_PR_curve(threshold=-0.99,model=svc2)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#precision-recall-curve","position":59},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Model selection","lvl3":"Precision-Recall curve","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#model-selection","position":60},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Model selection","lvl3":"Precision-Recall curve","lvl2":"Probabilistic evaluation"},"content":"Some models can achieve trade-offs that others can’t\n\nYour application may require very high recall (or very high precision)\n\nChoose the model that offers the best trade-off, given your application\n\nThe area under the PR curve (AUPRC) gives the best overall model\n\nfrom sklearn.metrics import auc\ncolors=['b','r','g','y']\n\ndef plot_PR_curves(models):\n    \n    plt.figure(figsize=(10*fig_scale,4*fig_scale))\n    for i, model in enumerate(models):\n        if hasattr(model, \"predict_proba\"):\n            precision, recall, thresholds = precision_recall_curve(\n                yp_test, model.predict_proba(Xp_test)[:, 1])\n            close_zero = np.argmin(np.abs(thresholds-0.5))\n        else:\n            precision, recall, thresholds = precision_recall_curve(\n                yp_test, model.decision_function(Xp_test))\n            close_zero = np.argmin(np.abs(thresholds))\n          \n        plt.plot(recall, precision, lw=2, c=colors[i], label=\"PR curve {}, Area: {:.4f}\".format(model, auc(recall, precision))) \n        plt.plot(recall[close_zero], precision[close_zero], 'o', markersize=10,\n                 fillstyle=\"none\", c=colors[i], mew=2, label=\"Default threshold {}\".format(model))\n        plt.ylabel(\"Precision\")\n        plt.xlabel(\"Recall\")\n        plt.legend(loc=\"lower left\", prop={\"size\":9});\n        \n#svc2 = SVC(gamma=0.01).fit(X_train, y_train)\nplot_PR_curves([svc2, rf2])\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#model-selection","position":61},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Hyperparameter effects","lvl3":"Precision-Recall curve","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#hyperparameter-effects","position":62},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Hyperparameter effects","lvl3":"Precision-Recall curve","lvl2":"Probabilistic evaluation"},"content":"Of course, hyperparameters affect predictions and hence also the shape of the curve\n\nsvc3 = SVC(gamma=0.01).fit(Xp_train, yp_train)\nsvc4 = SVC(gamma=1).fit(Xp_train, yp_train)\nplot_PR_curves([svc3, svc2, svc4])\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#hyperparameter-effects","position":63},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#receiver-operating-characteristics-roc","position":64},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"content":"Trade off true positive rate \\textit{TPR}= \\frac{TP}{TP + FN} with false positive rate \\textit{FPR} = \\frac{FP}{FP + TN}\n\nPlotting TPR against FPR for all possible thresholds yields a Receiver Operating Characteristics curve\n\nChange the treshold until you find a sweet spot in the TPR-FPR trade-off\n\nLower thresholds yield higher TPR (recall), higher FPR, and vice versa\n\nfrom sklearn.metrics import roc_curve\n\n@interact\ndef plot_ROC_curve(threshold=(-3.19,1.4,0.1), model=[svc2, rf2]):\n    if hasattr(model, \"predict_proba\"):\n        fpr, tpr, thresholds = roc_curve(yp_test, model.predict_proba(Xp_test)[:, 1])\n    else:\n        fpr, tpr, thresholds = roc_curve(yp_test, model.decision_function(Xp_test))\n    # find existing threshold closest to zero\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.figure(figsize=(10*fig_scale,4*fig_scale))\n    plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n    plt.plot(fpr, tpr, lw=2, label=\"ROC curve\")\n    \n    closest = np.argmin(np.abs(thresholds-threshold))\n    plt.plot(fpr[closest], tpr[closest], 'o', markersize=10, label=\"threshold {:.2f}\".format(threshold))\n    \n    plt.ylabel(\"TPR (recall)\")\n    plt.xlabel(\"FPR\")\n    plt.legend(loc=\"best\", prop={\"size\":10});\n    plt.show()\n\n\n\nif not interactive:\n    plot_ROC_curve(threshold=-0.99,model=svc2)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#receiver-operating-characteristics-roc","position":65},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Visualization","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#visualization","position":66},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Visualization","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"content":"Histograms show the amount of points with a certain decision value (for each class)\n\n\\textit{TPR}= \\frac{\\color{red}{TP}}{\\color{red}{TP} + \\color{magenta}{FN}} can be seen from the positive predictions (top histogram)\n\n\\textit{FPR} = \\frac{\\color{cyan}{FP}}{\\color{cyan}{FP} + \\color{blue}{TN}}  can be seen from the negative predictions (bottom histogram)\n\n# More data for a smoother curve\nXb, yb = make_blobs(n_samples=(4000, 4000), centers=2, cluster_std=[3, 3], random_state=7)\nXb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, random_state=0)\nsvc_roc = SVC(C=2).fit(Xb_train, yb_train)\nprobs_roc = svc_roc.decision_function(Xb_test)\n\n@interact\ndef plot_roc_threshold(threshold=(-2,2,0.1)):\n    fig = plt.figure(constrained_layout=True, figsize=(10*fig_scale,4*fig_scale))\n    axes = []\n    gs = fig.add_gridspec(2, 2)\n    axes.append(fig.add_subplot(gs[0, :-1]))\n    axes.append(fig.add_subplot(gs[1, :-1]))\n    axes.append(fig.add_subplot(gs[:, 1]))\n    \n    n=50 # number of histogram bins\n    color=['b','r']\n    color_fill=['b','c','m','r']\n    labels=['TN','FP','FN','TP']\n    \n    # Histograms\n    for label in range(2):\n        ps = probs_roc[yb_test == label] # get prediction for given label\n        p, x = np.histogram(ps, bins=n) # bin it into n bins\n        x = x[:-1] + (x[1] - x[0])/2 # convert bin edges to center\n        axes[1-label].plot(x, p, c=color[label], lw=2)\n        axes[1-label].fill_between(x[x<threshold], p[x<threshold], -5, facecolor=color_fill[2*label], label='{}: {}'.format(labels[2*label],np.sum(p[x<threshold])))\n        axes[1-label].fill_between(x[x>=threshold], p[x>threshold], -5, facecolor=color_fill[2*label+1], label='{}: {}'.format(labels[2*label+1],np.sum(p[x>=threshold])))\n        axes[1-label].set_title('Histogram of decision values for points with class {}'.format(label), fontsize=12*fig_scale)\n        axes[1-label].legend(prop={\"size\":10})\n        \n    #ROC curve\n    fpr, tpr, thresholds = roc_curve(yb_test, svc_roc.decision_function(Xb_test))\n    axes[2].plot(fpr, tpr, lw=2, label=\"ROC curve\", c='k')\n    closest = np.argmin(np.abs(thresholds-threshold))\n    axes[2].plot(fpr[closest], tpr[closest], 'o', markersize=10, label=\"threshold {:.2f}\".format(threshold))\n    \n    axes[2].set_title('ROC curve', fontsize=12*fig_scale)\n    axes[2].set_xlabel(\"FPR\")\n    axes[2].set_ylabel(\"TPR\")\n    axes[2].legend(prop={\"size\":10})\n    plt.show()\n\n\n\nif not interactive:\n    plot_roc_threshold(threshold=-0.99)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#visualization","position":67},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Model selection","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#model-selection-1","position":68},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Model selection","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"content":"Again, some models can achieve trade-offs that others can’t\n\nYour application may require minizing FPR (low FP), or maximizing TPR (low FN)\n\nThe area under the ROC curve (AUROC or AUC) gives the best overall model\n\nFrequently used for evaluating models on imbalanced data\n\nRandom guessing (TPR=FPR) or predicting majority class (TPR=FPR=1): 0.5 AUC\n\nfrom sklearn.metrics import auc\nfrom sklearn.dummy import DummyClassifier\n\ndef plot_ROC_curves(models):\n    fig = plt.figure(figsize=(10*fig_scale,4*fig_scale))\n    for i, model in enumerate(models):\n        if hasattr(model, \"predict_proba\"):\n            fpr, tpr, thresholds = roc_curve(\n                yb_test, model.predict_proba(Xb_test)[:, 1])\n            close_zero = np.argmin(np.abs(thresholds-0.5))\n        else:\n            fpr, tpr, thresholds = roc_curve(\n                yb_test, model.decision_function(Xb_test))\n            close_zero = np.argmin(np.abs(thresholds))\n        plt.plot(fpr, tpr, lw=2, c=colors[i], label=\"ROC curve {}, Area: {:.4f}\".format(model, auc(fpr, tpr))) \n        plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n                 fillstyle=\"none\", c=colors[i], mew=2) #label=\"Default threshold {}\".format(model)\n        plt.ylabel(\"TPR (recall)\")\n        plt.xlabel(\"FPR\")\n        plt.legend(loc=\"lower right\",prop={\"size\":10});\n        \nsvc = SVC(gamma=.1).fit(Xb_train, yb_train)\nrf = RandomForestClassifier(n_estimators=100).fit(Xb_train, yb_train)\ndc = DummyClassifier(strategy='most_frequent').fit(Xb_train, yb_train)\ndc2 = DummyClassifier(strategy='uniform').fit(Xb_train, yb_train)\nplot_ROC_curves([dc, dc2, svc, rf])\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#model-selection-1","position":69},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Multi-class AUROC (or AUPRC)","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#multi-class-auroc-or-auprc","position":70},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Multi-class AUROC (or AUPRC)","lvl3":"Receiver Operating Characteristics (ROC)","lvl2":"Probabilistic evaluation"},"content":"We again need to choose between micro- or macro averaging TPR and FPR.\n\nMicro-average if every sample is equally important (irrespective of class)\n\nMacro-average if every class is equally important, especially for imbalanced data\n\nfrom itertools import cycle\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom numpy import interp\nfrom sklearn.metrics import roc_auc_score\n\n# 3 class imbalanced data\nXi, yi = make_blobs(n_samples=(800, 500, 60), centers=3, cluster_std=[7.0, 2, 3.0], random_state=22)\nsizes = [800, 500, 60]\n\n# Binarize the output\nyi = label_binarize(yi, classes=[0, 1, 2])\nn_classes = yi.shape[1]\nXi_train, Xi_test, yi_train, yi_test = train_test_split(Xi, yi, test_size=.5, random_state=0)\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(SVC(probability=True))\ny_score = classifier.fit(Xi_train, yi_train).decision_function(Xi_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(yi_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(yi_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure(figsize=(10*fig_scale,4*fig_scale))\n\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=2, linestyle='-',\n             label='ROC curve of class {} (size: {}) (area = {:0.2f})'\n             ''.format(i, sizes[i], roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extension of ROC to multi-class')\nplt.legend(loc=\"lower right\", prop={\"size\":10})\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#multi-class-auroc-or-auprc","position":71},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"type":"lvl3","url":"/notebooks/model-evaluation#model-calibration","position":72},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"content":"For some models, the predicted uncertainty does not reflect the actual uncertainty\n\nIf a model is 90% sure that samples are positive, is it also 90% accurate on these?\n\nA model is called calibrated if the reported uncertainty actually matches how correct it is\n\nOverfitted models also tend to be over-confident\n\nLogisticRegression models are well calibrated since they learn probabilities\n\nSVMs are not well calibrated. Biased towards points close to the decision boundary.\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import brier_score_loss, accuracy_score\n\ndef load_data():\n    X, y = make_classification(n_samples=100000, n_features=20, random_state=0)\n    train_samples = 2000  # Samples used for training the models\n    X_train = X[:train_samples]\n    X_test = X[train_samples:]\n    y_train = y[:train_samples]\n    y_test = y[train_samples:]\n\n    return X_train, X_test, y_train, y_test\n\nXc_train, Xc_test, yc_train, yc_test = load_data()\n\ndef plot_calibration_curve(y_true, y_prob, n_bins=5, ax=None, hist=True, normalize=False):\n    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins)\n    if ax is None:\n        ax = plt.gca()\n    if hist:\n        ax.hist(y_prob, weights=np.ones_like(y_prob) / len(y_prob), alpha=.4,\n               bins=np.maximum(10, n_bins))\n    ax.plot([0, 1], [0, 1], ':', c='k')\n    curve = ax.plot(prob_pred, prob_true, marker=\"o\")\n\n    ax.set_xlabel(\"predicted probability\")\n    ax.set_ylabel(\"fraction of pos. samples\")\n    ax.set(aspect='equal')\n    return curve\n\n# Plot calibration curves for `models`, optionally show a calibrator run on a calibratee\ndef plot_calibration_comparison(models, calibrator=None, calibratee=None): \n    def get_probabilities(clf, X):\n        if hasattr(clf, \"predict_proba\"): # Use probabilities if classifier has predict_proba\n            prob_pos = clf.predict_proba(X)[:, 1]\n        else:  # Otherwise, use decision function and scale\n            prob_pos = clf.decision_function(X)\n            prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n        return prob_pos\n    \n    nr_plots = len(models)\n    if calibrator:\n        nr_plots += 1\n    fig, axes = plt.subplots(1, nr_plots, figsize=(3*nr_plots*fig_scale, 4*nr_plots*fig_scale))\n    for ax, clf in zip(axes[:len(models)], models):\n            clf.fit(Xc_train, yc_train)\n            prob_pos = get_probabilities(clf,Xc_test)           \n            bs = brier_score_loss(yc_test,prob_pos)\n            plot_calibration_curve(yc_test, prob_pos, n_bins=20, ax=ax)\n            ax.set_title(clf.__class__.__name__, fontsize=10*fig_scale)\n            ax.text(0,0.95,\"Brier score: {:.3f}\".format(bs), size=10*fig_scale)\n    if calibrator:\n        calibratee.fit(Xc_train, yc_train)\n        # We're visualizing the trained calibrator, hence let it predict the training data\n        prob_pos = get_probabilities(calibratee, Xc_train) # get uncalibrated predictions\n        y_sort = [x for _,x in sorted(zip(prob_pos,yc_train))] # sort for nicer plots\n        prob_pos.sort()\n        cal_prob = calibrator.fit(prob_pos, y_sort).predict(prob_pos) # fit calibrator\n        axes[-1].scatter(prob_pos,y_sort, s=2)\n        axes[-1].scatter(prob_pos,cal_prob, s=2)\n        axes[-1].plot(prob_pos,cal_prob)\n        axes[-1].set_title(\"Calibrator: {}\".format(calibrator.__class__.__name__), fontsize=10*fig_scale)\n        axes[-1].set_xlabel(\"predicted probability\")\n        axes[-1].set_ylabel(\"outcome\")\n        axes[-1].set(aspect='equal')\n    plt.tight_layout()\n    \nplot_calibration_comparison([LogisticRegression(), SVC()])   \n\n\n\n","type":"content","url":"/notebooks/model-evaluation#model-calibration","position":73},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Brier score","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#brier-score","position":74},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Brier score","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"content":"You may want to select models based on how accurate the class confidences are.\n\nThe Brier score loss: squared loss between predicted probability \\hat{p} and actual outcome y\n\nLower is better\n\\mathcal{L}_{Brier} =  \\frac{1}{n}\\sum_{i=1}^n (\\hat{p}_i - y_i)^2\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nXC_train, XC_test, yC_train, yC_test = train_test_split(cancer.data, cancer.target, test_size=.5, random_state=0)\n\n# LogReg\nlogreg = LogisticRegression().fit(XC_train, yC_train)\nprobs = logreg.predict_proba(XC_test)[:,1]\nprint(\"Logistic Regression Brier score loss: {:.4f}\".format(brier_score_loss(yC_test,probs)))\n\n# SVM: scale decision functions\nsvc = SVC().fit(XC_train, yC_train)\nprob_pos = svc.decision_function(XC_test)\nprob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\nprint(\"SVM Brier score loss: {:.4f}\".format(brier_score_loss(yC_test,prob_pos)))\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#brier-score","position":75},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Model calibration techniques","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"type":"lvl4","url":"/notebooks/model-evaluation#model-calibration-techniques","position":76},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl4":"Model calibration techniques","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"content":"We can post-process trained models to make them more calibrated.\n\nFit a regression model (a calibrator) to map the model’s outcomes f(x) to a calibrated probability in [0,1]\n\nf(x) returns the decision values or probability estimates\n\nf_{calib} is fitted on the training data to map these to the correct outcome\n\nOften an internal cross-validation with few folds is used\n\nMulti-class models require one calibrator per classf_{calib}(f(x))≈p(y)\n\n","type":"content","url":"/notebooks/model-evaluation#model-calibration-techniques","position":77},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl5":"Platt Scaling","lvl4":"Model calibration techniques","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"type":"lvl5","url":"/notebooks/model-evaluation#platt-scaling","position":78},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl5":"Platt Scaling","lvl4":"Model calibration techniques","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"content":"Calibrator is a logistic (sigmoid) function:\n\nLearn the weight w_1 and bias w_0 from data\nf_{platt}=\\frac{1}{1+\\exp(−w_1 f(x)− w_0)}\n\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression\n\n# Wrapped LogisticRegression to get sigmoid predictions\nclass Sigmoid():\n    model = LogisticRegression()\n    \n    def fit(self, X, y):\n        self.model.fit(X.reshape(-1, 1),y)\n        return self\n\n    def predict(self, X):\n        return self.model.predict_proba(X.reshape(-1, 1))[:, 1]\n        \nsvm = SVC()\nsvm_platt = CalibratedClassifierCV(svm, cv=2, method='sigmoid')\nplot_calibration_comparison([svm, svm_platt],Sigmoid(),svm)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#platt-scaling","position":79},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl5":"Isotonic regression","lvl4":"Model calibration techniques","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"type":"lvl5","url":"/notebooks/model-evaluation#isotonic-regression","position":80},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl5":"Isotonic regression","lvl4":"Model calibration techniques","lvl3":"Model calibration","lvl2":"Probabilistic evaluation"},"content":"Maps input x_i to an output \\hat{y}_i so that \\hat{y}_i increases monotonically with x_i and minimizes loss \\sum_i^n (y_i-\\hat{y}_i)\n\nPredictions are made by interpolating the predicted \\hat{y}_i\n\nFit to minimize the loss between the uncalibrated predictions f(x) and the actual labels\n\nCorrects any monotonic distortion, but tends to overfit on small samples\n\nfrom sklearn.isotonic import IsotonicRegression\n\nmodel = SVC()\niso = CalibratedClassifierCV(model, cv=2, method='isotonic')\nplot_calibration_comparison([model, iso],IsotonicRegression(),model)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#isotonic-regression","position":81},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"type":"lvl2","url":"/notebooks/model-evaluation#cost-sensitive-classification-dealing-with-imbalance","position":82},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"content":"In the real world, different kinds of misclassification can have different costs\n\nMisclassifying certain classes can be more costly than others\n\nMisclassifying certain samples can be more costly than others\n\nCost-sensitive resampling: resample (or reweight) the data to represent real-world expectations\n\noversample minority classes (or undersample majority) to ‘correct’ imbalance\n\nincrease weight of misclassified samples (e.g. in boosting)\n\ndecrease weight of misclassified (noisy) samples (e.g. in model compression)\n\n","type":"content","url":"/notebooks/model-evaluation#cost-sensitive-classification-dealing-with-imbalance","position":83},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Class weighting","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"type":"lvl3","url":"/notebooks/model-evaluation#class-weighting","position":84},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Class weighting","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"content":"If some classes are more important than others, we can give them more weight\n\nE.g. for imbalanced data, we can give more weight to minority classes\n\nMost classification models can include it in their loss function and optimize for it\n\nE.g. Logistic regression: add a class weight w_c in the log loss function\n\\mathcal{L_{log}}(\\mathbf{w}) = - \\sum_{c=1}^{C} \\color{red}{w_c} \\sum_{n=1}^{N} p_{n,c} log(q_{n,c})\n\ndef plot_decision_function(classifier, X, y, sample_weight, axis, title):\n    # plot the decision function\n    xx, yy = np.meshgrid(np.linspace(np.min(X[:,0])-1, np.max(X[:,0])+1, 500), np.linspace(np.min(X[:,1])-1, np.max(X[:,1])+1, 500))\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n    axis.scatter(X[:, 0], X[:, 1], c=y, s=100 * sample_weight, alpha=0.9,\n                 cmap=plt.cm.bone, edgecolors='black')\n\n    axis.axis('off')\n    axis.set_title(title)\n    \ndef plot_class_weights():\n    X, y = make_blobs(n_samples=(50, 10), centers=2, cluster_std=[7.0, 2], random_state=4)\n    \n    # fit the models\n    clf_weights = SVC(gamma=0.1, C=0.1, class_weight={1: 10}).fit(X, y)\n    clf_no_weights = SVC(gamma=0.1, C=0.1).fit(X, y)\n\n    fig, axes = plt.subplots(1, 2, figsize=(9*fig_scale, 4*fig_scale))\n    plot_decision_function(clf_no_weights, X, y, 1, axes[0],\n                           \"Constant weights\")\n    plot_decision_function(clf_weights, X, y, 1, axes[1],\n                           \"Modified class weights\")\nplot_class_weights()\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#class-weighting","position":85},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Instance weighting","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"type":"lvl3","url":"/notebooks/model-evaluation#instance-weighting","position":86},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Instance weighting","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"content":"If some training instances are important to get right, we can give them more weight\n\nE.g. when some examples are from groups underrepresented in the data\n\nThese are passed during training (fit), and included in the loss function\n\nE.g. Logistic regression: add a instance weight w_n in the log loss function\n\\mathcal{L_{log}}(\\mathbf{w}) = - \\sum_{c=1}^{C} \\sum_{n=1}^{N} \\color{red}{w_n} p_{n,c} log(q_{n,c})\n\n# Example from https://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html\ndef plot_instance_weights():\n    np.random.seed(0)\n    X = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\n    y = [1] * 10 + [-1] * 10\n    sample_weight_last_ten = abs(np.random.randn(len(X)))\n    sample_weight_constant = np.ones(len(X))\n    # and bigger weights to some outliers\n    sample_weight_last_ten[15:] *= 5\n    sample_weight_last_ten[9] *= 15\n\n    # for reference, first fit without sample weights\n\n    # fit the model\n    clf_weights = SVC(gamma=1)\n    clf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n\n    clf_no_weights = SVC(gamma=1)\n    clf_no_weights.fit(X, y)\n\n    fig, axes = plt.subplots(1, 2, figsize=(9*fig_scale, 4*fig_scale))\n    plot_decision_function(clf_no_weights, X, y, sample_weight_constant, axes[0],\n                           \"Constant weights\")\n    plot_decision_function(clf_weights, X, y, sample_weight_last_ten, axes[1],\n                           \"Modified instance weights\")\nplot_instance_weights()   \n\n\n\n","type":"content","url":"/notebooks/model-evaluation#instance-weighting","position":87},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Cost-sensitive algorithms","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"type":"lvl3","url":"/notebooks/model-evaluation#cost-sensitive-algorithms","position":88},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Cost-sensitive algorithms","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"content":"Cost-sensitive algorithms\n\nIf misclassification cost of some classes is higher, we can give them higher weights\n\nSome support cost matrix C: costs c_{i,j} for every possible type of error\n\nCost-sensitive ensembles: convert cost-insensitive classifiers into cost-sensitive ones\n\nMetaCost: Build a model (ensemble) to learn the class probabilities P(j|x)\n\nRelabel training data to minimize expected cost: \\underset{i}{\\operatorname{argmin}} \\sum_j P_j(x) c_{i,j}\n\nAccuracy may decrease but cost decreases as well.\n\nAdaCost: Boosting with reweighting instances to reduce costs\n\n","type":"content","url":"/notebooks/model-evaluation#cost-sensitive-algorithms","position":89},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Tuning the decision threshold","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"type":"lvl3","url":"/notebooks/model-evaluation#tuning-the-decision-threshold","position":90},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Tuning the decision threshold","lvl2":"Cost-sensitive classification (dealing with imbalance)"},"content":"If every FP or FN has a certain cost, we can compute the total cost for a given model:\n\\text{total cost} = \\text{FPR} * cost_{FP} * ratio_{pos} + (1-\\text{TPR}) *  cost_{FN} * (1-ratio_{pos})\n\nThis yields different isometrics (lines of equal cost) in ROC space\n\nOptimal threshold is the point on the ROC curve where cost is minimal (line search)\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# Cost function\ndef cost(fpr, tpr, cost_FN, cost_FP, ratio_P):\n    return fpr * cost_FP * ratio_P + (1 - tpr) * (1-ratio_P) * cost_FN;\n\n@interact\ndef plot_isometrics(cost_FN=(1,10.0,1.0), cost_FP=(1,10.0,1.0)):\n    from sklearn.metrics import roc_curve\n    fpr, tpr, thresholds = roc_curve(yb_test, svc_roc.decision_function(Xb_test))\n\n    # get minimum\n    ratio_P = len(yb_test[yb_test==1]) / len(yb_test)\n    costs = [cost(fpr[x],tpr[x],cost_FN,cost_FP, ratio_P) for x in range(len(thresholds))]\n    min_cost = np.min(costs)\n    min_thres = np.argmin(costs)\n\n    # plot contours\n    x = np.arange(0.0, 1.1, 0.1)\n    y = np.arange(0.0, 1.1, 0.1)\n    XX, YY = np.meshgrid(x, y)\n    costs = [cost(f, t, cost_FN, cost_FP, ratio_P) for f, t in zip(XX,YY)]\n\n    if interactive:\n        fig, axes = plt.subplots(1, 1, figsize=(9*fig_scale, 3*fig_scale))\n    else:\n        fig, axes = plt.subplots(1, 1, figsize=(6*fig_scale, 1.8*fig_scale))\n    plt.plot(fpr, tpr, label=\"ROC Curve\", lw=2)\n    levels = np.linspace(np.array(costs).min(), np.array(costs).max(), 10)\n    levels = np.sort(np.append(levels, min_cost))\n    CS = plt.contour(XX, YY, costs, levels)\n    plt.clabel(CS, inline=1, fontsize=10)\n\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR (recall)\")\n    # find threshold closest to zero:\n    plt.plot(fpr[min_thres], tpr[min_thres], 'o', markersize=4,\n             label=\"optimal\", fillstyle=\"none\", c='k', mew=2)\n    plt.legend(loc=4, prop={\"size\":10});\n    plt.title(\"Isometrics, cost_FN: {}, cost_FP: {}\".format(cost_FN, cost_FP), fontsize=10*fig_scale)\n    plt.tight_layout()\n    plt.show()\n\n\n\nif not interactive:\n    plot_isometrics(1,1)\n    plot_isometrics(1,9)\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#tuning-the-decision-threshold","position":91},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Regression metrics"},"type":"lvl2","url":"/notebooks/model-evaluation#regression-metrics","position":92},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Regression metrics"},"content":"Most commonly used are\n\nmean squared error: \\frac{\\sum_{i}(y_{pred_i}-y_{actual_i})^2}{n}\n\nroot mean squared error (RMSE) often used as well\n\nmean absolute error: \\frac{\\sum_{i}|y_{pred_i}-y_{actual_i}|}{n}\n\nLess sensitive to outliers and large errors\n\n","type":"content","url":"/notebooks/model-evaluation#regression-metrics","position":93},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"R squared","lvl2":"Regression metrics"},"type":"lvl3","url":"/notebooks/model-evaluation#r-squared","position":94},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"R squared","lvl2":"Regression metrics"},"content":"R^2 = 1 - \\frac{\\color{blue}{\\sum_{i}(y_{pred_i}-y_{actual_i})^2}}{\\color{red}{\\sum_{i}(y_{mean}-y_{actual_i})^2}}\n\nRatio of variation explained by the model / total variation\n\nBetween 0 and 1, but negative if the model is worse than just predicting the mean\n\nEasier to interpret (higher is better).\n\n","type":"content","url":"/notebooks/model-evaluation#r-squared","position":95},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Visualizing regression errors","lvl2":"Regression metrics"},"type":"lvl3","url":"/notebooks/model-evaluation#visualizing-regression-errors","position":96},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Visualizing regression errors","lvl2":"Regression metrics"},"content":"Prediction plot (left): predicted vs actual target values\n\nResidual plot (right): residuals vs actual target values\n\nOver- and underpredictions can be given different costs\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nboston = fetch_openml(name=\"boston\", as_frame=True)\n\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target)\nridge_pipe = make_pipeline(StandardScaler(),Ridge())\n\npred = ridge_pipe.fit(X_train, y_train).predict(X_test)\n\nplt.subplot(1, 2, 1)\nplt.gca().set_aspect(\"equal\")\nplt.plot([10, 50], [10, 50], '--', c='k')\nplt.plot(y_test, pred, 'o', alpha=.5, markersize=7*fig_scale)\nplt.ylabel(\"predicted\", fontsize=10*fig_scale)\nplt.xlabel(\"true\", fontsize=10*fig_scale);\n\nplt.subplot(1, 2, 2)\nplt.gca().set_aspect(\"equal\")\nplt.plot([10, 50], [0,0], '--', c='k')\nplt.plot(y_test, y_test - pred, 'o', alpha=.5, markersize=7*fig_scale)\nplt.xlabel(\"true\", fontsize=10*fig_scale)\nplt.ylabel(\"true - predicted\", fontsize=10*fig_scale)\nplt.tight_layout();\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#visualizing-regression-errors","position":97},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Bias-Variance decomposition"},"type":"lvl2","url":"/notebooks/model-evaluation#bias-variance-decomposition","position":98},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Bias-Variance decomposition"},"content":"Evaluate the same algorithm multiple times on different random samples of the data\n\nTwo types of errors can be observed:\n\nBias error: systematic error, independent of the training sample\n\nThese points are predicted (equally) wrong every time\n\nVariance error: error due to variability of the model w.r.t. the training sample\n\nThese points are sometimes predicted accurately, sometimes inaccurately\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#bias-variance-decomposition","position":99},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Computing bias and variance error","lvl2":"Bias-Variance decomposition"},"type":"lvl3","url":"/notebooks/model-evaluation#computing-bias-and-variance-error","position":100},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Computing bias and variance error","lvl2":"Bias-Variance decomposition"},"content":"Take 100 or more bootstraps (or shuffle-splits)\n\nRegression: for each data point x:\n\nbias(x)^2 = (x_{true} - mean(x_{predicted}))^2\n\nvariance(x) = var(x_{predicted})\n\nClassification: for each data point x:\n\nbias(x) = misclassification ratio\n\nvariance(x) = (1 - (P(class_1)^2 + P(class_2)^2))/2\n\nP(class_i) is ratio of class i predictions\n\nTotal bias: \\sum_{x} bias(x)^2 * w_x\nw_x: the percentage of times x occurs in the test sets\n\nTotal variance: \\sum_{x} variance(x) * w_x\n\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, GridSearchCV\n\n# Bias-Variance Computation \ndef compute_bias_variance(clf, X, y):\n    # Bootstraps\n    n_repeat = 40 # 40 is on the low side to get a good estimate. 100 is better.\n    shuffle_split = ShuffleSplit(test_size=0.33, n_splits=n_repeat, random_state=0)\n\n    # Store sample predictions\n    y_all_pred = [[] for _ in range(len(y))]\n\n    # Train classifier on each bootstrap and score predictions\n    for i, (train_index, test_index) in enumerate(shuffle_split.split(X)):\n        # Train and predict\n        clf.fit(X[train_index], y[train_index])\n        y_pred = clf.predict(X[test_index])\n\n        # Store predictions\n        for j,index in enumerate(test_index):\n            y_all_pred[index].append(y_pred[j])\n\n    # Compute bias, variance, error\n    bias_sq = sum([ (1 - x.count(y[i])/len(x))**2 * len(x)/n_repeat \n                for i,x in enumerate(y_all_pred)])\n    var = sum([((1 - ((x.count(0)/len(x))**2 + (x.count(1)/len(x))**2))/2) * len(x)/n_repeat\n               for i,x in enumerate(y_all_pred)])\n    error = sum([ (1 - x.count(y[i])/len(x)) * len(x)/n_repeat \n            for i,x in enumerate(y_all_pred)])\n\n    return np.sqrt(bias_sq), var, error\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#computing-bias-and-variance-error","position":101},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Bias and variance, underfitting and overfitting","lvl2":"Bias-Variance decomposition"},"type":"lvl3","url":"/notebooks/model-evaluation#bias-and-variance-underfitting-and-overfitting","position":102},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Bias and variance, underfitting and overfitting","lvl2":"Bias-Variance decomposition"},"content":"High variance means that you are likely overfitting\n\nUse more regularization or use a simpler model\n\nHigh bias means that you are likely underfitting\n\nDo less regularization or use a more flexible/complex model\n\nEnsembling techniques (see later) reduce bias or variance directly\n\nBagging (e.g. RandomForests) reduces variance, Boosting reduces bias\n\n","type":"content","url":"/notebooks/model-evaluation#bias-and-variance-underfitting-and-overfitting","position":103},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Understanding under- and overfitting","lvl2":"Bias-Variance decomposition"},"type":"lvl3","url":"/notebooks/model-evaluation#understanding-under-and-overfitting","position":104},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Understanding under- and overfitting","lvl2":"Bias-Variance decomposition"},"content":"Regularization reduces variance error (increases stability of predictions)\n\nBut too much increases bias error (inability to learn ‘harder’ points)\n\nHigh regularization (left side): Underfitting, high bias error, low variance error\n\nHigh training error and high test error\n\nLow regularization (right side): Overfitting, low bias error, high variance error\n\nLow training error and higher test error\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\ncancer = load_breast_cancer()\n\ndef plot_bias_variance(clf, param, X, y, ax):\n    bias_scores = []\n    var_scores = []\n    err_scores = []\n    name, vals = next(iter(param.items()))\n\n    for i in vals:\n        b,v,e = compute_bias_variance(clf.set_params(**{name:i}),X,y)\n        bias_scores.append(b)\n        var_scores.append(v)\n        err_scores.append(e)\n\n    ax.set_title(clf.__class__.__name__)\n    ax.plot(vals, var_scores,label =\"variance\", lw=2 )\n    ax.plot(vals, bias_scores,label =\"bias\", lw=2 )\n    ax.set_xscale('log')\n    ax.set_xlabel(name)\n    ax.legend(loc=\"best\")\n    \ndef plot_train_test(clf, param, X, y, ax):\n    gs = GridSearchCV(clf, param, cv=5, return_train_score=True).fit(X,y)\n    name, vals = next(iter(param.items()))\n\n    ax.set_title(clf.__class__.__name__)\n    ax.plot(vals, (1-gs.cv_results_['mean_train_score']),label =\"train error\", lw=2 )\n    ax.plot(vals, (1-gs.cv_results_['mean_test_score']),label =\"test error\", lw=2 )\n    ax.set_xscale('log')\n    ax.set_xlabel(name)\n    ax.legend(loc=\"best\")\n    \nfig, axes = plt.subplots(2, 2, figsize=(6*fig_scale, 4*fig_scale))\nX, y = cancer.data, cancer.target\nsvm = SVC(gamma=2e-4, random_state=0)\nparam = {'C': [1e-4, 1e-2, 1, 1e2, 1e4]}\n\nplot_bias_variance(svm, param, X, y, axes[0,1])\nplot_train_test(svm, param, X, y, axes[1,1]) \n\nlr = LogisticRegression(random_state=0)\nparam = {'C': [1e-2, 1, 92, 1e4, 1e6]}\nplot_bias_variance(lr, param, X, y, axes[0,0])\nplot_train_test(lr, param, X, y, axes[1,0])\nplt.tight_layout()\n\n\n\nSummary Flowchart (by Andrew Ng)\n\n","type":"content","url":"/notebooks/model-evaluation#understanding-under-and-overfitting","position":105},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Hyperparameter tuning"},"type":"lvl2","url":"/notebooks/model-evaluation#hyperparameter-tuning","position":106},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Hyperparameter tuning"},"content":"There exists a huge range of techniques to tune hyperparameters. The simplest:\n\nGrid search: Choose a range of values for every hyperparameter, try every combination\n\nDoesn’t scale to many hyperparameters (combinatorial explosion)\n\nRandom search: Choose random values for all hyperparameters, iterate n times\n\nBetter, especially when some hyperparameters are less important\n\nMany more advanced techniques exist, see lecture on Automated Machine Learning\n\nFirst, split the data in training and test sets (outer split)\n\nSplit up the training data again (inner cross-validation)\n\nGenerate hyperparameter configurations (e.g. random/grid search)\n\nEvaluate all configurations on all inner splits, select the best one (on average)\n\nRetrain best configurations on full training set, evaluate on held-out test data\n\nif interactive:\n    plt.rcParams['figure.dpi'] = 75\nmglearn.plots.plot_grid_search_overview()\nplt.rcParams.update(print_config)\n\n\n\nfrom scipy.stats.distributions import expon\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#hyperparameter-tuning","position":107},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Nested cross-validation","lvl2":"Hyperparameter tuning"},"type":"lvl3","url":"/notebooks/model-evaluation#nested-cross-validation","position":108},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl3":"Nested cross-validation","lvl2":"Hyperparameter tuning"},"content":"Simplest approach: single outer split and single inner split (shown below)\n\nRisk of over-tuning hyperparameters on specific train-test split\n\nOnly recommended for very large datasets\n\nNested cross-validation:\n\nOuter loop: split full dataset in k_1 training and test splits\n\nInner loop: split training data into k_2 train and validation sets\n\nThis yields k_1 scores for k_1 possibly different hyperparameter settings\n\nAverage score is the expected performance of the tuned model\n\nTo use the model in practice, retune on the entire datasethps = {'C': expon(scale=100), 'gamma': expon(scale=.1)}\nscores = cross_val_score(RandomizedSearchCV(SVC(), hps, cv=3), X, y, cv=5)\n\nmglearn.plots.plot_threefold_split()\n\n\n\n","type":"content","url":"/notebooks/model-evaluation#nested-cross-validation","position":109},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/model-evaluation#summary","position":110},{"hierarchy":{"lvl1":"Lecture 3. Model Evaluation","lvl2":"Summary"},"content":"Split the data into training and test sets according to the application\n\nHoldout only for large datasets, cross-validation for smaller ones\n\nFor classification, always use stratification\n\nGrouped or ordered data requires special splitting\n\nChoose a metric that fits your application\n\nE.g. precision to avoid false positives, recall to avoid false negatives\n\nCalibrate the decision threshold to fit your application\n\nROC curves or Precision-Recall curves can help to find a good tradeoff\n\nIf possible, include the actual or relative costs of misclassifications\n\nClass weighting, instance weighting, ROC isometrics can help\n\nBe careful with imbalanced or unrepresentative datasets\n\nWhen using the predicted probabilities in applications, calibrate the models\n\nAlways tune the most important hyperparameters\n\nManual tuning: Use insight and train-test scores for guidance\n\nHyperparameter optimization: be careful not to over-tune","type":"content","url":"/notebooks/model-evaluation#summary","position":111},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning"},"type":"lvl1","url":"/notebooks/ensemble-learning","position":0},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning"},"content":"Crowd intelligence\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots\nif interactive:\n    fig_scale = 0.9\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.3\n    plt.rcParams.update(print_config)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning","position":1},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Ensemble learning"},"type":"lvl2","url":"/notebooks/ensemble-learning#ensemble-learning","position":2},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Ensemble learning"},"content":"If different models make different mistakes, can we simply average the predictions?\n\nVoting Classifier: gives every model a vote on the class label\n\nHard vote: majority class wins (class order breaks ties)\n\nSoft vote: sum class probabilities p_{m,c} over M models: \\underset{c}{\\operatorname{argmax}} \\sum_{m=1}^{M} w_c p_{m,c}\n\nClasses can get different weights w_c (default: w_c=1)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# Toy data\nX, y = make_moons(noise=.2, random_state=18) # carefully picked random state for illustration\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n\n# Plot grid\nx_lin = np.linspace(X_train[:, 0].min() - .5, X_train[:, 0].max() + .5, 100)\ny_lin = np.linspace(X_train[:, 1].min() - .5, X_train[:, 1].max() + .5, 100)\nx_grid, y_grid = np.meshgrid(x_lin, y_lin)\nX_grid = np.c_[x_grid.ravel(), y_grid.ravel()]\nmodels = [LogisticRegression(C=100),\n          DecisionTreeClassifier(max_depth=3, random_state=0),\n          KNeighborsClassifier(n_neighbors=1),\n          KNeighborsClassifier(n_neighbors=30)]\n\n@interact\ndef combine_voters(model1=models, model2=models):\n    # Voting Classifier and components\n    voting = VotingClassifier([('model1', model1),('model2', model2)],voting='soft')\n    voting.fit(X_train, y_train)\n\n    # transform produces individual probabilities\n    y_probs =  voting.transform(X_grid)\n\n    fig, axes = plt.subplots(1, 3, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(11*fig_scale, 3*fig_scale))\n    scores = [voting.estimators_[0].score(X_test, y_test),\n             voting.estimators_[1].score(X_test, y_test),\n             voting.score(X_test, y_test)]\n    titles = [model1.__class__.__name__, model2.__class__.__name__, 'VotingClassifier']\n    for prob, score, title, ax in zip([y_probs[:, 1], y_probs[:, 3], y_probs[:, 1::2].sum(axis=1)], scores, titles, axes.ravel()):\n        ax.contourf(x_grid, y_grid, prob.reshape(x_grid.shape), alpha=.4, cmap='bwr')\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', s=7*fig_scale)\n        ax.set_title(title + f\" \\n acc={score:.2f}\", pad=0, fontsize=9)\n    plt.show()\n\n\n\nif not interactive:\n    combine_voters(models[0],models[1])\n\n\n\nWhy does this work?\n\nDifferent models may be good at different ‘parts’ of data (even if they underfit)\n\nIndividual mistakes can be ‘averaged out’ (especially if models overfit)\n\nWhich models should be combined?\n\nBias-variance analysis teaches us that we have two options:\n\nIf model underfits (high bias, low variance): combine with other low-variance models\n\nNeed to be different: ‘experts’ on different parts of the data\n\nBias reduction. Can be done with Boosting\n\nIf model overfits (low bias, high variance): combine with other low-bias models\n\nNeed to be different: individual mistakes must be different\n\nVariance reduction. Can be done with Bagging\n\nModels must be uncorrelated but good enough (otherwise the ensemble is worse)\n\nWe can also learn how to combine the predictions of different models: Stacking\n\n","type":"content","url":"/notebooks/ensemble-learning#ensemble-learning","position":3},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Decision trees (recap)"},"type":"lvl2","url":"/notebooks/ensemble-learning#decision-trees-recap","position":4},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Decision trees (recap)"},"content":"Representation: Tree that splits data points into leaves based on tests\n\nEvaluation (loss): Heuristic for purity of leaves (Gini index, entropy,...)\n\nOptimization: Recursive, heuristic greedy search (Hunt’s algorithm)\n\nConsider all splits (thresholds) between adjacent data points, for every feature\n\nChoose the one that yields the purest leafs, repeat\n\nimport graphviz\n\n@interact\ndef plot_depth(depth=(1,5,1)):\n    X, y = make_moons(noise=.2, random_state=18) # carefully picked random state for illustration\n    fig, ax = plt.subplots(1, 2, figsize=(12*fig_scale, 4*fig_scale),\n                           subplot_kw={'xticks': (), 'yticks': ()})\n\n    tree = mglearn.plots.plot_tree(X, y, max_depth=depth)\n    ax[0].imshow(mglearn.plots.tree_image(tree))\n    ax[0].set_axis_off()\n    plt.show()\n\n\n\nif not interactive:\n    plot_depth(depth=3)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#decision-trees-recap","position":5},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Evaluation (loss function for classification)","lvl2":"Decision trees (recap)"},"type":"lvl3","url":"/notebooks/ensemble-learning#evaluation-loss-function-for-classification","position":6},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Evaluation (loss function for classification)","lvl2":"Decision trees (recap)"},"content":"Every leaf predicts a class probability \\hat{p}_c = the relative frequency of class c\n\nLeaf impurity measures (splitting criteria) for L leafs, leaf l has data X_l:\n\nGini-Index: Gini(X_{l}) = \\sum_{c\\neq c'} \\hat{p}_c \\hat{p}_{c'}\n\nEntropy (more expensive): E(X_{l}) = -\\sum_{c\\neq c'} \\hat{p}_c \\log_{2}\\hat{p}_c\n\nBest split maximizes information gain (idem for Gini index) Gain(X,X_i) = E(X) - \\sum_{l=1}^L \\frac{|X_{i=l}|}{|X_{i}|} E(X_{i=l})\n\ndef gini(p):\n   return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))\n\ndef entropy(p):\n   return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n\ndef classification_error(p):\n   return 1 - np.max([p, 1 - p])\n\nx = np.arange(0.0, 1.0, 0.01)\nent = [entropy(p) if p != 0 else None for p in x]\nscaled_ent = [e*0.5 if e else None for e in ent]\nc_err = [classification_error(i) for i in x]\n\nfig = plt.figure(figsize=(5*fig_scale, 2.5*fig_scale))\nax = plt.subplot(111)\n\nfor j, lab, ls, c, in zip(\n      [ent, scaled_ent, gini(x), c_err],\n      ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'],\n      ['-', '-', '--', '-.'],\n      ['lightgray', 'red', 'green', 'blue']):\n   line = ax.plot(x, j, label=lab, linestyle=ls, lw=2*fig_scale, color=c)\n\nax.legend(loc='upper left', ncol=1, fancybox=True, shadow=False)\nax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\nax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.ylim([0, 1.1])\nplt.xlabel('p(j=1)',labelpad=0)\nplt.ylabel('Impurity Index')\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#evaluation-loss-function-for-classification","position":7},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Regression trees","lvl2":"Decision trees (recap)"},"type":"lvl3","url":"/notebooks/ensemble-learning#regression-trees","position":8},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Regression trees","lvl2":"Decision trees (recap)"},"content":"Every leaf predicts the mean target value \\mu of all points in that leaf\n\nChoose the split that minimizes squared error of the leaves: \\sum_{x_{i} \\in L} (y_i - \\mu)^2\n\nYields non-smooth step-wise predictions, cannot extrapolate\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef plot_decision_tree_regression(regr_1, regr_2):\n    # Create a random dataset\n    rng = np.random.RandomState(5)\n    X = np.sort(7 * rng.rand(80, 1), axis=0)\n    y = np.sin(X).ravel()\n    y[::5] += 3 * (0.5 - rng.rand(16))\n    split = 65\n\n    # Fit regression model of first 60 points\n    regr_1.fit(X[:split], y[:split])\n    regr_2.fit(X[:split], y[:split])\n\n    # Predict\n    X_test = np.arange(0.0, 7.0, 0.01)[:, np.newaxis]\n    y_1 = regr_1.predict(X_test)\n    y_2 = regr_2.predict(X_test)\n\n    # Plot the results\n    plt.figure(figsize=(8*fig_scale,5*fig_scale))\n    plt.scatter(X[:split], y[:split], c=\"darkorange\", label=\"training data\")\n    plt.scatter(X[split:], y[split:], c=\"blue\", label=\"test data\")\n    plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2*fig_scale)\n    plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2*fig_scale)\n    plt.xlabel(\"data\", fontsize=9)\n    plt.ylabel(\"target\", fontsize=9)\n    plt.title(\"Decision Tree Regression\", fontsize=9)\n    plt.legend()\n    plt.show()\n\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\n\nplot_decision_tree_regression(regr_1,regr_2)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#regression-trees","position":9},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Impurity/Entropy-based feature importance","lvl2":"Decision trees (recap)"},"type":"lvl3","url":"/notebooks/ensemble-learning#impurity-entropy-based-feature-importance","position":10},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Impurity/Entropy-based feature importance","lvl2":"Decision trees (recap)"},"content":"We can measure the importance of features (to the model) based on\n\nWhich features we split on\n\nHow high up in the tree we split on them (first splits ar emore important)\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nXc_train, Xc_test, yc_train, yc_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)\ntree = DecisionTreeClassifier(random_state=0).fit(Xc_train, yc_train)\n\ndef plot_feature_importances_cancer(model):\n    n_features = cancer.data.shape[1]\n    plt.figure(figsize=(7*fig_scale,5.4*fig_scale))\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), cancer.feature_names, fontsize=7)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)\nplot_feature_importances_cancer(tree)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#impurity-entropy-based-feature-importance","position":11},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Under- and overfitting","lvl2":"Decision trees (recap)"},"type":"lvl3","url":"/notebooks/ensemble-learning#under-and-overfitting","position":12},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Under- and overfitting","lvl2":"Decision trees (recap)"},"content":"We can easily control the (maximum) depth of the trees as a hyperparameter\n\nBias-variance analysis:\n\nShallow trees have high bias but very low variance (underfitting)\n\nDeep trees have high variance but low bias (overfitting)\n\nBecause we can easily control their complexity, they are ideal for ensembling\n\nDeep trees: keep low bias, reduce variance with Bagging\n\nShallow trees: keep low variance, reduce bias with Boosting\n\nfrom sklearn.model_selection import ShuffleSplit, train_test_split\n\n# Bias-Variance Computation \ndef compute_bias_variance(clf, X, y):\n    # Bootstraps\n    n_repeat = 40 # 40 is on the low side to get a good estimate. 100 is better.\n    shuffle_split = ShuffleSplit(test_size=0.33, n_splits=n_repeat, random_state=0)\n\n    # Store sample predictions\n    y_all_pred = [[] for _ in range(len(y))]\n\n    # Train classifier on each bootstrap and score predictions\n    for i, (train_index, test_index) in enumerate(shuffle_split.split(X)):\n        # Train and predict\n        clf.fit(X[train_index], y[train_index])\n        y_pred = clf.predict(X[test_index])\n\n        # Store predictions\n        for j,index in enumerate(test_index):\n            y_all_pred[index].append(y_pred[j])\n\n    # Compute bias, variance, error\n    bias_sq = sum([ (1 - x.count(y[i])/len(x))**2 * len(x)/n_repeat \n                for i,x in enumerate(y_all_pred)])\n    var = sum([((1 - ((x.count(0)/len(x))**2 + (x.count(1)/len(x))**2))/2) * len(x)/n_repeat\n               for i,x in enumerate(y_all_pred)])\n    error = sum([ (1 - x.count(y[i])/len(x)) * len(x)/n_repeat \n            for i,x in enumerate(y_all_pred)])\n\n    return np.sqrt(bias_sq), var, error\n\ndef plot_bias_variance(clf, X, y):\n    bias_scores = []\n    var_scores = []\n    err_scores = []\n    max_depth= range(2,11)\n\n    for i in max_depth:\n        b,v,e = compute_bias_variance(clf.set_params(random_state=0,max_depth=i),X,y)\n        bias_scores.append(b)\n        var_scores.append(v)\n        err_scores.append(e)\n\n    plt.figure(figsize=(8*fig_scale,3*fig_scale))\n    plt.suptitle(clf.__class__.__name__, fontsize=9)\n    plt.plot(max_depth, var_scores,label =\"variance\", lw=2*fig_scale )\n    plt.plot(max_depth, np.square(bias_scores),label =\"bias^2\", lw=2*fig_scale )\n    plt.plot(max_depth, err_scores,label =\"error\", lw=2*fig_scale)\n    plt.xlabel(\"max_depth\", fontsize=7)\n    plt.legend(loc=\"best\", fontsize=7)\n    plt.show()\n\ndt = DecisionTreeClassifier()\nplot_bias_variance(dt, cancer.data, cancer.target)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#under-and-overfitting","position":13},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl2","url":"/notebooks/ensemble-learning#bagging-bootstrap-aggregating","position":14},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"Obtain different models by training the same model on different training samples\n\nReduce overfitting by averaging out individual predictions (variance reduction)\n\nIn practice: take I bootstrap samples of your data, train a model on each bootstrap\n\nHigher I: more models, more smoothing (but slower training and prediction)\n\nBase models should be unstable: different training samples yield different models\n\nE.g. very deep decision trees, or even randomized decision trees\n\nDeep Neural Networks can also benefit from bagging (deep ensembles)\n\nPrediction by averaging predictions of base models\n\nSoft voting for classification (possibly weighted)\n\nMean value for regression\n\nCan produce uncertainty estimates as well\n\nBy combining class probabilities of individual models (or variances for regression)\n\n","type":"content","url":"/notebooks/ensemble-learning#bagging-bootstrap-aggregating","position":15},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Random Forests","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl3","url":"/notebooks/ensemble-learning#random-forests","position":16},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Random Forests","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"Uses randomized trees to make models even less correlated (more unstable)\n\nAt every split, only consider max_features features, randomly selected\n\nExtremely randomized trees: considers 1 random threshold for random set of features (faster)\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nmodels=[RandomForestClassifier(n_estimators=5, random_state=7, n_jobs=-1),ExtraTreesClassifier(n_estimators=5, random_state=2, n_jobs=-1)]\n\n@interact\ndef run_forest_run(model=models):\n    forest = model.fit(X_train, y_train) \n    fig, axes = plt.subplots(2, 3, figsize=(12*fig_scale, 6*fig_scale))\n    for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n        ax.set_title(\"Tree {}\".format(i), pad=0, fontsize=9)\n        mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n\n    mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n                                    alpha=.4)\n    axes[-1, -1].set_title(model.__class__.__name__, pad=0, fontsize=9)\n    axes[-1, -1].set_xticks(())\n    axes[-1, -1].set_yticks(())\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, s=10*fig_scale);\n    plt.show()\n\n\n\nif not interactive:\n    run_forest_run(model=models[0])\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#random-forests","position":17},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Effect on bias and variance","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl3","url":"/notebooks/ensemble-learning#effect-on-bias-and-variance","position":18},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Effect on bias and variance","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"Increasing the number of models (trees) decreases variance (less overfitting)\n\nBias is mostly unaffected, but will increase if the forest becomes too large (oversmoothing)\n\nfrom sklearn.model_selection import ShuffleSplit, train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\ncancer = load_breast_cancer()\n\n# Faster version of plot_bias_variance that uses warm-starting\ndef plot_bias_variance_rf(model, X, y, warm_start=False):\n    bias_scores = []\n    var_scores = []\n    err_scores = []\n\n    # Bootstraps\n    n_repeat = 40 # 40 is on the low side to get a good estimate. 100 is better.\n    shuffle_split = ShuffleSplit(test_size=0.33, n_splits=n_repeat, random_state=0)\n    \n    # Ensemble sizes\n    n_estimators = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n\n    # Store sample predictions. One per n_estimators\n    # n_estimators : [predictions]\n    predictions = {}\n    for nr_trees in n_estimators:\n        predictions[nr_trees] = [[] for _ in range(len(y))]\n\n    # Train classifier on each bootstrap and score predictions\n    for i, (train_index, test_index) in enumerate(shuffle_split.split(X)):\n                \n        # Initialize \n        clf = model(random_state=0)\n        if model.__class__.__name__ == 'RandomForestClassifier':\n            clf.n_jobs = -1\n        if model.__class__.__name__ != 'AdaBoostClassifier':\n            clf.warm_start = warm_start\n        \n        prev_n_estimators = 0\n    \n        # Train incrementally        \n        for nr_trees in n_estimators:\n            if model.__class__.__name__ == 'HistGradientBoostingClassifier':\n                clf.max_iter = nr_trees\n            else:\n                clf.n_estimators = nr_trees\n            \n            # Fit and predict\n            clf.fit(X[train_index], y[train_index])\n            y_pred = clf.predict(X[test_index])\n            for j,index in enumerate(test_index):\n                predictions[nr_trees][index].append(y_pred[j])\n    \n    for nr_trees in n_estimators:\n        # Compute bias, variance, error\n        bias_sq = sum([ (1 - x.count(y[i])/len(x))**2 * len(x)/n_repeat \n                       for i,x in enumerate(predictions[nr_trees])])\n        var = sum([((1 - ((x.count(0)/len(x))**2 + (x.count(1)/len(x))**2))/2) * len(x)/n_repeat \n                   for i,x in enumerate(predictions[nr_trees])])\n        error = sum([ (1 - x.count(y[i])/len(x)) * len(x)/n_repeat\n                     for i,x in enumerate(predictions[nr_trees])])\n\n        bias_scores.append(bias_sq)\n        var_scores.append(var)\n        err_scores.append(error)\n\n    plt.figure(figsize=(8*fig_scale,3*fig_scale))\n    plt.suptitle(clf.__class__.__name__, fontsize=9)\n    plt.plot(n_estimators, var_scores,label = \"variance\", lw=2*fig_scale )\n    plt.plot(n_estimators, bias_scores,label = \"bias^2\", lw=2*fig_scale )\n    plt.plot(n_estimators, err_scores,label = \"error\", lw=2*fig_scale  )\n    plt.xscale('log',base=2)\n    plt.xlabel(\"n_estimators\", fontsize=9)\n    plt.legend(loc=\"best\", fontsize=7)\n    plt.show()\n\n\n\nplot_bias_variance_rf(RandomForestClassifier, cancer.data, cancer.target, warm_start=True)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#effect-on-bias-and-variance","position":19},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"In practice","lvl3":"Effect on bias and variance","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl4","url":"/notebooks/ensemble-learning#in-practice","position":20},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"In practice","lvl3":"Effect on bias and variance","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"Different implementations can be used. E.g. in scikit-learn:\n\nBaggingClassifier: Choose your own base model and sampling procedure\n\nRandomForestClassifier: Default implementation, many options\n\nExtraTreesClassifier: Uses extremely randomized trees\n\nMost important parameters:\n\nn_estimators (>100, higher is better, but diminishing returns)\n\nWill start to underfit (bias error component increases slightly)\n\nmax_features\n\nDefaults: sqrt(p) for classification, log2(p) for regression\n\nSet smaller to reduce space/time requirements\n\nparameters of trees, e.g. max_depth, min_samples_split,...\n\nPrepruning useful to reduce model size, but don’t overdo it\n\nEasy to parallelize (set n_jobs to -1)\n\nFix random_state (bootstrap samples) for reproducibility\n\n","type":"content","url":"/notebooks/ensemble-learning#in-practice","position":21},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Out-of-bag error","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl3","url":"/notebooks/ensemble-learning#out-of-bag-error","position":22},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Out-of-bag error","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"RandomForests don’t need cross-validation: you can use the out-of-bag (OOB) error\n\nFor each tree grown, about 33% of samples are out-of-bag (OOB)\n\nRemember which are OOB samples for every model, do voting over these\n\nOOB error estimates are great to speed up model selection\n\nAs good as CV estimates, althought slightly pessimistic\n\nIn scikit-learn: oob_error = 1 - clf.oob_score_\n\nfrom collections import OrderedDict\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nRANDOM_STATE = 123\n\n# Generate a binary classification dataset.\nX, y = make_classification(n_samples=500, n_features=25,\n                           n_clusters_per_class=1, n_informative=15,\n                           random_state=RANDOM_STATE)\n\n# NOTE: Setting the `warm_start` construction parameter to `True` disables\n# support for parallelized ensembles but is necessary for tracking the OOB\n# error trajectory during training.\nensemble_clfs = [\n    (\"RandomForestClassifier, max_features='sqrt'\",\n        RandomForestClassifier(warm_start=True, oob_score=True,\n                               max_features=\"sqrt\", n_jobs=-1,\n                               random_state=RANDOM_STATE)),\n    (\"RandomForestClassifier, max_features='log2'\",\n        RandomForestClassifier(warm_start=True, max_features='log2',\n                               oob_score=True, n_jobs=-1,\n                               random_state=RANDOM_STATE)),\n    (\"RandomForestClassifier, max_features=None\",\n        RandomForestClassifier(warm_start=True, max_features=None,\n                               oob_score=True, n_jobs=-1,\n                               random_state=RANDOM_STATE))\n]\n\n# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n# Range of `n_estimators` values to explore.\nmin_estimators = 15\nmax_estimators = 175\n\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1):\n        clf.set_params(n_estimators=i)\n        clf.fit(X, y)\n\n        # Record the OOB error for each `n_estimators=i` setting.\n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\nplt.figure(figsize=(8*fig_scale,3*fig_scale))\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label, lw=2*fig_scale)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel(\"n_estimators\", fontsize=7)\nplt.ylabel(\"OOB error rate\", fontsize=7)\nplt.legend(loc=\"upper right\", fontsize=7)\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#out-of-bag-error","position":23},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Feature importance","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl3","url":"/notebooks/ensemble-learning#feature-importance","position":24},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Feature importance","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"RandomForests provide more reliable feature importances, based on many alternative hypotheses (trees)\n\nforest = RandomForestClassifier(random_state=0, n_estimators=512, n_jobs=-1)\nforest.fit(Xc_train, yc_train)\nplot_feature_importances_cancer(forest)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#feature-importance","position":25},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Other tips","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl3","url":"/notebooks/ensemble-learning#other-tips","position":26},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Other tips","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"Model calibration\n\nRandomForests are poorly calibrated.\n\nCalibrate afterwards (e.g. isotonic regression) if you aim to use probabilities\n\nWarm starting\n\nGiven an ensemble trained for I iterations, you can simply add more models later\n\nYou warm start from the existing model instead of re-starting from scratch\n\nCan be useful to train models on new, closely related data\n\nNot ideal if the data batches change over time (concept drift)\n\nBoosting is more robust against this (see later)\n\n","type":"content","url":"/notebooks/ensemble-learning#other-tips","position":27},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Strength and weaknesses","lvl2":"Bagging (Bootstrap Aggregating)"},"type":"lvl3","url":"/notebooks/ensemble-learning#strength-and-weaknesses","position":28},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Strength and weaknesses","lvl2":"Bagging (Bootstrap Aggregating)"},"content":"RandomForest are among most widely used algorithms:\n\nDon’t require a lot of tuning\n\nTypically very accurate\n\nHandles heterogeneous features well (trees)\n\nImplictly selects most relevant features\n\nDownsides:\n\nless interpretable, slower to train (but parallellizable)\n\ndon’t work well on high dimensional sparse data (e.g. text)\n\n","type":"content","url":"/notebooks/ensemble-learning#strength-and-weaknesses","position":29},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Adaptive Boosting (AdaBoost)"},"type":"lvl2","url":"/notebooks/ensemble-learning#adaptive-boosting-adaboost","position":30},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Adaptive Boosting (AdaBoost)"},"content":"Obtain different models by reweighting the training data every iteration\n\nReduce underfitting by focusing on the ‘hard’ training examples\n\nIncrease weights of instances misclassified by the ensemble, and vice versa\n\nBase models should be simple so that different instance weights lead to different models\n\nUnderfitting models: decision stumps (or very shallow trees)\n\nEach is an ‘expert’ on some parts of the data\n\nAdditive model: Predictions at iteration I are sum of base model predictions\n\nIn Adaboost, also the models each get a unique weight w_i\nf_I(\\mathbf{x}) = \\sum_{i=1}^I w_i g_i(\\mathbf{x})\n\nAdaboost minimizes exponential loss. For instance-weighted error \\varepsilon:\n\\mathcal{L}_{Exp} = \\sum_{n=1}^N e^{\\varepsilon(f_I(\\mathbf{x}))}\n\nBy deriving \\frac{\\partial \\mathcal{L}}{\\partial w_i} you can find that optimal w_{i} =  \\frac{1}{2}\\log(\\frac{1-\\varepsilon}{\\varepsilon})\n\n","type":"content","url":"/notebooks/ensemble-learning#adaptive-boosting-adaboost","position":31},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"AdaBoost algorithm","lvl2":"Adaptive Boosting (AdaBoost)"},"type":"lvl3","url":"/notebooks/ensemble-learning#adaboost-algorithm","position":32},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"AdaBoost algorithm","lvl2":"Adaptive Boosting (AdaBoost)"},"content":"Initialize sample weights: s_{n,0} = \\frac{1}{N}\n\nBuild a model (e.g. decision stumps) using these sample weights\n\nGive the model a weight w_i related to its weighted error rate \\varepsilon\nw_{i} =  \\lambda\\log(\\frac{1-\\varepsilon}{\\varepsilon})\n\nGood trees get more weight than bad trees\n\nLogit function maps error \\varepsilon from [0,1] to weight in [-Inf,Inf] (use small minimum error)\n\nLearning rate \\lambda (shrinkage) decreases impact of individual classifiers\n\nSmall updates are often better but requires more iterations\n\nUpdate the sample weights\n\nIncrease weight of incorrectly predicted samples:\ns_{n,i+1} = s_{n,i}e^{w_i}\n\nDecrease weight of correctly predicted samples:\ns_{n,i+1} = s_{n,i}e^{-w_i}\n\nNormalize weights to add up to 1\n\nRepeat for I iterations\n\n","type":"content","url":"/notebooks/ensemble-learning#adaboost-algorithm","position":33},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"AdaBoost variants","lvl2":"Adaptive Boosting (AdaBoost)"},"type":"lvl3","url":"/notebooks/ensemble-learning#adaboost-variants","position":34},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"AdaBoost variants","lvl2":"Adaptive Boosting (AdaBoost)"},"content":"Discrete Adaboost: error rate \\varepsilon is simply the error rate (1-Accuracy)\n\nReal Adaboost: \\varepsilon is based on predicted class probabilities \\hat{p}_c (better)\n\nAdaBoost for regression: \\varepsilon is either linear (|y_i-\\hat{y}_i|), squared ((y_i-\\hat{y}_i)^2), or exponential loss\n\nGentleBoost: adds a bound on model weights w_i\n\nLogitBoost: Minimizes logistic loss instead of exponential loss\n\\mathcal{L}_{Logistic} = \\sum_{n=1}^N log(1+e^{\\varepsilon(f_I(\\mathbf{x}))})\n\n","type":"content","url":"/notebooks/ensemble-learning#adaboost-variants","position":35},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Adaboost in action","lvl2":"Adaptive Boosting (AdaBoost)"},"type":"lvl3","url":"/notebooks/ensemble-learning#adaboost-in-action","position":36},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Adaboost in action","lvl2":"Adaptive Boosting (AdaBoost)"},"content":"Size of the samples represents sample weight\n\nBackground shows the latest tree’s predictions\n\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.tree import DecisionTreeClassifier\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\nfrom sklearn.preprocessing import normalize\n\n# Code adapted from https://xavierbourretsicotte.github.io/AdaBoost.html\ndef AdaBoost_scratch(X,y, M=10, learning_rate = 0.5):\n    #Initialization of utility variables\n    N = len(y)\n    estimator_list, y_predict_list, estimator_error_list, estimator_weight_list, sample_weight_list = [],[],[],[],[]\n\n    #Initialize the sample weights\n    sample_weight = np.ones(N) / N\n    sample_weight_list.append(sample_weight.copy())\n\n    #For m = 1 to M\n    for m in range(M):   \n\n        #Fit a classifier\n        estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes=2)\n        estimator.fit(X, y, sample_weight=sample_weight)\n        y_predict = estimator.predict(X)\n\n        #Misclassifications\n        incorrect = (y_predict != y)\n\n        #Estimator error\n        estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))\n        \n        #Boost estimator weights\n        estimator_weight =  learning_rate * np.log((1. - estimator_error) / estimator_error)\n\n        #Boost sample weights\n        sample_weight *= np.exp(estimator_weight * incorrect * ((sample_weight > 0) | (estimator_weight < 0)))\n        sample_weight *= np.exp(-estimator_weight * np.invert(incorrect * ((sample_weight > 0) | (estimator_weight < 0))))\n        sample_weight /= np.linalg.norm(sample_weight)\n        \n        #Save iteration values\n        estimator_list.append(estimator)\n        y_predict_list.append(y_predict.copy())\n        estimator_error_list.append(estimator_error.copy())\n        estimator_weight_list.append(estimator_weight.copy())\n        sample_weight_list.append(sample_weight.copy())\n        \n    #Convert to np array for convenience   \n    estimator_list = np.asarray(estimator_list)\n    y_predict_list = np.asarray(y_predict_list)\n    estimator_error_list = np.asarray(estimator_error_list)\n    estimator_weight_list = np.asarray(estimator_weight_list)\n    sample_weight_list = np.asarray(sample_weight_list)\n\n    #Predictions\n    preds = (np.array([np.sign((y_predict_list[:,point] * estimator_weight_list).sum()) for point in range(N)]))\n    #print('Accuracy = ', (preds == y).sum() / N) \n    \n    return estimator_list, estimator_weight_list, sample_weight_list, estimator_error_list\n\ndef plot_decision_boundary(classifier, X, y, N = 10, scatter_weights = np.ones(len(y)) , ax = None, title=None ):\n    '''Utility function to plot decision boundary and scatter plot of data'''\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n\n    # Get current axis and plot\n    if ax is None:\n        ax = plt.gca()\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 1000, edgecolors='none')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    if title:\n        ax.set_title(title, pad=1)\n    \n    # Plot classifier background\n    if classifier is not None:\n        xx, yy = np.meshgrid( np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n        \n        #Check what methods are available\n        if hasattr(classifier, \"decision_function\"):\n            zz = np.array( [classifier.decision_function(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n        elif hasattr(classifier, \"predict_proba\"):\n            zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n        else:\n            zz = np.array( [classifier(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n\n        # reshape result and plot\n        Z = zz.reshape(xx.shape)\n    \n        ax.contourf(xx, yy, Z, 2, cmap='RdBu', alpha=.5, levels=[0,0.5,1])\n        #ax.contour(xx, yy, Z, 2, cmap='RdBu', levels=[0,0.5,1])\n\n\nfrom sklearn.datasets import make_circles\nXa, ya = make_circles(n_samples=400, noise=0.15, factor=0.5, random_state=1)\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\n\nestimator_list, estimator_weight_list, sample_weight_list, estimator_error_list = AdaBoost_scratch(Xa, ya, M=60, learning_rate = 0.5)\ncurrent_ax = None\nweight_scale = 1\n\n@interact\ndef plot_adaboost(iteration=(0,60,1)):\n    if iteration == 0:\n        s_weights = (sample_weight_list[0,:] / sample_weight_list[0,:].sum() ) * weight_scale\n        plot_decision_boundary(None, Xa, ya, N = 20, scatter_weights =s_weights)\n    else:\n        s_weights = (sample_weight_list[iteration,:] / sample_weight_list[iteration,:].sum() ) * weight_scale\n        title = \"Base model {}, error: {:.2f}, weight: {:.2f}\".format(\n            iteration,estimator_error_list[iteration-1],estimator_weight_list[iteration-1])\n        plot_decision_boundary(estimator_list[iteration-1], Xa, ya, N = 20, scatter_weights =s_weights, ax=current_ax, title=title )\n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(2, 2, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(11*fig_scale, 6*fig_scale))\n    weight_scale = 0.5\n    for iteration, ax in zip([1, 5, 37, 59],axes.flatten()):\n        current_ax = ax\n        plot_adaboost(iteration)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#adaboost-in-action","position":37},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Examples","lvl2":"Adaptive Boosting (AdaBoost)"},"type":"lvl3","url":"/notebooks/ensemble-learning#examples","position":38},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Examples","lvl2":"Adaptive Boosting (AdaBoost)"},"content":"\n\nfrom sklearn.ensemble import AdaBoostClassifier\nnames = [\"AdaBoost 1 tree\", \"AdaBoost 3 trees\", \"AdaBoost 100 trees\"]\n\nclassifiers = [\n    AdaBoostClassifier(n_estimators=1, random_state=0, learning_rate=0.5),\n    AdaBoostClassifier(n_estimators=3, random_state=0, learning_rate=0.5),\n    AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate=0.5)\n    ]\n\nmglearn.plots.plot_classifiers(names, classifiers, figuresize=(6*fig_scale,3*fig_scale))  \n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#examples","position":39},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Bias-Variance analysis","lvl2":"Adaptive Boosting (AdaBoost)"},"type":"lvl3","url":"/notebooks/ensemble-learning#bias-variance-analysis","position":40},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Bias-Variance analysis","lvl2":"Adaptive Boosting (AdaBoost)"},"content":"AdaBoost reduces bias (and a little variance)\n\nBoosting is a bias reduction technique\n\nBoosting too much will eventually increase variance\n\nplot_bias_variance_rf(AdaBoostClassifier, cancer.data, cancer.target, warm_start=False)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#bias-variance-analysis","position":41},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Gradient Boosting"},"type":"lvl2","url":"/notebooks/ensemble-learning#gradient-boosting","position":42},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Gradient Boosting"},"content":"Ensemble of models, each fixing the remaining mistakes of the previous ones\n\nEach iteration, the task is to predict the residual error of the ensemble\n\nAdditive model: Predictions at iteration I are sum of base model predictions\n\nLearning rate (or shrinkage ) \\eta: small updates work better (reduces variance)\nf_I(\\mathbf{x}) = g_0(\\mathbf{x}) + \\sum_{i=1}^I \\eta \\cdot g_i(\\mathbf{x}) = f_{I-1}(\\mathbf{x}) + \\eta \\cdot g_I(\\mathbf{x})\n\nThe pseudo-residuals r_i are computed according to differentiable loss function\n\nE.g. least squares loss for regression and log loss for classification\n\nGradient descent: predictions get updated step by step until convergence\ng_i(\\mathbf{x}) \\approx r_{i} = - \\frac{\\partial \\mathcal{L}(y_i,f_{i-1}(x_i))}{\\partial f_{i-1}(x_i)}\n\nBase models g_i should be low variance, but flexible enough to predict residuals accurately\n\nE.g. decision trees of depth 2-5\n\n","type":"content","url":"/notebooks/ensemble-learning#gradient-boosting","position":43},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Gradient Boosting Trees (Regression)","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/notebooks/ensemble-learning#gradient-boosting-trees-regression","position":44},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Gradient Boosting Trees (Regression)","lvl2":"Gradient Boosting"},"content":"Base models are regression trees, loss function is square loss: \\mathcal{L} = \\frac{1}{2}(y_i - \\hat{y}_i)^2\n\nThe pseudo-residuals are simply the prediction errors for every sample:r_i = -\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} = -2 * \\frac{1}{2}(y_i - \\hat{y}_i) * (-1) =  y_i - \\hat{y}_i\n\nInitial model g_0 simply predicts the mean of y\n\nFor iteration m=1..M:\n\nFor all samples i=1..n, compute pseudo-residuals r_i = y_i - \\hat{y}_i\n\nFit a new regression tree model g_m(\\mathbf{x}) to r_{i}\n\nIn g_m(\\mathbf{x}), each leaf predicts the mean of all its values\n\nUpdate ensemble predictions \\hat{y} = g_0(\\mathbf{x}) + \\sum_{m=1}^M \\eta \\cdot g_m(\\mathbf{x})\n\nEarly stopping (optional): stop when performance on validation set does not improve for nr iterations\n\n","type":"content","url":"/notebooks/ensemble-learning#gradient-boosting-trees-regression","position":45},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Gradient Boosting Regression in action","lvl3":"Gradient Boosting Trees (Regression)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#gradient-boosting-regression-in-action","position":46},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Gradient Boosting Regression in action","lvl3":"Gradient Boosting Trees (Regression)","lvl2":"Gradient Boosting"},"content":"Residuals quickly drop to (near) zero\n\n# Example adapted from Andreas Mueller\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Make some toy data\ndef make_poly(n_samples=100):\n    rnd = np.random.RandomState(42)\n    x = rnd.uniform(-3, 3, size=n_samples)\n    y_no_noise = (x) ** 3\n    y = (y_no_noise + rnd.normal(scale=3, size=len(x))) / 2\n    return x.reshape(-1, 1), y\nXp, yp = make_poly()\n\n# Train gradient booster and get predictions\nXp_train, Xp_test, yp_train, yp_test = train_test_split(Xp, yp, random_state=0)\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=61, learning_rate=.3, random_state=0).fit(Xp_train, yp_train)\ngbrt.score(Xp_test, yp_test)\n\nline = np.linspace(Xp.min(), Xp.max(), 1000)\npreds = list(gbrt.staged_predict(line[:, np.newaxis]))\npreds_train = [np.zeros(len(yp_train))] + list(gbrt.staged_predict(Xp_train))\n\n# Plot\ndef plot_gradient_boosting_step(step, axes):\n    axes[0].plot(Xp_train[:, 0], yp_train - preds_train[step], 'o', alpha=0.5, markersize=10*fig_scale)\n    axes[0].plot(line, gbrt.estimators_[step, 0].predict(line[:, np.newaxis]), linestyle='-', lw=3*fig_scale)\n    axes[0].plot(line, [0]*len(line), c='k', linestyle='-', lw=1*fig_scale)\n    axes[1].plot(Xp_train[:, 0], yp_train, 'o',  alpha=0.5, markersize=10*fig_scale)\n    axes[1].plot(line, preds[step], linestyle='-', lw=3*fig_scale)\n    axes[1].vlines(Xp_train[:, 0], yp_train, preds_train[step+1])\n\n    axes[0].set_title(\"Residual prediction step {}\".format(step + 1), fontsize=9)\n    axes[1].set_title(\"Total prediction step {}\".format(step + 1), fontsize=9)\n    axes[0].set_ylim(yp.min(), yp.max())\n    axes[1].set_ylim(yp.min(), yp.max())\n    plt.tight_layout();\n\n@interact\ndef plot_gradient_boosting(step = (0, 60, 1)):\n    fig, axes = plt.subplots(1, 2, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(10*fig_scale, 4*fig_scale))\n    plot_gradient_boosting_step(step, axes)\n    plt.show()\n\n\n\n\nif not interactive:\n    fig, all_axes = plt.subplots(3, 2, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(10*fig_scale, 5*fig_scale))\n    for i, s in enumerate([0,3,9]):\n        axes = all_axes[i,:]\n        plot_gradient_boosting_step(s, axes)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#gradient-boosting-regression-in-action","position":47},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/notebooks/ensemble-learning#gradientboosting-algorithm-classification","position":48},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"content":"Base models are regression trees, predict probability of positive class p\n\nFor multi-class problems, train one tree per class\n\nUse (binary) log loss, with true class y_i \\in {0,1}: \\mathcal{L_{log}} = - \\sum_{i=1}^{N} \\big[ y_i log(p_i) + (1-y_i) log(1-p_i) \\big] \n\nThe pseudo-residuals are simply the difference between true class and predicted p:\n\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}} = \\frac{\\partial \\mathcal{L}}{\\partial log(p_i)} = y_i - p_i\n\nInitial model g_0 predicts p = log(\\frac{\\#positives}{\\#negatives})\n\nFor iteration m=1..M:\n\nFor all samples i=1..n, compute pseudo-residuals r_i = y_i - p_i\n\nFit a new regression tree model g_m(\\mathbf{x}) to r_{i}\n\nIn g_m(\\mathbf{x}), each leaf predicts \\frac{\\sum_{i} r_i}{\\sum_{i} p_i(1-p_i)}\n\nUpdate ensemble predictions \\hat{y} = g_0(\\mathbf{x}) + \\sum_{m=1}^M \\eta \\cdot g_m(\\mathbf{x})\n\nEarly stopping (optional): stop when performance on validation set does not improve for nr iterations\n\n","type":"content","url":"/notebooks/ensemble-learning#gradientboosting-algorithm-classification","position":49},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Gradient Boosting Classification in action","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#gradient-boosting-classification-in-action","position":50},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Gradient Boosting Classification in action","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"content":"Size of the samples represents the residual weights: most quickly drop to (near) zero\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nXa_train, Xa_test, ya_train, ya_test = train_test_split(Xa, ya, random_state=0)\ngbct = GradientBoostingClassifier(max_depth=2, n_estimators=60, learning_rate=.3, random_state=0).fit(Xa_train, ya_train)\ngbct.score(Xa_test, ya_test)\npreds_train_cl = [np.zeros(len(ya_train))] + list(gbct.staged_predict_proba(Xa_train))\ncurrent_gb_ax = None\nweight_scale = 1\n\ndef plot_gb_decision_boundary(gbmodel, step, X, y, N = 10, scatter_weights = np.ones(len(y)) , ax = None, title = None ):\n    '''Utility function to plot decision boundary and scatter plot of data'''\n    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n\n    # Get current axis and plot\n    if ax is None:\n        ax = plt.gca()\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 40, edgecolors='none')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    if title:\n        ax.set_title(title, pad='0.5')\n    \n    # Plot classifier background\n    if gbmodel is not None:\n        xx, yy = np.meshgrid( np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n        zz = np.array( [list(gbmodel.staged_predict_proba(np.array([xi,yi]).reshape(1,-1)))[step][:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n        Z = zz.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, 2, cmap='RdBu', alpha=.5, levels=[0,0.5,1])\n\n\n@interact\ndef plot_gboost(iteration=(1,60,1)):\n    pseudo_residuals = np.abs(ya_train - preds_train_cl[iteration][:,1])\n    title = \"Base model {}, error: {:.2f}\".format(iteration,np.sum(pseudo_residuals))\n    plot_gb_decision_boundary(gbct, (iteration-1), Xa_train, ya_train, N = 20, scatter_weights =pseudo_residuals * weight_scale, ax=current_gb_ax, title=title ) \n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(2, 2, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(10*fig_scale, 6*fig_scale))\n    weight_scale = 0.3\n    for iteration, ax in zip([1, 5, 17, 59],axes.flatten()):\n        current_gb_ax = ax\n        plot_gboost(iteration)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#gradient-boosting-classification-in-action","position":51},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Examples","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#examples-1","position":52},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Examples","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"content":"\n\nnames = [\"GradientBoosting 1 tree\", \"GradientBoosting 3 trees\", \"GradientBoosting 100 trees\"]\n\nclassifiers = [\n    GradientBoostingClassifier(n_estimators=1, random_state=0, learning_rate=0.5),\n    GradientBoostingClassifier(n_estimators=3, random_state=0, learning_rate=0.5),\n    GradientBoostingClassifier(n_estimators=100, random_state=0, learning_rate=0.5)\n    ]\n\nmglearn.plots.plot_classifiers(names, classifiers, figuresize=(6*fig_scale,3*fig_scale))  \n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#examples-1","position":53},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Bias-variance analysis","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#bias-variance-analysis-1","position":54},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Bias-variance analysis","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"content":"Gradient Boosting is very effective at reducing bias error\n\nBoosting too much will eventually increase variance\n\n# Note: I tried if HistGradientBoostingClassifier is faster. It's not.\n# We're training many small models here and the thread spawning likely causes too much overhead\nplot_bias_variance_rf(GradientBoostingClassifier, cancer.data, cancer.target, warm_start=True)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#bias-variance-analysis-1","position":55},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Feature importance","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#feature-importance-1","position":56},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Feature importance","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"content":"Gradient Boosting also provide feature importances, based on many trees\n\nCompared to RandomForests, the trees are smaller, hence more features have zero importance\n\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(Xc_train, yc_train)\nplot_feature_importances_cancer(gbrt)\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#feature-importance-1","position":57},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Gradient Boosting: strengths and weaknesses","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#gradient-boosting-strengths-and-weaknesses","position":58},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"Gradient Boosting: strengths and weaknesses","lvl3":"GradientBoosting Algorithm (Classification)","lvl2":"Gradient Boosting"},"content":"Among the most powerful and widely used models\n\nWork well on heterogeneous features and different scales\n\nTypically better than random forests, but requires more tuning, longer training\n\nDoes not work well on high-dimensional sparse data\n\nMain hyperparameters:\n\nn_estimators: Higher is better, but will start to overfit\n\nlearning_rate: Lower rates mean more trees are needed to get more complex models\n\nSet n_estimators as high as possible, then tune learning_rate\n\nOr, choose a learning_rate and use early stopping to avoid overfitting\n\nmax_depth: typically kept low (<5), reduce when overfitting\n\nmax_features: can also be tuned, similar to random forests\n\nn_iter_no_change: early stopping: algorithm stops if improvement is less than a certain tolerance tol for more than n_iter_no_change iterations.\n\n","type":"content","url":"/notebooks/ensemble-learning#gradient-boosting-strengths-and-weaknesses","position":59},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Extreme Gradient Boosting (XGBoost)","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/notebooks/ensemble-learning#extreme-gradient-boosting-xgboost","position":60},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Extreme Gradient Boosting (XGBoost)","lvl2":"Gradient Boosting"},"content":"Faster version of gradient boosting: allows more iterations on larger datasets\n\nNormal regression trees: split to minimize squared loss of leaf predictions\n\nXGBoost trees only fit residuals: split so that residuals in leaf are more similar\n\nDon’t evaluate every split point, only q quantiles per feature (binning)\n\nq is hyperparameter (sketch_eps, default 0.03)\n\nFor large datasets, XGBoost uses approximate quantiles\n\nCan be parallelized (multicore) by chunking the data and combining histograms of data\n\nFor classification, the quantiles are weighted by p(1-p)\n\nGradient descent sped up by using the second derivative of the loss function\n\nStrong regularization by pre-pruning the trees\n\nColumn and row are randomly subsampled when computing splits\n\nSupport for out-of-core computation (data compression in RAM, sharding,...)\n\n","type":"content","url":"/notebooks/ensemble-learning#extreme-gradient-boosting-xgboost","position":61},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"XGBoost in practice","lvl3":"Extreme Gradient Boosting (XGBoost)","lvl2":"Gradient Boosting"},"type":"lvl4","url":"/notebooks/ensemble-learning#xgboost-in-practice","position":62},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl4":"XGBoost in practice","lvl3":"Extreme Gradient Boosting (XGBoost)","lvl2":"Gradient Boosting"},"content":"Not part of scikit-learn, but HistGradientBoostingClassifier is similar\n\nbinning, multicore,...\n\nThe xgboost python package is sklearn-compatible\n\nInstall separately, conda install -c conda-forge xgboost\n\nAllows learning curve plotting and warm-starting\n\nFurther reading:\n\nXGBoost Documentation\n\nPaper\n\nVideo\n\n","type":"content","url":"/notebooks/ensemble-learning#xgboost-in-practice","position":63},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"LightGBM","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/notebooks/ensemble-learning#lightgbm","position":64},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"LightGBM","lvl2":"Gradient Boosting"},"content":"Another fast boosting technique\n\nUses gradient-based sampling\n\nuse all instances with large gradients/residuals (e.g. 10% largest)\n\nrandomly sample instances with small gradients, ignore the rest\n\nintuition: samples with small gradients are already well-trained.\n\nrequires adapted information gain criterion\n\nDoes smarter encoding of categorical features\n\n","type":"content","url":"/notebooks/ensemble-learning#lightgbm","position":65},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"CatBoost","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/notebooks/ensemble-learning#catboost","position":66},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"CatBoost","lvl2":"Gradient Boosting"},"content":"Another fast boosting technique\n\nOptimized for categorical variables\n\nUses bagged and smoothed version of target encoding\n\nUses symmetric trees: same split for all nodes on a given level\n\nCan be much faster\n\nAllows monotonicity constraints for numeric features\n\nModel must be be a non-decreasing function of these features\n\nLots of tooling (e.g. GPU training)\n\n","type":"content","url":"/notebooks/ensemble-learning#catboost","position":67},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Stacking"},"type":"lvl2","url":"/notebooks/ensemble-learning#stacking","position":68},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Stacking"},"content":"Choose M different base-models, generate predictions\n\nStacker (meta-model) learns mapping between predictions and correct label\n\nCan also be repeated: multi-level stacking\n\nPopular stackers: linear models (fast) and gradient boosting (accurate)\n\nCascade stacking: adds base-model predictions as extra features\n\nModels need to be sufficiently different, be experts at different parts of the data\n\nCan be very accurate, but also very slow to predict\n\n","type":"content","url":"/notebooks/ensemble-learning#stacking","position":69},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Other ensembling techniques"},"type":"lvl2","url":"/notebooks/ensemble-learning#other-ensembling-techniques","position":70},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl2":"Other ensembling techniques"},"content":"Hyper-ensembles: same basic model but with different hyperparameter settings\n\nCan combine overfitted and underfitted models\n\nDeep ensembles: ensembles of deep learning models\n\nBayes optimal classifier: ensemble of all possible models (largely theoretic)\n\nBayesian model averaging: weighted average of probabilistic models, weighted by their posterior probabilities\n\nCross-validation selection: does internal cross-validation to select best of M models\n\nAny combination of different ensembling techniques\n\n%%HTML\n<style>\ntd {font-size: 20px}\nth {font-size: 20px}\n.rendered_html table, .rendered_html td, .rendered_html th {\n    font-size: 20px;\n}\n</style>\n\n\n\n","type":"content","url":"/notebooks/ensemble-learning#other-ensembling-techniques","position":71},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Algorithm overview","lvl2":"Other ensembling techniques"},"type":"lvl3","url":"/notebooks/ensemble-learning#algorithm-overview","position":72},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Algorithm overview","lvl2":"Other ensembling techniques"},"content":"Name\n\nRepresentation\n\nLoss function\n\nOptimization\n\nRegularization\n\nClassification trees\n\nDecision tree\n\nEntropy / Gini index\n\nHunt’s algorithm\n\nTree depth,...\n\nRegression trees\n\nDecision tree\n\nSquare loss\n\nHunt’s algorithm\n\nTree depth,...\n\nRandomForest\n\nEnsemble of randomized trees\n\nEntropy / Gini / Square\n\n(Bagging)\n\nNumber/depth of trees,...\n\nAdaBoost\n\nEnsemble of stumps\n\nExponential loss\n\nGreedy search\n\nNumber/depth of trees,...\n\nGradientBoostingRegression\n\nEnsemble of regression trees\n\nSquare loss\n\nGradient descent\n\nNumber/depth of trees,...\n\nGradientBoostingClassification\n\nEnsemble of regression trees\n\nLog loss\n\nGradient descent\n\nNumber/depth of trees,...\n\nXGBoost, LightGBM, CatBoost\n\nEnsemble of XGBoost trees\n\nSquare/log loss\n\n2nd order gradients\n\nNumber/depth of trees,...\n\nStacking\n\nEnsemble of heterogeneous models\n\n/\n\n/\n\nNumber of models,...\n\n","type":"content","url":"/notebooks/ensemble-learning#algorithm-overview","position":73},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Summary","lvl2":"Other ensembling techniques"},"type":"lvl3","url":"/notebooks/ensemble-learning#summary","position":74},{"hierarchy":{"lvl1":"Lecture 4. Ensemble Learning","lvl3":"Summary","lvl2":"Other ensembling techniques"},"content":"Ensembles of voting classifiers improve performance\n\nWhich models to choose? Consider bias-variance tradeoffs!\n\nBagging / RandomForest is a variance-reduction technique\n\nBuild many high-variance (overfitting) models on random data samples\n\nThe more different the models, the better\n\nAggregation (soft voting) over many models reduces variance\n\nDiminishing returns, over-smoothing may increase bias error\n\nParallellizes easily, doesn’t require much tuning\n\nBoosting is a bias-reduction technique\n\nBuild low-variance models that correct each other’s mistakes\n\nBy reweighting misclassified samples: AdaBoost\n\nBy predicting the residual error: Gradient Boosting\n\nAdditive models: predictions are sum of base-model predictions\n\nCan drive the error to zero, but risk overfitting\n\nDoesn’t parallelize easily. Slower to train, much faster to predict.\n\nXGBoost,LightGBM,... are fast and offer some parallellization\n\nStacking: learn how to combine base-model predictions\n\nBase-models still have to be sufficiently different","type":"content","url":"/notebooks/ensemble-learning#summary","position":75},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing"},"type":"lvl1","url":"/notebooks/data-preprocessing","position":0},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing"},"content":"Real-world machine learning pipelines\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots\nif interactive:\n    fig_scale = 0.9\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.35\n    plt.rcParams.update(print_config)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing","position":1},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Data transformations"},"type":"lvl2","url":"/notebooks/data-preprocessing#data-transformations","position":2},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Data transformations"},"content":"Machine learning models make a lot of assumptions about the data\n\nIn reality, these assumptions are often violated\n\nWe build pipelines that transform the data before feeding it to the learners\n\nScaling (or other numeric transformations)\n\nEncoding (convert categorical features into numerical ones)\n\nAutomatic feature selection\n\nFeature engineering (e.g. binning, polynomial features,...)\n\nHandling missing data\n\nHandling imbalanced data\n\nDimensionality reduction (e.g. PCA)\n\nLearned embeddings (e.g. for text)\n\nSeek the best combinations of transformations and learning methods\n\nOften done empirically, using cross-validation\n\nMake sure that there is no data leakage during this process!\n\n","type":"content","url":"/notebooks/data-preprocessing#data-transformations","position":3},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Scaling"},"type":"lvl2","url":"/notebooks/data-preprocessing#scaling","position":4},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Scaling"},"content":"Use when different numeric features have different scales (different range of values)\n\nFeatures with much higher values may overpower the others\n\nMethod based on distances (e.g. kNN, SVMs) will over-emphasize those features\n\nParametric models (e.g. linear models, neural nets) become hard to train (sensitive weights)\n\nGoal: bring them all within the same range\n\nDifferent methods exist (see the recap for details)\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler, Normalizer, MaxAbsScaler\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# Iris dataset with some added noise\ndef noisy_iris():\n    iris = fetch_openml(\"iris\", return_X_y=True, as_frame=False)\n    X, y = iris\n    np.random.seed(0)\n    noise = np.random.normal(0, 0.1, 150)\n    for i in range(4):\n        X[:, i] = X[:, i] + noise\n    X[:, 0] = X[:, 0] + 3 # add more skew \n    label_encoder = LabelEncoder().fit(y)\n    y = label_encoder.transform(y)\n    return X, y\n\nscalers = [StandardScaler(), RobustScaler(), MinMaxScaler(), Normalizer(norm='l1'), MaxAbsScaler()]\n\n@interact\ndef plot_scaling(scaler=scalers):\n    X, y = noisy_iris()\n    X = X[:,:2] # Use only first 2 features\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8*fig_scale, 3*fig_scale))\n    axes[0].scatter(X[:, 0], X[:, 1], c=y, s=1*fig_scale, cmap=\"brg\")\n    axes[0].set_xlim(-15, 15)\n    axes[0].set_ylim(-5, 5)\n    axes[0].set_title(\"Original Data\")\n    axes[0].spines['left'].set_position('zero')\n    axes[0].spines['bottom'].set_position('zero')\n    \n    X_ = scaler.fit_transform(X)\n    axes[1].scatter(X_[:, 0], X_[:, 1], c=y, s=1*fig_scale, cmap=\"brg\")\n    axes[1].set_xlim(-2, 2)\n    axes[1].set_ylim(-2, 2)\n    axes[1].set_title(type(scaler).__name__)\n    axes[1].set_xticks([-1,1])\n    axes[1].set_yticks([-1,1])\n    axes[1].spines['left'].set_position('center')\n    axes[1].spines['bottom'].set_position('center')\n\n    for ax in axes:\n        ax.spines['right'].set_color('none')\n        ax.spines['top'].set_color('none')\n        ax.xaxis.set_ticks_position('bottom')\n        ax.yaxis.set_ticks_position('left')\n    plt.show()\n\n\n\nif not interactive:\n    plot_scaling(scalers[0])\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#scaling","position":5},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Power transformations","lvl2":"Scaling"},"type":"lvl3","url":"/notebooks/data-preprocessing#power-transformations","position":6},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Power transformations","lvl2":"Scaling"},"content":"Some features follow certain distributions\n\nE.g. number of Instagram followers is log-normal distributed\n\nYet most ML methods assume normally-distributed data\n\nBox-Cox transformations transform these to normal distributions (\\lambda is fitted)\n\nOnly works for positive values, use Yeo-Johnson otherwise\nbc_{\\lambda}(x) = \\begin{cases} log(x) & \\lambda = 0\\\\ \\frac{x^{\\lambda}-1}{\\lambda} & \\lambda \\neq 0 \\\\ \\end{cases}\n\n# Adapted from an example by Eric Chang and Nicolas Hug \nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Power transformer with Box-Cox\nbc = PowerTransformer(method='box-cox')\n\n# Generate data\nrng = np.random.RandomState(304) # Random number generator\nsize = (1000, 1)\nX_lognormal = rng.lognormal(size=size) # lognormal distribution\nX_chisq = rng.chisquare(df=3, size=size) # chi-squared distribution\nX_weibull = rng.weibull(a=50, size=size) # weibull distribution\n\n# create plots\ndistributions = [\n    ('Lognormal', X_lognormal),\n    ('Chi-squared', X_chisq),\n    ('Weibull', X_weibull)\n]\ncolors = ['#D81B60', '#0188FF', '#FFC107']\n\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(9*fig_scale,2.8*fig_scale))\naxes = axes.flatten()\naxes_idxs = [(0, 3), (1, 4), (2, 5)]\naxes_list = [(axes[i], axes[j]) for (i, j) in axes_idxs]\n\nfor distribution, color, axes in zip(distributions, colors, axes_list):\n    name, X = distribution\n    X_train, X_test = train_test_split(X, test_size=.5)\n\n    # perform power transforms and quantile transform\n    X_trans_bc = bc.fit(X_train).transform(X_test)\n    lmbda_bc = round(bc.lambdas_[0], 2)\n\n    ax_original, ax_bc = axes\n    ax_original.hist(X_train, color=color, bins=30)\n    ax_original.set_title(name)\n    ax_original.tick_params(axis='both', which='major')\n\n    ax_bc.hist(X_trans_bc, color=color, bins=30)\n    title = 'After {}'.format('Box-Cox')\n    if lmbda_bc is not None:\n        title += r' $\\lambda$ = {}'.format(lmbda_bc)\n    ax_bc.set_title(title)\n    ax_bc.tick_params(axis='both', which='major')\n    ax_bc.set_xlim([-3.5, 3.5])\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#power-transformations","position":7},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Categorical feature encoding"},"type":"lvl2","url":"/notebooks/data-preprocessing#categorical-feature-encoding","position":8},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Categorical feature encoding"},"content":"Many algorithms can only handle numeric features, so we need to encode the categorical ones\n\ntable_font_size = 20\nheading_properties = [('font-size', table_font_size)]\ncell_properties = [('font-size', table_font_size)]\ndfstyle = [dict(selector=\"th\", props=heading_properties),\\\n dict(selector=\"td\", props=cell_properties)]\n\n\n\n%%HTML\n<style>\ntd {font-size: 10px}\nth {font-size: 10px}\n.rendered_html table, .rendered_html td, .rendered_html th {\n    font-size: 20px;\n}\n</style>\n\n\n\nimport pandas as pd\n\nX = pd.DataFrame({'boro': ['Manhattan', 'Queens', 'Manhattan', 'Brooklyn', 'Brooklyn', 'Bronx'],\n                   'salary': [103, 89, 142, 54, 63, 219]})\ny = pd.DataFrame({'vegan': [0, 0, 0, 1, 1, 0]})\ndf = X.copy()\ndf['vegan'] = y\ndf.style.set_table_styles(dfstyle)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#categorical-feature-encoding","position":9},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Ordinal encoding","lvl2":"Categorical feature encoding"},"type":"lvl3","url":"/notebooks/data-preprocessing#ordinal-encoding","position":10},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Ordinal encoding","lvl2":"Categorical feature encoding"},"content":"Simply assigns an integer value to each category in the order they are encountered\n\nOnly really useful if there exist a natural order in categories\n\nModel will consider one category to be ‘higher’ or ‘closer’ to another\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nencoder = OrdinalEncoder(dtype=int)\n\n# Encode first feature, rest passthrough\npreprocessor = ColumnTransformer(transformers=[('cat', encoder, [0])], remainder='passthrough')\nX_ordinal = preprocessor.fit_transform(X,y)\n\n# Convert to pandas for nicer output\ndf = pd.DataFrame(data=X_ordinal, columns=[\"boro_ordinal\",\"salary\"])\ndf = pd.concat([X['boro'], df], axis=1)\ndf.style.set_table_styles(dfstyle)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#ordinal-encoding","position":11},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"One-hot encoding (dummy encoding)","lvl2":"Categorical feature encoding"},"type":"lvl3","url":"/notebooks/data-preprocessing#one-hot-encoding-dummy-encoding","position":12},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"One-hot encoding (dummy encoding)","lvl2":"Categorical feature encoding"},"content":"Simply adds a new 0/1 feature for every category, having 1 (hot) if the sample has that category\n\nCan explode if a feature has lots of values, causing issues with high dimensionality\n\nWhat if test set contains a new category not seen in training data?\n\nEither ignore it (just use all 0’s in row), or handle manually (e.g. resample)\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(dtype=int)\n\n# Encode first feature, rest passthrough\npreprocessor = ColumnTransformer(transformers=[('cat', encoder, [0])], remainder='passthrough')\nX_ordinal = preprocessor.fit_transform(X,y)\n\n# Convert to pandas for nicer output\ndf = pd.DataFrame(data=X_ordinal, columns=[\"boro_Bronx\",\"boro_Brooklyn\",\"boro_Manhattan\",\"boro_Queens\",\"salary\"])\ndf = pd.concat([X['boro'], df], axis=1)\ndf.style.set_table_styles(dfstyle)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#one-hot-encoding-dummy-encoding","position":13},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Target encoding","lvl2":"Categorical feature encoding"},"type":"lvl3","url":"/notebooks/data-preprocessing#target-encoding","position":14},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Target encoding","lvl2":"Categorical feature encoding"},"content":"Value close to 1 if category correlates with class 1, close to 0 if correlates with class 0\n\nPreferred when you have lots of category values. It only creates one new feature per class\n\nBlends posterior probability of the target \\frac{n_{iY}}{n_i} and prior probability \\frac{n_Y}{n}.\n\nn_{iY}: nr of samples with category i and class Y=1, n_{i}: nr of samples with category i\n\nBlending: gradually decrease as you get more examples of category i and class Y=0\nEnc(i) = \\color{blue}{\\frac{1}{1+e^{-(n_{i}-1)}} \\frac{n_{iY}}{n_i}} + \\color{green}{(1-\\frac{1}{1+e^{-(n_{i}-1)}}) \\frac{n_Y}{n}}\n\nSame for regression, using \\frac{n_{iY}}{n_i}: average target value with category i, \\frac{n_{Y}}{n}: overall mean\n\n# smoothed sigmoid\ndef sigmoid(x, smoothing=1):\n    return 1 / (1 + np.exp(-(x-1)/smoothing))\n\ndef plot_blend():\n    n = 20 # 20 points\n    ny = 10 # 10 points of class Yes\n    niy = 2 # number of points of category i and class Yes\n    ni = np.linspace(niy,10,100) #  10 points of category i\n    \n    fig, ax = plt.subplots(figsize=(6*fig_scale,1.8*fig_scale))\n\n    ax.plot(ni,sigmoid(ni)*(niy/ni),lw=1.5,c='b',label='posterior term', linestyle='-')\n    ax.plot(ni,(1-sigmoid(ni))*(ny/n),lw=1.5,c='g',label='prior term', linestyle='-')\n    ax.plot(ni,sigmoid(ni)*(niy/ni) + (1-sigmoid(ni))*(ny/n),lw=1.5,c='r',label='blend', linestyle='-')\n    ax.set_xlabel(r\"$n_{i}$ ($n_{iY} = 2$)\")\n    ax.set_ylabel(\"Encoding for Y=1\")\n    ax.set_ylim(0,1)\n    plt.grid()\n    plt.legend();\nplot_blend()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#target-encoding","position":15},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"Example (category_encoders)","lvl3":"Target encoding","lvl2":"Categorical feature encoding"},"type":"lvl4","url":"/notebooks/data-preprocessing#example-category-encoders","position":16},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"Example (category_encoders)","lvl3":"Target encoding","lvl2":"Categorical feature encoding"},"content":"For Brooklyn, n_{iY}=2, n_{i}=2, n_{Y}=2, n=6\n\nWould be closer to 1 if there were more examples, all with label 1\nEnc(Brooklyn) = \\frac{1}{1+e^{-1}} \\frac{2}{2} + (1-\\frac{1}{1+e^{-1}}) \\frac{2}{6}  = 0,82\n\nNote: the implementation used here sets Enc(i)=\\frac{n_Y}{n} when n_{iY}=1\n\n# Not in sklearn yet, use package category_encoders\nfrom category_encoders import TargetEncoder\n\nencoder = TargetEncoder(return_df=True, smoothing=1, min_samples_leaf=1)\nencoder.fit(X, y)\npd_te = encoder.fit_transform(X,y)\n\n# Convert to pandas for nicer output\ndf = pd.DataFrame(data=pd_te, columns=[\"boro\",\"salary\"]).rename(columns={'boro': 'boro_encoded'})\ndf = pd.concat([X['boro'], df, y], axis=1)\ndf.style.set_table_styles(dfstyle)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#example-category-encoders","position":17},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (scikit-learn)","lvl2":"Categorical feature encoding"},"type":"lvl3","url":"/notebooks/data-preprocessing#in-practice-scikit-learn","position":18},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (scikit-learn)","lvl2":"Categorical feature encoding"},"content":"Ordinal encoding, one-hot encoding, target encoding are implemented in scikit-learn\n\ndtype defines that the output should be an integerordinal_encoder = OrdinalEncoder(dtype=int)\none_hot_encoder = OneHotEncoder(dtype=int)\ntarget_encoder = TargetEncoder(smooth=0.25)\n\nTarget encoding is available in category_encoders\n\nscikit-learn compatible\n\nAlso includes other, very specific encoderstarget_encoder = TargetEncoder(return_df=True)\n\nAll encoders (and scalers) follow the fit-transform paradigm\n\nfit prepares the encoder, transform actually encodes the features\n\nWe’ll discuss this nextencoder.fit(X, y)\nX_encoded = encoder.transform(X,y)\n\n","type":"content","url":"/notebooks/data-preprocessing#in-practice-scikit-learn","position":19},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Applying data transformations"},"type":"lvl2","url":"/notebooks/data-preprocessing#applying-data-transformations","position":20},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Applying data transformations"},"content":"Data transformations should always follow a fit-predict paradigm\n\n‘Fit’ (train) the transformer on the training data only\n\nE.g. for a standard scaler: record the mean and standard deviation\n\nTransform (e.g. scale) the training data, then train the learning model\n\nTransform (e.g. scale) the test data, then evaluate the model\n\nOnly scale the input features (X), not the targets (y)!\n\nIf you fit and transform the whole dataset before splitting, you get data leakage\n\nYou have looked at the test data before training the model\n\nIf you fit and transform the training and test data separately, you distort the test set\n\nE.g. training and test points are scaled differently\n\nIn both cases, model evaluations will be misleading or meaningless\n\n","type":"content","url":"/notebooks/data-preprocessing#applying-data-transformations","position":21},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Test set distortion","lvl2":"Applying data transformations"},"type":"lvl3","url":"/notebooks/data-preprocessing#test-set-distortion","position":22},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Test set distortion","lvl2":"Applying data transformations"},"content":"Properly scaled: fit on training set, transform on training and test set\n\nImproperly scaled: fit and transform on the training and test data separately\n\nTest data points nowhere near same training data points\n\nfrom matplotlib.axes._axes import _log as matplotlib_axes_logger\nmatplotlib_axes_logger.setLevel('ERROR')\n\nfrom sklearn.datasets import make_blobs\n# make synthetic data\nX, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n# split it into training and test set\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n\n# plot the training and test set\nfig, axes = plt.subplots(1, 3, figsize=(10*fig_scale, 4*fig_scale))\naxes[0].scatter(X_train[:, 0], X_train[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=10*fig_scale)\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=10*fig_scale)\naxes[0].legend(loc='upper left')\naxes[0].set_title(\"Original Data\")\n\n# scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# visualize the properly scaled data\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=10*fig_scale)\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=10*fig_scale)\naxes[1].set_title(\"Scaled Data\")\n\n# rescale the test set separately\n# so that test set min is 0 and test set max is 1\n# DO NOT DO THIS! For illustration purposes only\ntest_scaler = MinMaxScaler()\ntest_scaler.fit(X_test)\nX_test_scaled_badly = test_scaler.transform(X_test)\n\n# visualize wrongly scaled data\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"training set\", s=10*fig_scale)\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n                marker='^', c=mglearn.cm2(1), label=\"test set\", s=10*fig_scale)\naxes[2].set_title(\"Improperly Scaled Data\")\n\nfor ax in axes:\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\nfig.tight_layout()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#test-set-distortion","position":23},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Data leakage","lvl2":"Applying data transformations"},"type":"lvl3","url":"/notebooks/data-preprocessing#data-leakage","position":24},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Data leakage","lvl2":"Applying data transformations"},"content":"Cross-validation: training set is split into training and validation sets for model selection\n\nIncorrect: Scaler is fit on whole training set before doing cross-validation\n\nData leaks from validation folds into training folds, selected model may be optimistic\n\nRight: Scaler is fit on training folds only\n\n","type":"content","url":"/notebooks/data-preprocessing#data-leakage","position":25},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Data processing pipelines","lvl2":"Applying data transformations"},"type":"lvl3","url":"/notebooks/data-preprocessing#data-processing-pipelines","position":26},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Data processing pipelines","lvl2":"Applying data transformations"},"content":"Many ML libraries have ways to build ‘pipelines’ of data transformations and learners\n\nEnsures that data transformations are applied correctly\n\nE.g. scikit-learn pipelines have a fit, predict, and score method\n\nInternally applies transformations correctly\n\nIf not, ensure that no data leakage happens!\n\nData transformations should be independent from the test set\n\n","type":"content","url":"/notebooks/data-preprocessing#data-processing-pipelines","position":27},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"In practice (scikit-learn)","lvl3":"Data processing pipelines","lvl2":"Applying data transformations"},"type":"lvl4","url":"/notebooks/data-preprocessing#in-practice-scikit-learn-1","position":28},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"In practice (scikit-learn)","lvl3":"Data processing pipelines","lvl2":"Applying data transformations"},"content":"A pipeline combines multiple processing steps in a single estimator\n\nCan also be split, combined, tuned,...\n\nSee the lab tutorial for more in-depth examples\n\n# Make pipeline\npipe = make_pipeline(MinMaxScaler(), Ridge())\n\n# Correct fit and score\nscore = pipe.fit(X_train, y_train).score(X_test, y_test)\n\n# Correct cross-validation\nscores = cross_val_score(pipe, X, y)\n\n# Correct tuning\nparam_grid = {'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train).best_params_\n\n","type":"content","url":"/notebooks/data-preprocessing#in-practice-scikit-learn-1","position":29},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Automatic Feature Selection"},"type":"lvl2","url":"/notebooks/data-preprocessing#automatic-feature-selection","position":30},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Automatic Feature Selection"},"content":"It can be a good idea to reduce the number of features to only the most useful ones\n\nSimpler models that generalize better (less overfitting)\n\nCurse of dimensionality (e.g. kNN)\n\nEven models such as RandomForest can benefit from this\n\nSometimes it is one of the main methods to improve models (e.g. gene expression data)\n\nFaster prediction and training\n\nTraining time can be quadratic (or cubic) in number of features\n\nEasier data collection, smaller models (less storage)\n\nMore interpretable models: fewer features to look at\n\n","type":"content","url":"/notebooks/data-preprocessing#automatic-feature-selection","position":31},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Example: bike sharing","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#example-bike-sharing","position":32},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Example: bike sharing","lvl2":"Automatic Feature Selection"},"content":"The Bike Sharing Demand dataset shows the amount of bikes rented in Washington DC\n\nSome features are clearly more informative than others (e.g. temp, hour)\n\nSome are correlated (e.g. temp and feel_temp)\n\nWe add two random features at the end\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Get bike sharing data from OpenML\nbikes = fetch_openml(data_id=42713, as_frame=True)\nX_bike_cat, y_bike = bikes.data, bikes.target\n\n# Optional: take half of the data to speed up processing\nX_bike_cat = X_bike_cat.sample(frac=0.5, random_state=1)\ny_bike = y_bike.sample(frac=0.5, random_state=1)\n\n# One-hot encode the categorical features\nencoder = OneHotEncoder(dtype=int)\npreprocessor = ColumnTransformer(transformers=[('cat', encoder, [0,7])], remainder='passthrough')\nX_bike = preprocessor.fit_transform(X_bike_cat,y)\n\n# Add 2 random features at the end\nrandom_features = np.random.rand(len(X_bike),2)\nX_bike = np.append(X_bike,random_features, axis=1)\n\n# Create feature names\nbike_names = ['summer','winter', 'spring', 'fall', 'clear', 'misty', 'rain', 'heavy_rain']\nbike_names.extend(X_bike_cat.columns[1:7])\nbike_names.extend(X_bike_cat.columns[8:])\nbike_names.extend(['random_1','random_2'])\n\n\n\n#pd.set_option('display.max_columns', 20)\n#pd.DataFrame(data=X_bike, columns=bike_names).head()\n\n\n\nfig, axes = plt.subplots(2, 10, figsize=(6*fig_scale, 2*fig_scale))\nfor i, ax in enumerate(axes.ravel()):\n    ax.plot(X_bike[:, i], y_bike[:], '.', alpha=.1)\n    ax.set_xlabel(\"{}\".format(bike_names[i]))\n    ax.get_yaxis().set_visible(False)\nfor i in range(2):\n    axes[i][0].get_yaxis().set_visible(True)\n    axes[i][0].set_ylabel(\"count\")\nfig.tight_layout()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#example-bike-sharing","position":33},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Feature selection: overview","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#feature-selection-overview","position":34},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Feature selection: overview","lvl2":"Automatic Feature Selection"},"content":"Basic techniques (see the data processing recap)\n\nUnsupervised selection (e.g., variance and covariance-based techniques)\n\nUnivariate statistics (e.g., F-test and Mutual Information)\n\nWe’ll focus on the most powerful techniques:\n\nModel-based: Random Forests, Linear models, kNN\n\nWrapping techniques (black-box search)\n\nPermutation importance\n\nfrom sklearn.feature_selection import f_regression, SelectPercentile, mutual_info_regression, SelectFromModel, RFE\nfrom sklearn.preprocessing import scale\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom sklearn.inspection import permutation_importance\nfrom tqdm.notebook import trange, tqdm\n\n# Pre-compute all importances on bike sharing dataset\n# Scaled feature selection thresholds\nthresholds = [0.25, 0.5, 0.75, 1]\n# Dict to store all data\nfs = {}\nmethods = ['FTest','MutualInformation','RandomForest','Ridge','Lasso','RFE',\n           'ForwardSelection','FloatingForwardSelection','Permutation']\nfor m in methods:\n    fs[m] = {}\n    fs[m]['select'] = {}\n    fs[m]['cv_score'] = {}\n\ndef cv_score(selector):\n    model = RandomForestRegressor() #RidgeCV() #RandomForestRegressor(n_estimators=100) \n    select_pipe = make_pipeline(StandardScaler(), selector, model)    \n    return np.mean(cross_val_score(select_pipe, X_bike, y_bike, cv=3))\n\n# Tuned RF (pre-computed to save time)\n# param_grid = {\"max_features\":[2,4,8,16],\"max_depth\":[8,16,32,64,128]}\n# randomforestCV = GridSearchCV(RandomForestRegressor(n_estimators=200),\n#                              param_grid=param_grid).fit(X_bike, y_bike).best_estimator_\nrandomforestCV = RandomForestRegressor(n_estimators=200,max_features=16,max_depth=128)\n\n# F test\nprint(\"Computing F test\")\nfs['FTest']['label'] = \"F test\"\nfs['FTest']['score'] = f_regression(scale(X_bike),y_bike)[0]\nfs['FTest']['scaled_score'] = fs['FTest']['score'] / np.max(fs['FTest']['score'])\nfor t in tqdm(thresholds):\n    selector = SelectPercentile(score_func=f_regression, percentile=t*100).fit(scale(X_bike), y_bike)\n    fs['FTest']['select'][t] = selector.get_support()\n    fs['FTest']['cv_score'][t] = cv_score(selector)\n\n# Mutual information\nprint(\"Computing Mutual information\")\nfs['MutualInformation']['label'] = \"Mutual Information\"\nfs['MutualInformation']['score'] = mutual_info_regression(scale(X_bike),y_bike,discrete_features=range(13)) # first 13 features are discrete\nfs['MutualInformation']['scaled_score'] = fs['MutualInformation']['score'] / np.max(fs['MutualInformation']['score'])\nfor t in tqdm(thresholds):\n    selector = SelectPercentile(score_func=mutual_info_regression, percentile=t*100).fit(scale(X_bike), y_bike)\n    fs['MutualInformation']['select'][t] = selector.get_support()\n    fs['MutualInformation']['cv_score'][t] = cv_score(selector)\n    \n# Random Forest\nprint(\"Computing Random Forest\")\nfs['RandomForest']['label'] = \"Random Forest\"\nfs['RandomForest']['score'] = randomforestCV.fit(X_bike, y_bike).feature_importances_\nfs['RandomForest']['scaled_score'] = fs['RandomForest']['score'] / np.max(fs['RandomForest']['score'])\nfor t in tqdm(thresholds):\n    selector = SelectFromModel(randomforestCV, threshold=\"{}*mean\".format((1-t))).fit(X_bike, y_bike) # Threshold can't be easily scaled here\n    fs['RandomForest']['select'][t] = selector.get_support()\n    fs['RandomForest']['cv_score'][t] = cv_score(selector)\n    \n# Ridge, Lasso\nfor m in [RidgeCV(),LassoCV()]:\n    name = m.__class__.__name__.replace('CV','')\n    print(\"Computing\", name)\n    fs[name]['label'] = name\n    fs[name]['score'] = m.fit(X_bike, y_bike).coef_\n    fs[name]['scaled_score'] = np.abs(fs[name]['score']) / np.max(np.abs(fs[name]['score'])) # Use absolute values\n    for t in tqdm(thresholds):\n        selector = SelectFromModel(m, threshold=\"{}*mean\".format((1-t)*2)).fit(scale(X_bike), y_bike)\n        fs[name]['select'][t] = selector.get_support()\n        fs[name]['cv_score'][t] = cv_score(selector)\n        \n# Recursive Feature Elimination\nprint(\"Computing RFE\")\nfs['RFE']['label'] = \"Recursive Feature Elimination (with RandomForest)\"\nfs['RFE']['score'] = RFE(RandomForestRegressor(), n_features_to_select=1).fit(X_bike, y_bike).ranking_\nfs['RFE']['scaled_score'] = (20 - fs['RFE']['score'])/ 19\nfor t in tqdm(thresholds):\n    selector = RFE(RandomForestRegressor(), n_features_to_select=int(t*20)).fit(X_bike, y_bike)\n    fs['RFE']['select'][t] = selector.support_\n    fs['RFE']['cv_score'][t] = cv_score(selector)\n\n# Sequential Feature Selection\nprint(\"Computing Forward selection\")\nfor floating in [False]: # Doing only non-floating to speed up computations\n    name = \"{}ForwardSelection\".format(\"Floating\" if floating else \"\")\n    fs[name]['label'] = \"{} (with Ridge)\".format(name)\n    fs[name]['scaled_score'] = np.ones(20) # There is no scoring here\n    for t in tqdm(thresholds):\n        selector = SequentialFeatureSelector(RidgeCV(), k_features=int(t*20), forward=True, floating=floating).fit(X_bike, y_bike)\n        fs[name]['select'][t] = np.array([x in selector.k_feature_idx_ for x in range(20)])\n        fs[name]['cv_score'][t] = cv_score(selector)\n        \n# Permutation Importance  \nprint(\"Computing Permutation importance\")\nfs['Permutation']['label'] = \"Permutation importance (with RandomForest))\"\nfs['Permutation']['score'] = permutation_importance(RandomForestRegressor().fit(X_bike, y_bike), X_bike, y_bike, \n                                                    n_repeats=10, random_state=42, n_jobs=-1).importances_mean\nfs['Permutation']['scaled_score'] = fs['Permutation']['score'] / np.max(fs['Permutation']['score'])\nsorted_idx = (-fs['Permutation']['score']).argsort() # inverted sort\nfor t in tqdm(thresholds):\n    mask = np.array([x in sorted_idx[:int(t*20)] for x in range(20)])\n    fs['Permutation']['select'][t] = mask\n    # Hard to use this in a pipeline, resorting to transforming the data beforehand\n    fs['Permutation']['cv_score'][t] = np.mean(cross_val_score(RandomForestRegressor(), X_bike[:,mask], y_bike, cv=3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef plot_feature_importances(method1='f_test', method2=None, threshold=0.5):\n    \n    # Plot scores\n    x = np.arange(20)\n    fig, ax1 = plt.subplots(1, 1, figsize=(4*fig_scale, 1*fig_scale))\n    w = 0.3\n    imp = fs[method1]\n    mask = imp['select'][threshold]\n    m1 = ax1.bar(x[mask], imp['scaled_score'][mask], width=w, color='b', align='center')\n    ax1.bar(x[~mask], imp['scaled_score'][~mask], width=w, color='b', align='center', alpha=0.3)\n    if method2:\n        imp2 = fs[method2]\n        mask2 = imp2['select'][threshold]\n        ax2 = ax1.twinx()\n        m2 = ax2.bar(x[mask2] + w, imp2['scaled_score'][mask2], width=w,color='g',align='center')\n        ax2.bar(x[~mask2] + w, imp2['scaled_score'][~mask2], width=w,color='g',align='center', alpha=0.3)\n        plt.legend([m1, m2],['{} (Ridge R2:{:.2f})'.format(imp['label'],imp['cv_score'][threshold]),\n                             '{} (Ridge R2:{:.2f})'.format(imp2['label'],imp2['cv_score'][threshold])], loc='upper left')\n    else:\n        plt.legend([m1],['{} (Ridge R2:{:.2f})'.format(imp['label'],imp['cv_score'][threshold])], loc='upper left')\n    ax1.set_xticks(range(len(bike_names)))\n    ax1.set_xticklabels(bike_names, rotation=45, ha=\"right\");\n    plt.title(\"Feature importance (selection threshold {:.2f})\".format(threshold))\n                        \n    plt.show()\n\n\n\n@interact\ndef compare_feature_importances(method1=methods, method2=methods, threshold=(0.25,1,0.25)):\n    plot_feature_importances(method1,method2,threshold)\n    plt.show()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#feature-selection-overview","position":35},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Model-based Feature Selection","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#model-based-feature-selection","position":36},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Model-based Feature Selection","lvl2":"Automatic Feature Selection"},"content":"Use a \n\ntuned(!) supervised model to judge the importance of each feature\n\nLinear models (Ridge, Lasso, LinearSVM,...): features with highest weights (coefficients)\n\nTree–based models: features used in first nodes (high information gain)\n\nSelection model can be different from the one you use for final modelling\n\nCaptures interactions: features are more/less informative in combination (e.g. winter, temp)\n\nRandomForests: learns complex interactions (e.g. hour), but biased to high cardinality features\n\nplot_feature_importances('RandomForest', 'Lasso', threshold=0.75)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#model-based-feature-selection","position":37},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"Relief: Model-based selection with kNN","lvl3":"Model-based Feature Selection","lvl2":"Automatic Feature Selection"},"type":"lvl4","url":"/notebooks/data-preprocessing#relief-model-based-selection-with-knn","position":38},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"Relief: Model-based selection with kNN","lvl3":"Model-based Feature Selection","lvl2":"Automatic Feature Selection"},"content":"For I iterations, choose a random point \\mathbf{x_i} and find k nearest neighbors \\mathbf{x_{k}}\n\nIncrease feature weights if \\mathbf{x_i} and \\mathbf{x_{k}} have different class (near miss), else decrease\n\n\\mathbf{w_i} = \\mathbf{w_{i-1}} + (\\mathbf{x_i} - \\text{nearMiss}_i)^2 - (\\mathbf{x_i} - \\text{nearHit}_i)^2\n\nMany variants: ReliefF (uses L1 norm, faster), RReliefF (for regression), ...\n\n","type":"content","url":"/notebooks/data-preprocessing#relief-model-based-selection-with-knn","position":39},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"Iterative Model-based Feature Selection","lvl3":"Model-based Feature Selection","lvl2":"Automatic Feature Selection"},"type":"lvl4","url":"/notebooks/data-preprocessing#iterative-model-based-feature-selection","position":40},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl4":"Iterative Model-based Feature Selection","lvl3":"Model-based Feature Selection","lvl2":"Automatic Feature Selection"},"content":"Dropping many features at once is not ideal: feature importance may change in subset\n\nRecursive Feature Elimination (RFE)\n\nRemove s least important feature(s), recompute remaining importances, repeat\n\nCan be rather slow\n\nplot_feature_importances('RFE', 'RandomForest', threshold=0.5)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#iterative-model-based-feature-selection","position":41},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Sequential feature selection (Wrapping)","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#sequential-feature-selection-wrapping","position":42},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Sequential feature selection (Wrapping)","lvl2":"Automatic Feature Selection"},"content":"Evaluate your model with different sets of features, find best subset based on performance\n\nGreedy black-box search (can end up in local minima)\n\nBackward selection: remove least important feature, recompute importances, repeat\n\nForward selection: set aside most important feature, recompute importances, repeat\n\nFloating: add best new feature, remove worst one, repeat (forward or backward)\n\nStochastic search: use random mutations in candidate subset (e.g. simulated annealing)\n\nplot_feature_importances('ForwardSelection', 'Ridge', threshold=0.5)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#sequential-feature-selection-wrapping","position":43},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Permutation feature importance","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#permutation-feature-importance","position":44},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Permutation feature importance","lvl2":"Automatic Feature Selection"},"content":"Defined as the decrease in model performance when a single feature value is randomly shuffled\n\nThis breaks the relationship between the feature and the target\n\nModel agnostic, metric agnostic, and can be calculated many times with different permutations\n\nCan be applied to unseen data (not possible with model-based techniques)\n\nLess biased towards high-cardinality features (compared with RandomForests)\n\nplot_feature_importances('Permutation', 'RandomForest', threshold=0.5)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#permutation-feature-importance","position":45},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Comparison","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#comparison","position":46},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Comparison","lvl2":"Automatic Feature Selection"},"content":"Feature importances (scaled) and cross-validated R^2 score of pipeline\n\nPipeline contains features selection + Ridge\n\nSelection threshold value ranges from 25% to 100% of all features\n\nBest method ultimately depends on the problem and dataset at hand\n\n@interact\ndef compare_feature_importances(method1=methods, method2=methods, threshold=(0.25,1,0.25)):\n    plot_feature_importances(method1,method2,threshold)\n    plt.show()\n\n\n\nif not interactive:\n    plot_feature_importances('Permutation', 'FTest', threshold=0.5)\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#comparison","position":47},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (scikit-learn)","lvl2":"Automatic Feature Selection"},"type":"lvl3","url":"/notebooks/data-preprocessing#in-practice-scikit-learn-2","position":48},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (scikit-learn)","lvl2":"Automatic Feature Selection"},"content":"Unsupervised: VarianceTresholdselector = VarianceThreshold(threshold=0.01)\nX_selected = selector.fit_transform(X)\nvariances = selector.variances_\n\nUnivariate:\n\nFor regression: f_regression, mutual_info_regression\n\nFor classification: f_classification, chi2, mutual_info_classication\n\nSelecting: SelectKBest, SelectPercentile, SelectFpr,...selector = SelectPercentile(score_func=f_regression, percentile=50)\nX_selected = selector.fit_transform(X,y)\nselected_features = selector.get_support()\nf_values, p_values = f_regression(X,y)\nmi_values = mutual_info_regression(X,y,discrete_features=[])\n\nModel-based:\n\nSelectFromModel: requires a model and a selection threshold\n\nRFE, RFECV (recursive feature elimination): requires model and final nr featuresselector = SelectFromModel(RandomForestRegressor(), threshold='mean')\nrfe_selector = RFE(RidgeCV(), n_features_to_select=20)\nX_selected = selector.fit_transform(X)\nrf_importances = Randomforest().fit(X, y).feature_importances_\n\nSequential feature selection (from mlxtend, sklearn-compatible)selector = SequentialFeatureSelector(RidgeCV(), k_features=20, forward=True, \n                                     floating=True)\nX_selected = selector.fit_transform(X)\n\nPermutation Importance (in sklearn.inspection), no fit-transform interfaceimportances = permutation_importance(RandomForestRegressor().fit(X,y), \n                                     X, y, n_repeats=10).importances_mean\nfeature_ids = (-importances).argsort()[:n]\n\n","type":"content","url":"/notebooks/data-preprocessing#in-practice-scikit-learn-2","position":49},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Missing value imputation"},"type":"lvl2","url":"/notebooks/data-preprocessing#missing-value-imputation","position":50},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Missing value imputation"},"content":"Data can be missing in different ways:\n\nMissing Completely at Random (MCAR): purely random points are missing\n\nMissing at Random (MAR): something affects missingness, but no relation with the value\n\nE.g. faulty sensors, some people don’t fill out forms correctly\n\nMissing Not At Random (MNAR): systematic missingness linked to the value\n\nHas to be modelled or resolved (e.g. sensor decay, sick people leaving study)\n\nMissingness can be encoded in different ways:‘?’, ‘-1’, ‘unknown’, ‘NA’,...\n\nAlso labels can be missing (remove example or use semi-supervised learning)\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom fancyimpute import SoftImpute, IterativeSVD, MatrixFactorization\n\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nimport matplotlib.lines as mlines\n\n# Colors\nnorm = plt.Normalize()\ncolors = plt.cm.jet(norm([0,1,2]))\ncolors\n\n# Iris dataset with some added noise and missing values\ndef missing_iris():\n    iris = fetch_openml(\"iris\", version=1, return_X_y=True, as_frame=False)\n    X, y = iris\n    \n    # Make some noise\n    np.random.seed(0)\n    noise = np.random.normal(0, 0.1, 150)\n    for i in range(4):\n        X[:, i] = X[:, i] + noise\n    \n    # Add missing data. Set smallest leaf measurements to NaN\n    rng = np.random.RandomState(0)\n    mask = np.abs(X[:, 2] - rng.normal(loc=3.5, scale=.9, size=X.shape[0])) < 0.6\n    X[mask, 2] = np.NaN\n    mask2 = np.abs(X[:, 3] - rng.normal(loc=7.5, scale=.9, size=X.shape[0])) > 6.5\n    X[mask2, 3] = np.NaN\n    \n    label_encoder = LabelEncoder().fit(y)\n    y = label_encoder.transform(y)\n    return X, y\n\n# List of my favorite imputers\nimputers = {\"Mean Imputation\": SimpleImputer(strategy=\"mean\"), \n            \"kNN Imputation\": KNNImputer(), \n            \"Iterative Imputation (RandomForest)\": IterativeImputer(RandomForestRegressor()),\n            \"Iterative Imputation (BayesianRidge)\": IterativeImputer(BayesianRidge()),\n            \"SoftImpute\": SoftImpute(verbose=0),\n            \"Matrix Factorization\": MatrixFactorization(verbose=0)\n           }\n\n@ignore_warnings(category=ConvergenceWarning)\n@ignore_warnings(category=UserWarning)\ndef plot_imputation(imputer=imputers.keys()):\n    X, y = missing_iris()\n    imputed_mask = np.any(np.isnan(X), axis=1)\n    \n    X_imp = None\n    scores = None\n    imp = imputers[imputer]\n    if isinstance(imp, SimpleImputer) or isinstance(imp, KNNImputer) or isinstance(imp, IterativeImputer):\n        X_imp = imp.fit_transform(X)\n        imp_pipe = make_pipeline(SimpleImputer(), imp, LogisticRegression())\n        scores = cross_val_score(imp_pipe, X_imp, y)\n    else:\n        X_imp = imp.fit_transform(X)\n        scores = cross_val_score(LogisticRegression(), X_imp, y)\n    \n    fig, ax = plt.subplots(1, 1, figsize=(5*fig_scale, 3*fig_scale))\n    ax.set_title(\"{} (ACC:{:.3f})\".format(imputer, np.mean(scores)))\n    ax.scatter(X_imp[imputed_mask, 2], X_imp[imputed_mask, 3], c=y[imputed_mask], cmap='brg', alpha=.3, marker=\"s\", linewidths=0.25, s=4)\n    ax.scatter(X_imp[~imputed_mask, 2], X_imp[~imputed_mask, 3], c=y[~imputed_mask], cmap='brg', alpha=.6)\n    # this is for creating the legend...\n    square = plt.Line2D((0,), (0,), linestyle='', marker=\"s\", markerfacecolor=\"w\", markeredgecolor=\"k\", markeredgewidth=0.5, label='Imputed data')\n    circle = plt.Line2D((0,), (0,), linestyle='', marker=\"o\", markerfacecolor=\"w\", markeredgecolor=\"k\", markeredgewidth=0.5, label='Real data')\n    plt.legend(handles=[square, circle], numpoints=1, loc=\"best\")\n    plt.show()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#missing-value-imputation","position":51},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Overview","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#overview","position":52},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Overview","lvl2":"Missing value imputation"},"content":"Mean/constant imputation\n\nkNN-based imputation\n\nIterative (model-based) imputation\n\nMatrix Factorization techniques\n\nfrom ipywidgets import interact, interact_manual\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\n\n@interact\ndef compare_imputers(imputer=imputers.keys()):\n    plot_imputation(imputer=imputer)\n    plt.show()\n\n\n\nif not interactive:\n    plot_imputation(\"kNN Imputation\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#overview","position":53},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Mean imputation","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#mean-imputation","position":54},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Mean imputation","lvl2":"Missing value imputation"},"content":"Replace all missing values of a feature by the same value\n\nNumerical features: mean or median\n\nCategorical features: most frequent category\n\nConstant value, e.g. 0 or ‘missing’ for text features\n\nOptional: add an indicator column for missingness\n\nExample: Iris dataset (randomly removed values in 3rd and 4th column)\n\nplot_imputation(\"Mean Imputation\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#mean-imputation","position":55},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"kNN imputation","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#knn-imputation","position":56},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"kNN imputation","lvl2":"Missing value imputation"},"content":"Use special version of kNN to predict value of missing points\n\nUses only non-missing data when computing distances\n\nplot_imputation(\"kNN Imputation\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#knn-imputation","position":57},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Iterative (model-based) Imputation","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#iterative-model-based-imputation","position":58},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Iterative (model-based) Imputation","lvl2":"Missing value imputation"},"content":"Better known as Multiple Imputation by Chained Equations (MICE)\n\nIterative approach\n\nDo first imputation (e.g. mean imputation)\n\nTrain model (e.g. RandomForest) to predict missing values of a given feature\n\nTrain new model on imputed data to predict missing values of the next feature\n\nRepeat m times in round-robin fashion, leave one feature out at a time\n\nplot_imputation(\"Iterative Imputation (RandomForest)\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#iterative-model-based-imputation","position":59},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Matrix Factorization","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#matrix-factorization","position":60},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Matrix Factorization","lvl2":"Missing value imputation"},"content":"Basic idea: low-rank approximation\n\nReplace missing values by 0\n\nFactorize \\mathbf{X} with rank r: \\mathbf{X}^{n\\times p}=\\mathbf{U}^{n\\times r} \\mathbf{V}^{r\\times p}\n\nWith n data points and p features\n\nSolved using gradient descent\n\nRecompute \\mathbf{X}: now complete\n\nplot_imputation(\"Matrix Factorization\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#matrix-factorization","position":61},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Soft-thresholded Singular Value Decomposition (SVD)","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#soft-thresholded-singular-value-decomposition-svd","position":62},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Soft-thresholded Singular Value Decomposition (SVD)","lvl2":"Missing value imputation"},"content":"Same basic idea, but smoother\n\nReplace missing values by 0, compute SVD: \\mathbf{X}=\\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V^{T}}\n\nSolved with gradient descent\n\nReduce eigenvalues by shrinkage factor: \\lambda_i = s\\cdot\\lambda_i\n\nRecompute \\mathbf{X}: now complete\n\nRepeat for m iterations\n\nplot_imputation(\"SoftImpute\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#soft-thresholded-singular-value-decomposition-svd","position":63},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Comparison","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#comparison-1","position":64},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Comparison","lvl2":"Missing value imputation"},"content":"Best method depends on the problem and dataset at hand. Use cross-validation.\n\nIterative Imputation (MICE) generally works well for missing (completely) at random data\n\nCan be slow if the prediction model is slow\n\nLow-rank approximation techniques scale well to large datasets\n\n@interact\ndef compare_imputers(imputer=imputers.keys()):\n    plot_imputation(imputer=imputer)\n    plt.show()\n\n\n\nif not interactive:\n    plot_imputation(\"Iterative Imputation (RandomForest)\")\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#comparison-1","position":65},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (scikit-learn)","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#in-practice-scikit-learn-3","position":66},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (scikit-learn)","lvl2":"Missing value imputation"},"content":"Simple replacement: SimpleImputer\n\nStrategies: mean (numeric), median, most_frequent (categorical)\n\nChoose whether to add indicator columns, and how missing values are encodedimp = SimpleImputer(strategy='mean', missing_values=np.nan, add_indicator=False)\nX_complete = imp.fit_transform(X_train)\n\nkNN Imputation: KNNImputerimp = KNNImputer(n_neighbors=5)\nX_complete = imp.fit_transform(X_train)\n\nMultiple Imputation (MICE): IterativeImputer\n\nChoose estimator (default: BayesianRidge) and number of iterations (default 10)imp = IterativeImputer(estimator=RandomForestClassifier(), max_iter=10)\nX_complete = imp.fit_transform(X_train)\n\n","type":"content","url":"/notebooks/data-preprocessing#in-practice-scikit-learn-3","position":67},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (fancyimpute)","lvl2":"Missing value imputation"},"type":"lvl3","url":"/notebooks/data-preprocessing#in-practice-fancyimpute","position":68},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (fancyimpute)","lvl2":"Missing value imputation"},"content":"Cannot be used in CV pipelines (has fit_transform but no transform)\n\nSoft-Thresholded SVD: SoftImpute\n\nChoose max number of gradient descent iterations\n\nChoose shrinkage value for eigenvectors (default: \\frac{1}{N})imp = SoftImpute(max_iter=10, shrinkage_value=None)\nX_complete = imp.fit_transform(X)\n\nLow-rank imputation: MatrixFactorization\n\nChoose rank of the low-rank approximation\n\nGradient descent hyperparameters: learning rate, epochs,...\n\nSeveral variants existimp = MatrixFactorization(rank=10, learning_rate=0.001, epochs=10000)\nX_complete = imp.fit_transform(X)\n\n","type":"content","url":"/notebooks/data-preprocessing#in-practice-fancyimpute","position":69},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Handling imbalanced data"},"type":"lvl2","url":"/notebooks/data-preprocessing#handling-imbalanced-data","position":70},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Handling imbalanced data"},"content":"Problem:\n\nYou have a majority class with many times the number of examples as the minority class\n\nOr: classes are balanced, but associated costs are not (e.g. FN are worse than FP)\n\nWe already covered some ways to resolve this:\n\nAdd class weights to the loss function: give the minority class more weight\n\nIn practice: set class_weight='balanced'\n\nChange the prediction threshold to minimize false negatives or false positives\n\nThere are also things we can do by preprocessing the data\n\nResample the data to correct the imbalance\n\nRandom or model-based\n\nGenerate synthetic samples for the minority class\n\nBuild ensembles over different resampled datasets\n\nCombinations of these\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, CondensedNearestNeighbour\nfrom imblearn.ensemble import EasyEnsembleClassifier, BalancedBaggingClassifier\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.pipeline import make_pipeline as make_imb_pipeline # avoid confusion\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Some imbalanced data\nrng = np.random.RandomState(0)\nn_samples_1 = 500\nn_samples_2 = 50\nX_syn = np.r_[1.5 * rng.randn(n_samples_1, 2),\n        0.5 * rng.randn(n_samples_2, 2) + [1, 1]]\ny_syn = np.array([0] * (n_samples_1) + [1] * (n_samples_2))\nX_syn, y_syn = shuffle(X_syn, y_syn)\nX_syn_train, X_syn_test, y_syn_train, y_syn_test = train_test_split(X_syn, y_syn)\nX0min, X0max, X1min, X1max = np.min(X_syn[:,0]), np.max(X_syn[:,0]), np.min(X_syn[:,1]), np.max(X_syn[:,1])\n\n#samplers = [RandomUnderSampler(),EditedNearestNeighbours(),CondensedNearestNeighbour(),RandomOverSampler(),\n#           SMOTE(), ADASYN(), EasyEnsembleClassifier(), BalancedBaggingClassifier(), SMOTEENN()]\nsamplers = [RandomUnderSampler(),EditedNearestNeighbours(),RandomOverSampler(),\n           SMOTE(), ADASYN(), EasyEnsembleClassifier(), BalancedBaggingClassifier(), SMOTEENN()]\nlearners = [LogisticRegression(), SVC(), RandomForestClassifier()]\n\ndef plot_imbalance(sampler=RandomUnderSampler(), sampler2=None, learner=LogisticRegression()):\n    \n    # Appends multiple undersamplings for plotting purposes\n    def simulate_bagging(sampler,X_syn, y_syn):\n        X_resampled, y_resampled = sampler.fit_resample(X_syn, y_syn) \n        for i in range(10):\n            X_resampled_i, y_resampled_i = sampler.fit_resample(X_syn, y_syn)\n            X_resampled = np.append(X_resampled,X_resampled_i, axis=0)\n            y_resampled = np.append(y_resampled,y_resampled_i, axis=0)\n        return X_resampled, y_resampled\n    \n    def build_evaluate(sampler):\n        # Build new data\n        X_resampled, y_resampled = X_syn, y_syn\n        if isinstance(sampler,EasyEnsembleClassifier):\n            X_resampled, y_resampled = simulate_bagging(RandomUnderSampler(),X_syn, y_syn)\n        elif isinstance(sampler,BalancedBaggingClassifier):\n            balancer = RandomUnderSampler(sampling_strategy='all', replacement=True)\n            X_resampled, y_resampled = simulate_bagging(balancer,X_syn, y_syn)\n        else:\n            X_resampled, y_resampled = sampler.fit_resample(X_syn, y_syn)\n\n        # Evaluate\n        if isinstance(sampler,EasyEnsembleClassifier):\n            pipe = EasyEnsembleClassifier(estimator=learner)\n        elif isinstance(sampler,BalancedBaggingClassifier):\n            pipe = BalancedBaggingClassifier(estimator=learner)\n        else:\n            pipe = make_imb_pipeline(sampler, learner)\n        scores = cross_validate(pipe, X_resampled, y_resampled, scoring='roc_auc')['test_score']\n        return X_resampled, y_resampled, scores\n    \n    orig_scores = cross_validate(LogisticRegression(), X_syn, y_syn, scoring='roc_auc')['test_score']\n        \n    # Plot\n    nr_plots = 2 if sampler2 is None else 3\n    fig, axes = plt.subplots(1, nr_plots, figsize=(nr_plots*2*fig_scale, 2*fig_scale), subplot_kw={'xticks':(), 'yticks':()})\n    axes[0].set_title(\"Original (AUC: {:.3f})\".format(np.mean(orig_scores)), fontsize=5)\n    axes[0].scatter(X_syn[:, 0], X_syn[:, 1], c=plt.cm.tab10(y_syn), alpha=.3)\n \n    X_resampled, y_resampled, scores = build_evaluate(sampler)\n    axes[1].set_title(\"{} (AUC: {:.3f})\".format(sampler.__class__.__name__, np.mean(scores)), fontsize=5);\n    axes[1].scatter(X_resampled[:, 0], X_resampled[:, 1], c=plt.cm.tab10(y_resampled), alpha=.3)\n    plt.setp(axes[1], xlim=(X0min, X0max), ylim=(X1min, X1max))\n    \n    if sampler2 is not None:\n        X_resampled, y_resampled, scores = build_evaluate(sampler2)\n        axes[2].set_title(\"{} (AUC: {:.3f})\".format(sampler2.__class__.__name__, np.mean(scores)), fontsize=5);\n        axes[2].scatter(X_resampled[:, 0], X_resampled[:, 1], c=plt.cm.tab10(y_resampled), alpha=.3)\n        plt.setp(axes[2], xlim=(X0min, X0max), ylim=(X1min, X1max))\n        \n    plt.tight_layout()\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#handling-imbalanced-data","position":71},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Random Undersampling","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#random-undersampling","position":72},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Random Undersampling","lvl2":"Handling imbalanced data"},"content":"Copy the points from the minority class\n\nRandomly sample from the majority class (with or without replacement) until balanced\n\nOptionally, sample until a certain imbalance ratio (e.g. 1/5) is reached\n\nMulti-class: repeat with every other class\n\nPreferred for large datasets, often yields smaller/faster models with similar performance\n\nplot_imbalance(RandomUnderSampler())\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#random-undersampling","position":73},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Model-based Undersampling","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#model-based-undersampling","position":74},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Model-based Undersampling","lvl2":"Handling imbalanced data"},"content":"Edited Nearest Neighbors\n\nRemove all majority samples that are misclassified by kNN (mode) or that have a neighbor from the other class (all).\n\nRemove their influence on the minority samples\n\nCondensed Nearest Neighbors\n\nRemove all majority samples that are not misclassified by kNN\n\nFocus on only the hard samples\n\nplot_imbalance(EditedNearestNeighbours(), CondensedNearestNeighbour())\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#model-based-undersampling","position":75},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Random Oversampling","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#random-oversampling","position":76},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Random Oversampling","lvl2":"Handling imbalanced data"},"content":"Copy the points from the majority class\n\nRandomly sample from the minority class, with replacement, until balanced\n\nOptionally, sample until a certain imbalance ratio (e.g. 1/5) is reached\n\nMakes models more expensive to train, doens’t always improve performance\n\nSimilar to giving minority class(es) a higher weight (and more expensive)\n\nplot_imbalance(RandomOverSampler())\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#random-oversampling","position":77},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Synthetic Minority Oversampling Technique (SMOTE)","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#synthetic-minority-oversampling-technique-smote","position":78},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Synthetic Minority Oversampling Technique (SMOTE)","lvl2":"Handling imbalanced data"},"content":"Repeatedly choose a random minority point and a neighboring minority point\n\nPick a new, artificial point on the line between them (uniformly)\n\nMay bias the data. Be careful never to create artificial points in the test set.\n\nADASYN (Adaptive Synthetic)\n\nSimilar, but starts from ‘hard’ minority points (misclassified by kNN)\n\nplot_imbalance(SMOTE(), ADASYN())\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#synthetic-minority-oversampling-technique-smote","position":79},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Combined techniques","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#combined-techniques","position":80},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Combined techniques","lvl2":"Handling imbalanced data"},"content":"Combines over- and under-sampling\n\nE.g. oversampling with SMOTE, undersampling with Edited Nearest Neighbors (ENN)\n\nSMOTE can generate ‘noisy’ point, close to majority class points\n\nENN will remove up these majority points to ‘clean up’ the space\n\nplot_imbalance(SMOTEENN())\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#combined-techniques","position":81},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Ensemble Resampling","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#ensemble-resampling","position":82},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Ensemble Resampling","lvl2":"Handling imbalanced data"},"content":"Bagged ensemble of balanced base learners. Acts as a learner, not a preprocessor\n\nBalancedBagging: take bootstraps, randomly undersample each, train models (e.g. trees)\n\nBenefits of random undersampling without throwing out so much data\n\nEasy Ensemble: take multiple random undersamplings directly, train models\n\nTraditionally uses AdaBoost as base learner, but can be replaced\n\nplot_imbalance(EasyEnsembleClassifier(), BalancedBaggingClassifier()) \n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#ensemble-resampling","position":83},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Comparison","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#comparison-2","position":84},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Comparison","lvl2":"Handling imbalanced data"},"content":"The best method depends on the data (amount of data, imbalance,...)\n\nFor a very large dataset, random undersampling may be fine\n\nYou still need to choose the appropriate learning algorithms\n\nDon’t forget about class weighting and prediction thresholding\n\nSome combinations are useful, e.g. SMOTE + class weighting + thresholding\n\n@interact\ndef compare_imbalance(sampler=samplers, learner=learners):\n    plot_imbalance(sampler, None, learner)\n    plt.show()\n\n\n\nif not interactive:\n    plot_imbalance(EasyEnsembleClassifier())\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#comparison-2","position":85},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (imblearn)","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#in-practice-imblearn","position":86},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"In practice (imblearn)","lvl2":"Handling imbalanced data"},"content":"Follows fit-sample paradigm (equivalent of fit-transform, but also affects y)\n\nUndersampling: RandomUnderSampler, EditedNearestNeighbours,...\n\n(Synthetic) Oversampling: RandomOverSampler, SMOTE, ADASYN,...\n\nCombinations: SMOTEENN,...X_resampled, y_resampled = SMOTE(k_neighbors=5).fit_sample(X, y)\n\nCan be used in imblearn pipelines (not sklearn pipelines)\n\nimblearn pipelines are compatible with GridSearchCV,...\n\nSampling is only done in fit (not in predict)smote_pipe = make_pipeline(SMOTE(), LogisticRegression())\nscores = cross_validate(smote_pipe, X_train, y_train)\nparam_grid = {\"k_neighbors\": [3,5,7]}\ngrid = GridSearchCV(smote_pipe, param_grid=param_grid, X, y)\n\nThe ensembling techniques should be used as wrappersclf = EasyEnsembleClassifier(estimator=SVC()).fit(X_train, y_train)\n\n","type":"content","url":"/notebooks/data-preprocessing#in-practice-imblearn","position":87},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Real-world data","lvl2":"Handling imbalanced data"},"type":"lvl3","url":"/notebooks/data-preprocessing#real-world-data","position":88},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl3":"Real-world data","lvl2":"Handling imbalanced data"},"content":"The effect of sampling procedures can be unpredictable\n\nBest method can depend on the data and FP/FN trade-offs\n\nSMOTE and ensembling techniques often work well\n\nfrom sklearn.metrics import roc_curve, precision_recall_curve\n\n# Datasets\n# Try more if you like: ['Speech' ,'mc1','mammography']\n# Only showing Speech by default to speed up compute\ndatasets = ['Speech','mc1','mammography'] \n    \nroc_curves = {}\n\nfor dataset in datasets:\n    print(\"Dataset\", dataset)\n    roc_curves[dataset] = {}\n    \n    # Get data\n    data_imb = fetch_openml(dataset)\n    X_imb, y_imb = data_imb.data, data_imb.target\n    y_imb = LabelEncoder().fit_transform(y_imb)\n    X_imb_train, X_imb_test, y_imb_train, y_imb_test = train_test_split(X_imb, y_imb, stratify=y_imb, random_state=0)\n\n    # Original data\n    rf = RandomForestClassifier().fit(X_imb_train, y_imb_train)\n    probs_original = rf.predict_proba(X_imb_test)[:, 1]\n    fpr_org, tpr_org, _ = roc_curve(y_imb_test, probs_original)\n    precision, recall, _ = precision_recall_curve(y_imb_test, probs_original)\n    roc_curves[dataset][\"original\"] = {}\n    roc_curves[dataset][\"original\"][\"fpr\"] = fpr_org\n    roc_curves[dataset][\"original\"][\"tpr\"] = tpr_org\n    roc_curves[dataset][\"original\"][\"precision\"] = precision\n    roc_curves[dataset][\"original\"][\"recall\"] = recall\n\n    # Corrected data\n    for i, sampler in enumerate(samplers):\n        sname = sampler.__class__.__name__\n        print(\"Evaluating\", sname)\n        # Evaluate\n        learner = RandomForestClassifier()\n        if isinstance(sampler,EasyEnsembleClassifier):\n            pipe = EasyEnsembleClassifier(estimator=learner)\n        elif isinstance(sampler,BalancedBaggingClassifier):\n            pipe = BalancedBaggingClassifier(estimator=learner)\n        else:\n            pipe = make_imb_pipeline(sampler, learner)\n\n        pipe.fit(X_imb_train, y_imb_train)\n        probs = pipe.predict_proba(X_imb_test)[:, 1]\n        fpr, tpr, _ = roc_curve(y_imb_test, probs)\n        precision, recall, _ = precision_recall_curve(y_imb_test, probs)\n        roc_curves[dataset][sname] = {}\n        roc_curves[dataset][sname][\"id\"] = i\n        roc_curves[dataset][sname][\"fpr\"] = fpr\n        roc_curves[dataset][sname][\"tpr\"] = tpr\n        roc_curves[dataset][sname][\"precision\"] = precision\n        roc_curves[dataset][sname][\"recall\"] = recall\n\n\n\n# Colors for 9 methods\ncm = plt.get_cmap('hsv')\nroccol = []\nfor i in range(1,10):\n    roccol.append(cm(1.*i/9))\n    \ncurves = ['ROC','Precision-Recall']\n\n@interact\ndef roc_imbalance(dataset=datasets, curve=curves):\n    fig, ax = plt.subplots(figsize=(5*fig_scale, 2*fig_scale))\n\n    # Add to plot\n    curvy = roc_curves[dataset][\"original\"]\n    if curve == 'ROC':\n        ax.plot(curvy[\"fpr\"], curvy[\"tpr\"], label=\"Original\", lw=2, linestyle='-', c='k', alpha=.7)\n    else:\n        ax.plot(curvy[\"recall\"], curvy[\"precision\"], label=\"Original\", lw=2, linestyle='-', c='k', alpha=.7)\n    \n    for method in roc_curves[dataset].keys():\n        if method != \"original\":\n            curvy = roc_curves[dataset][method]\n            if curve == 'ROC':\n                ax.plot(curvy[\"fpr\"], curvy[\"tpr\"], label=method, lw=2, linestyle='-', c=roccol[curvy[\"id\"]-1], alpha=.7)\n            else:\n                ax.plot(curvy[\"recall\"], curvy[\"precision\"], label=method, lw=2, linestyle='-', c=roccol[curvy[\"id\"]-1], alpha=.7)\n        \n    if curve == 'ROC':\n        ax.set_xlabel(\"FPR\")\n        ax.set_ylabel(\"TPR\")\n        ax.set_title(\"RandomForest ROC curve on {} dataset\".format(dataset))\n        ax.legend(ncol=2, loc=\"lower right\")\n    else:\n        ax.set_xlabel(\"Recall\")\n        ax.set_ylabel(\"Precision\")\n        ax.set_title(\"RandomForest PR curve on {} dataset\".format(dataset))\n        ax.legend(ncol=2, loc=\"lower left\")\n    plt.tight_layout()\n    plt.show()\n\n\n\nif not interactive:\n    roc_imbalance(dataset='Speech', curve='ROC')\n\n\n\n","type":"content","url":"/notebooks/data-preprocessing#real-world-data","position":89},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/data-preprocessing#summary","position":90},{"hierarchy":{"lvl1":"Lecture 5. Data preprocessing","lvl2":"Summary"},"content":"Data preprocessing is a crucial part of machine learning\n\nScaling is important for many distance-based methods (e.g. kNN, SVM, Neural Nets)\n\nCategorical encoding is necessary for numeric methods (or implementations)\n\nSelecting features can speed up models and reduce overfitting\n\nFeature engineering is often useful for linear models\n\nIt is often better to impute missing data than to remove data\n\nImbalanced datasets require extra care to build useful models\n\nPipelines allow us to encapsulate multiple steps in a convenient way\n\nAvoids data leakage, crucial for proper evaluation\n\nChoose the right preprocessing steps and models in your pipeline\n\nCross-validation helps, but the search space is huge\n\nSmarter techniques exist to automate this process (AutoML)","type":"content","url":"/notebooks/data-preprocessing#summary","position":91},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks"},"type":"lvl1","url":"/notebooks/neural-networks","position":0},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks"},"content":"How to train your neurons\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = False # Set to True for interactive plots \nif interactive:\n    fig_scale = 0.5\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.4\n    plt.rcParams.update(print_config)\n    \n#HTML('''<style>.reveal pre code {max-height: 1000px !important;}</style>''')\n\n\n\n","type":"content","url":"/notebooks/neural-networks","position":1},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/neural-networks#overview","position":2},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Overview"},"content":"Neural architectures\n\nTraining neural nets\n\nForward pass: Tensor operations\n\nBackward pass: Backpropagation\n\nNeural network design:\n\nActivation functions\n\nWeight initialization\n\nOptimizers\n\nNeural networks in practice\n\nModel selection\n\nEarly stopping\n\nMemorization capacity and information bottleneck\n\nL1/L2 regularization\n\nDropout\n\nBatch normalization\n\ndef draw_neural_net(ax, layer_sizes, draw_bias=False, labels=False, activation=False, sigmoid=False,\n                    weight_count=False, random_weights=False, show_activations=False, figsize=(4, 4)):\n    \"\"\"\n    Draws a dense neural net for educational purposes\n    Parameters:\n        ax: plot axis\n        layer_sizes: array with the sizes of every layer\n        draw_bias: whether to draw bias nodes\n        labels: whether to draw labels for the weights and nodes\n        activation: whether to show the activation function inside the nodes\n        sigmoid: whether the last activation function is a sigmoid\n        weight_count: whether to show the number of weights and biases\n        random_weights: whether to show random weights as colored lines\n        show_activations: whether to show a variable for the node activations\n        scale_ratio: ratio of the plot dimensions, e.g. 3/4\n    \"\"\"\n    figsize = (figsize[0]*fig_scale, figsize[1]*fig_scale)\n    left, right, bottom, top = 0.1, 0.89*figsize[0]/figsize[1], 0.1, 0.89\n    n_layers = len(layer_sizes)\n    v_spacing = (top - bottom)/float(max(layer_sizes))\n    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n    colors = ['greenyellow','cornflowerblue','lightcoral']\n    w_count, b_count = 0, 0\n    ax.set_xlim(0, figsize[0]/figsize[1])\n    ax.axis('off')\n    ax.set_aspect('equal')\n    txtargs = {\"fontsize\":12*fig_scale, \"verticalalignment\":'center', \"horizontalalignment\":'center', \"zorder\":5}\n    \n    # Draw biases by adding a node to every layer except the last one\n    if draw_bias:\n        layer_sizes = [x+1 for x in layer_sizes]\n        layer_sizes[-1] = layer_sizes[-1] - 1\n        \n    # Nodes\n    for n, layer_size in enumerate(layer_sizes):\n        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2. \n        node_size = v_spacing/len(layer_sizes) if activation and n!=0 else v_spacing/3.\n        if n==0:\n            color = colors[0]\n        elif n==len(layer_sizes)-1:\n            color = colors[2]\n        else:\n            color = colors[1]\n        for m in range(layer_size):\n            ax.add_artist(plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), radius=node_size,\n                                      color=color, ec='k', zorder=4, linewidth=fig_scale))\n            b_count += 1\n            nx, ny = n*h_spacing + left, layer_top - m*v_spacing\n            nsx, nsy = [n*h_spacing + left,n*h_spacing + left], [layer_top - m*v_spacing - 0.5*node_size*2,layer_top - m*v_spacing + 0.5*node_size*2]\n            if draw_bias and m==0 and n<len(layer_sizes)-1:\n                ax.text(nx, ny, r'$1$', **txtargs)\n            elif labels and n==0:\n                ax.text(n*h_spacing + left,layer_top + v_spacing/1.5, 'input', **txtargs)\n                ax.text(nx, ny, r'$x_{}$'.format(m), **txtargs)\n            elif labels and n==len(layer_sizes)-1:\n                if activation:\n                    if sigmoid:\n                        ax.text(n*h_spacing + left,layer_top - m*v_spacing, r\"$z \\;\\;\\; \\sigma$\", **txtargs)\n                    else:\n                        ax.text(n*h_spacing + left,layer_top - m*v_spacing, r\"$z_{} \\;\\; g$\".format(m), **txtargs)\n                    ax.add_artist(plt.Line2D(nsx, nsy, c='k', zorder=6))\n                    if show_activations:        \n                        ax.text(n*h_spacing + left + 1.5*node_size,layer_top - m*v_spacing, r\"$\\hat{y}$\", fontsize=12*fig_scale, \n                                verticalalignment='center', horizontalalignment='left', zorder=5, c='r')\n\n                else:\n                    ax.text(nx, ny, r'$o_{}$'.format(m), **txtargs)\n                ax.text(n*h_spacing + left,layer_top + v_spacing, 'output', **txtargs)\n            elif labels:\n                if activation:\n                    ax.text(n*h_spacing + left,layer_top - m*v_spacing, r\"$z_{} \\;\\; f$\".format(m), **txtargs)\n                    ax.add_artist(plt.Line2D(nsx, nsy, c='k', zorder=6))\n                    if show_activations:        \n                        ax.text(n*h_spacing + left + node_size*1.2 ,layer_top - m*v_spacing, r\"$a_{}$\".format(m), fontsize=12*fig_scale, \n                                verticalalignment='center', horizontalalignment='left', zorder=5, c='b')\n                else:\n                    ax.text(nx, ny, r'$h_{}$'.format(m), **txtargs)\n                \n            \n    # Edges\n    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n        for m in range(layer_size_a):\n            for o in range(layer_size_b):\n                if not (draw_bias and o==0 and len(layer_sizes)>2 and n<layer_size_b-1):\n                    xs = [n*h_spacing + left, (n + 1)*h_spacing + left]\n                    ys = [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing]\n                    color = 'k' if not random_weights else plt.cm.bwr(np.random.random())\n                    ax.add_artist(plt.Line2D(xs, ys, c=color, alpha=0.6))\n                    if not (draw_bias and m==0):\n                        w_count += 1\n                    if labels and not random_weights:\n                        wl = r'$w_{{{},{}}}$'.format(m,o) if layer_size_b>1 else r'$w_{}$'.format(m)\n                        ax.text(xs[0]+np.diff(xs)/2, np.mean(ys)-np.diff(ys)/9, wl, ha='center', va='center', \n                                 fontsize=12*fig_scale)\n    # Count\n    if weight_count:\n        b_count = b_count - layer_sizes[0]\n        if draw_bias:\n            b_count = b_count - (len(layer_sizes) - 2)\n        ax.text(right*1.05, bottom, \"{} weights, {} biases\".format(w_count, b_count), ha='center', va='center')\n\n\n\n\n","type":"content","url":"/notebooks/neural-networks#overview","position":3},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Architecture"},"type":"lvl2","url":"/notebooks/neural-networks#architecture","position":4},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Architecture"},"content":"Logistic regression, drawn in a different, neuro-inspired, way\n\nLinear model: inner product (z) of input vector \\mathbf{x} and weight vector \\mathbf{w}, plus bias w_0\n\nLogistic (or sigmoid) function maps the output to a probability in [0,1]\n\nUses log loss (cross-entropy) and gradient descent to learn the weights\\hat{y}(\\mathbf{x}) = \\text{sigmoid}(z) = \\text{sigmoid}(w_0 + \\mathbf{w}\\mathbf{x}) = \\text{sigmoid}(w_0 + w_1 * x_1 + w_2 * x_2 +... + w_p * x_p)\n\nfig = plt.figure(figsize=(3*fig_scale,3*fig_scale))\nax = fig.gca()\ndraw_neural_net(ax, [4, 1], activation=True, draw_bias=True, labels=True, sigmoid=True)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#architecture","position":5},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Basic Architecture","lvl2":"Architecture"},"type":"lvl3","url":"/notebooks/neural-networks#basic-architecture","position":6},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Basic Architecture","lvl2":"Architecture"},"content":"Add one (or more) hidden layers h with k nodes (or units, cells, neurons)\n\nEvery ‘neuron’ is a tiny function, the network is an arbitrarily complex function\n\nWeights w_{i,j} between node i and node j form a weight matrix \\mathbf{W}^{(l)} per layer l\n\nEvery neuron weights the inputs \\mathbf{x} and passes it through a non-linear activation function\n\nActivation functions (f,g) can be different per layer, output \\mathbf{a} is called activation\n\\color{blue}{h(\\mathbf{x})} = \\color{blue}{\\mathbf{a}} = f(\\mathbf{z}) = f(\\mathbf{W}^{(1)} \\color{green}{\\mathbf{x}}+\\mathbf{w}^{(1)}_0) \\quad \\quad \\color{red}{o(\\mathbf{x})} = g(\\mathbf{W}^{(2)}  \\color{blue}{\\mathbf{a}}+\\mathbf{w}^{(2)}_0)\n\nfig, axes = plt.subplots(1,2, figsize=(10*fig_scale,5*fig_scale))\ndraw_neural_net(axes[0], [2, 3, 1],  draw_bias=True, labels=True, weight_count=True, figsize=(4, 4))\ndraw_neural_net(axes[1], [2, 3, 1],  activation=True, show_activations=True, draw_bias=True, \n                labels=True, weight_count=True,  figsize=(4, 4))\n\n\n\n","type":"content","url":"/notebooks/neural-networks#basic-architecture","position":7},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"More layers","lvl2":"Architecture"},"type":"lvl3","url":"/notebooks/neural-networks#more-layers","position":8},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"More layers","lvl2":"Architecture"},"content":"Add more layers, and more nodes per layer, to make the model more complex\n\nFor simplicity, we don’t draw the biases (but remember that they are there)\n\nIn dense (fully-connected) layers, every previous layer node is connected to all nodes\n\nThe output layer can also have multiple nodes (e.g. 1 per class in multi-class classification)\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n@interact\ndef plot_dense_net(nr_layers=(0,6,1), nr_nodes=(1,12,1)):\n    fig = plt.figure(figsize=(10*fig_scale, 5*fig_scale))\n    ax = fig.gca()\n    ax.axis('off')\n    hidden = [nr_nodes]*nr_layers\n    draw_neural_net(ax, [5] + hidden + [5], weight_count=True, figsize=(6, 4))\n    plt.show()\n\n\n\nif not interactive:\n    plot_dense_net(nr_layers=6, nr_nodes=10)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#more-layers","position":9},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Why layers?","lvl2":"Architecture"},"type":"lvl3","url":"/notebooks/neural-networks#why-layers","position":10},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Why layers?","lvl2":"Architecture"},"content":"Each layer acts as a filter and learns a new representation of the data\n\nSubsequent layers can learn iterative refinements\n\nEasier than learning a complex relationship in one go\n\nExample: for image input, each layer yields new (filtered) images\n\nCan learn multiple mappings at once: weight tensor \\mathit{W} yields activation tensor \\mathit{A}\n\nFrom low-level patterns (edges, end-points, ...) to combinations thereof\n\nEach neuron ‘lights up’ if certain patterns occur in the input\n\n","type":"content","url":"/notebooks/neural-networks#why-layers","position":11},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Other architectures","lvl2":"Architecture"},"type":"lvl3","url":"/notebooks/neural-networks#other-architectures","position":12},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Other architectures","lvl2":"Architecture"},"content":"There exist MANY types of networks for many different tasks\n\nConvolutional nets for image data, Recurrent nets for sequential data,...\n\nAlso used to learn representations (embeddings), generate new images, text,...\n\n","type":"content","url":"/notebooks/neural-networks#other-architectures","position":13},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Training Neural Nets"},"type":"lvl2","url":"/notebooks/neural-networks#training-neural-nets","position":14},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Training Neural Nets"},"content":"Design the architecture, choose activation functions (e.g. sigmoids)\n\nChoose a way to initialize the weights (e.g. random initialization)\n\nChoose a loss function (e.g. log loss) to measure how well the model fits training data\n\nChoose an optimizer (typically an SGD variant) to update the weights\n\n","type":"content","url":"/notebooks/neural-networks#training-neural-nets","position":15},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Mini-batch Stochastic Gradient Descent (recap)","lvl2":"Training Neural Nets"},"type":"lvl3","url":"/notebooks/neural-networks#mini-batch-stochastic-gradient-descent-recap","position":16},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Mini-batch Stochastic Gradient Descent (recap)","lvl2":"Training Neural Nets"},"content":"Draw a batch of batch_size training data \\mathbf{X} and \\mathbf{y}\n\nForward pass : pass \\mathbf{X} though the network to yield predictions \\mathbf{\\hat{y}}\n\nCompute the loss \\mathcal{L} (mismatch between  \\mathbf{\\hat{y}} and \\mathbf{y})\n\nBackward pass : Compute the gradient of the loss with regard to every weight\n\nBackpropagate the gradients through all the layers\n\nUpdate W: W_{(i+1)} = W_{(i)} - \\frac{\\partial L(x, W_{(i)})}{\\partial W} * \\eta\n\nRepeat until n passes (epochs) are made through the entire training set\n\n# TODO: show the actual weight updates\n@interact\ndef draw_updates(iteration=(1,100,1)):\n    fig, ax = plt.subplots(1, 1, figsize=(6*fig_scale, 4*fig_scale))\n    np.random.seed(iteration)\n    draw_neural_net(ax, [6,5,5,3], labels=True, random_weights=True, show_activations=True, figsize=(6, 4));\n    plt.show()\n\n\n\nif not interactive:\n    draw_updates(iteration=1)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#mini-batch-stochastic-gradient-descent-recap","position":17},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Forward pass","lvl2":"Training Neural Nets"},"type":"lvl3","url":"/notebooks/neural-networks#forward-pass","position":18},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Forward pass","lvl2":"Training Neural Nets"},"content":"We can naturally represent the data as tensors\n\nNumerical n-dimensional array (with n axes)\n\n2D tensor: matrix (samples, features)\n\n3D tensor: time series (samples, timesteps, features)\n\n4D tensor: color images (samples, height, width, channels)\n\n5D tensor: video (samples, frames, height, width, channels) \n\n","type":"content","url":"/notebooks/neural-networks#forward-pass","position":19},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Tensor operations","lvl3":"Forward pass","lvl2":"Training Neural Nets"},"type":"lvl4","url":"/notebooks/neural-networks#tensor-operations","position":20},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Tensor operations","lvl3":"Forward pass","lvl2":"Training Neural Nets"},"content":"The operations that the network performs on the data can be reduced to a series of tensor operations\n\nThese are also much easier to run on GPUs\n\nA dense layer with sigmoid activation, input tensor \\mathbf{X}, weight tensor \\mathbf{W}, bias \\mathbf{b}:y = sigmoid(np.dot(X, W) + b)\n\nTensor dot product for 2D inputs (a samples, b features, c hidden nodes)\n\n","type":"content","url":"/notebooks/neural-networks#tensor-operations","position":21},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Element-wise operations","lvl3":"Forward pass","lvl2":"Training Neural Nets"},"type":"lvl4","url":"/notebooks/neural-networks#element-wise-operations","position":22},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Element-wise operations","lvl3":"Forward pass","lvl2":"Training Neural Nets"},"content":"Activation functions and addition are element-wise operations:def sigmoid(x):\n  return 1/(1 + np.exp(-x)) \n\ndef add(x, y):\n  return x + y\n\nNote: if y has a lower dimension than x, it will be broadcasted: axes are added to match the dimensionality, and y is repeated along the new axes>>> np.array([[1,2],[3,4]]) + np.array([10,20])\narray([[11, 22],\n       [13, 24]])\n\n","type":"content","url":"/notebooks/neural-networks#element-wise-operations","position":23},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"type":"lvl3","url":"/notebooks/neural-networks#backward-pass-backpropagation","position":24},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"content":"For last layer, compute gradient of the loss function \\mathcal{L} w.r.t all weights of layer l\\nabla \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial W^{(l)}} = \n                  \\begin{bmatrix}\n                    \\frac{\\partial \\mathcal{L}}{\\partial w_{0,0}} & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_{0,l}} \\\\\n                    \\vdots & \\ddots & \\vdots \\\\\n                    \\frac{\\partial \\mathcal{L}}{\\partial w_{k,0}}  & \\ldots & \\frac{\\partial \\mathcal{L}}{\\partial w_{k,l}}\n                  \\end{bmatrix}\n\nSum up the gradients for all \\mathbf{x}_j in minibatch: \\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W^{(l)}}\n\nUpdate all weights in a layer at once (with learning rate \\eta): W_{(i+1)}^{(l)} = W_{(i)}^{(l)} - \\eta \\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W_{(i)}^{(l)}}\n\nRepeat for next layer, iterating backwards (most efficient, avoids redundant calculations)\n\n","type":"content","url":"/notebooks/neural-networks#backward-pass-backpropagation","position":25},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Example","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"type":"lvl4","url":"/notebooks/neural-networks#example","position":26},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Example","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"content":"Imagine feeding a single data point, output is \\hat{y} = g(z) = g(w_0 + w_1 * a_1 + w_2 * a_2 +... + w_p * a_p)\n\nDecrease loss by updating weights:\n\nUpdate the weights of last layer to maximize improvement:\nw_{i,(new)} = w_{i} - \\frac{\\partial \\mathcal{L}}{\\partial w_i} * \\eta\n\nTo compute gradient \\frac{\\partial \\mathcal{L}}{\\partial w_i} we need the chain rule: f(g(x)) = f'(g(x)) * g'(x)\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\color{red}{\\frac{\\partial \\mathcal{L}}{\\partial g}} \\color{blue}{\\frac{\\partial \\mathcal{g}}{\\partial z_0}} \\color{green}{\\frac{\\partial \\mathcal{z_0}}{\\partial w_i}}\n\nE.g., with \\mathcal{L} = \\frac{1}{2}(y-\\hat{y})^2 and sigmoid \\sigma: \\frac{\\partial \\mathcal{L}}{\\partial w_i} = \\color{red}{(y - \\hat{y})} * \\color{blue}{\\sigma'(z_0)} * \\color{green}{a_i}\n\nfig = plt.figure(figsize=(4*fig_scale, 3.5*fig_scale))\nax = fig.gca()\ndraw_neural_net(ax, [2, 3, 1],  activation=True, draw_bias=True, labels=True, \n                show_activations=True)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#example","position":27},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Backpropagation (2)","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"type":"lvl4","url":"/notebooks/neural-networks#backpropagation-2","position":28},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Backpropagation (2)","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"content":"Another way to decrease the loss \\mathcal{L} is to update the activations a_i\n\nTo update a_i = f(z_i), we need to update the weights of the previous layer\n\nWe want to nudge a_i in the right direction by updating w_{i,j}:\n\\frac{\\partial \\mathcal{L}}{\\partial w_{i,j}} = \\frac{\\partial \\mathcal{L}}{\\partial a_i} \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial \\mathcal{z_i}}{\\partial w_{i,j}} = \\left( \\frac{\\partial \\mathcal{L}}{\\partial g} \\frac{\\partial \\mathcal{g}}{\\partial z_0} \\frac{\\partial \\mathcal{z_0}}{\\partial a_i} \\right) \\frac{\\partial a_i}{\\partial z_i} \\frac{\\partial \\mathcal{z_i}}{\\partial w_{i,j}}\n\nWe know \\frac{\\partial \\mathcal{L}}{\\partial g} and \\frac{\\partial \\mathcal{g}}{\\partial z_0} from the previous step, \\frac{\\partial \\mathcal{z_0}}{\\partial a_i} = w_i, \\frac{\\partial a_i}{\\partial z_i} = f' and \\frac{\\partial \\mathcal{z_i}}{\\partial w_{i,j}} = x_j\n\nfig = plt.figure(figsize=(4*fig_scale, 4*fig_scale))\nax = fig.gca()\ndraw_neural_net(ax, [2, 3, 1],  activation=True, draw_bias=True, labels=True, \n                show_activations=True)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#backpropagation-2","position":29},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Backpropagation (3)","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"type":"lvl4","url":"/notebooks/neural-networks#backpropagation-3","position":30},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Backpropagation (3)","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"content":"With multiple output nodes, \\mathcal{L} is the sum of all per-output (per-class) losses\n\n\\frac{\\partial \\mathcal{L}}{\\partial a_i} is sum of the gradients for every output\n\nPer layer, sum up gradients for every point \\mathbf{x} in the batch: \\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W}\n\nUpdate all weights of every layer l\n\nW_{(i+1)}^{(l)} = W_{(i)}^{(l)} - \\eta \\sum_{j} \\frac{\\partial \\mathcal{L}(\\mathbf{x}_j,y_j)}{\\partial W_{(i)}^{(l)}}\n\nRepeat with a new batch of data until loss converges\n\nNice animation of the entire process\n\nfig = plt.figure(figsize=(8*fig_scale, 4*fig_scale))\nax = fig.gca()\ndraw_neural_net(ax, [2, 3, 3, 2],  activation=True, draw_bias=True, labels=True, \n                random_weights=True, show_activations=True, figsize=(8, 4))\n\n\n\n","type":"content","url":"/notebooks/neural-networks#backpropagation-3","position":31},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Summary","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"type":"lvl4","url":"/notebooks/neural-networks#summary","position":32},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Summary","lvl3":"Backward pass (backpropagation)","lvl2":"Training Neural Nets"},"content":"The network output a_o is defined by the weights W^{(o)} and biases \\mathbf{b}^{(o)} of the output layer, and\n\nThe activations of a hidden layer h_1 with activation function a_{h_1}, weights W^{(1)} and biases \\mathbf{b^{(1)}}:\\color{red}{a_o(\\mathbf{x})} = \\color{red}{a_o(\\mathbf{z_0})} = \\color{red}{a_o(W^{(o)}} \\color{blue}{a_{h_1}(z_{h_1})} \\color{red}{+ \\mathbf{b}^{(o)})} = \\color{red}{a_o(W^{(o)}} \\color{blue}{a_{h_1}(W^{(1)} \\color{green}{\\mathbf{x}} + \\mathbf{b}^{(1)})}\n  \\color{red}{+ \\mathbf{b}^{(o)})}\n\nMinimize the loss by SGD. For layer l, compute \\frac{\\partial \\mathcal{L}(a_o(x))}{\\partial W_l} and \\frac{\\partial \\mathcal{L}(a_o(x))}{\\partial b_{l,i}} using the chain rule\n\nDecomposes into \n\ngradient of layer above, \n\ngradient of activation function, \n\ngradient of layer input:\\frac{\\partial \\mathcal{L}(a_o)}{\\partial W^{(1)}} = \\color{red}{\\frac{\\partial \\mathcal{L}(a_o)}{\\partial a_{h_1}}} \\color{blue}{\\frac{\\partial a_{h_1}}{\\partial z_{h_1}}} \\color{green}{\\frac{\\partial z_{h_1}}{\\partial W^{(1)}}} \n= \\left( \\color{red}{\\frac{\\partial \\mathcal{L}(a_o)}{\\partial a_o}} \\color{blue}{\\frac{\\partial a_o}{\\partial z_o}} \\color{green}{\\frac{\\partial z_o}{\\partial a_{h_1}}}\\right) \\color{blue}{\\frac{\\partial a_{h_1}}{\\partial z_{h_1}}} \\color{green}{\\frac{\\partial z_{h_1}}{\\partial W^{(1)}}}\n\n","type":"content","url":"/notebooks/neural-networks#summary","position":33},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Activation functions for hidden layers"},"type":"lvl2","url":"/notebooks/neural-networks#activation-functions-for-hidden-layers","position":34},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Activation functions for hidden layers"},"content":"Sigmoid: f(z) = \\frac{1}{1+e^{-z}}\n\nTanh: f(z) = \\frac{2}{1+e^{-2z}} - 1\n\nActivations around 0 are better for gradient descent convergence\n\nRectified Linear (ReLU): f(z) = max(0,z)\n\nLess smooth, but much faster (note: not differentiable at 0)\n\nLeaky ReLU: f(z) = \\begin{cases} 0.01z & z<0 \\\\ z & otherwise \\end{cases}\n\ndef activation(X, function=\"sigmoid\"):     \n    if function == \"sigmoid\":      \n        return 1.0/(1.0 + np.exp(-X))    \n    if function == \"softmax\": \n        return np.exp(X) / np.sum(np.exp(X), axis=0)   \n    elif function == \"tanh\":      \n        return np.tanh(X)    \n    elif function == \"relu\":      \n        return np.maximum(0,X)    \n    elif function == \"leaky_relu\":      \n        return np.maximum(0.1*X,X)\n    elif function == \"none\":      \n        return X\n    \ndef activation_derivative(X, function=\"sigmoid\"):   \n    if function == \"sigmoid\": \n        sig = 1.0/(1.0 + np.exp(-X))   \n        return sig * (1 - sig)\n    elif function == \"tanh\":      \n        return 1 - np.tanh(X)**2   \n    elif function == \"relu\":      \n        return np.where(X > 0, 1, 0)\n    elif function == \"leaky_relu\":    \n        # Using 0.1 instead of 0.01 to make it visible in the plot\n        return np.where(X > 0, 1, 0.1)\n    elif function == \"none\":      \n        return X/X\n    \ndef plot_activation(function, ax, derivative=False):\n    if function==\"softmax\":       \n        x = np.linspace(-6,6,9)\n        ax.plot(x,activation(x, function),lw=2, c='b', linestyle='-', marker='o')\n    else:     \n        x = np.linspace(-6,6,101)\n        ax.plot(x,activation(x, function),lw=2, c='b', linestyle='-') \n        if derivative:\n            if function == \"relu\" or function == \"leaky_relu\":\n                ax.step(x,activation_derivative(x, function),lw=2, c='r', linestyle='-')\n            else:\n                ax.plot(x,activation_derivative(x, function),lw=2, c='r', linestyle='-')\n    ax.set_xlabel(\"input\", fontsize=16*fig_scale)\n    ax.set_ylabel(function, fontsize=18*fig_scale)\n    ax.tick_params(axis='both', labelsize=16*fig_scale)\n    ax.grid()\n    \nfunctions = [\"sigmoid\",\"tanh\",\"relu\",\"leaky_relu\"]\n\n@interact\ndef plot_activations(function=functions):\n    fig, ax = plt.subplots(figsize=(5,1.5))\n    plot_activation(function, ax)\n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(1,4, figsize=(10,2))\n    for function, ax in zip(functions,axes):\n        plot_activation(function, ax)\n    plt.tight_layout();\n\n\n\n","type":"content","url":"/notebooks/neural-networks#activation-functions-for-hidden-layers","position":35},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Effect of activation functions on the gradient","lvl2":"Activation functions for hidden layers"},"type":"lvl4","url":"/notebooks/neural-networks#effect-of-activation-functions-on-the-gradient","position":36},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Effect of activation functions on the gradient","lvl2":"Activation functions for hidden layers"},"content":"During gradient descent, the gradient depends on the activation function a_{h}: \\frac{\\partial \\mathcal{L}(a_o)}{\\partial W^{(l)}} = \\color{red}{\\frac{\\partial \\mathcal{L}(a_o)}{\\partial a_{h_l}}} \\color{blue}{\\frac{\\partial a_{h_l}}{\\partial z_{h_l}}} \\color{green}{\\frac{\\partial z_{h_l}}{\\partial W^{(l)}}}\n\nIf derivative of the activation function \\color{blue}{\\frac{\\partial a_{h_l}}{\\partial z_{h_l}}} is 0, the weights w_i are not updated\n\nMoreover, the gradients of previous layers will be reduced (vanishing gradient)\n\nsigmoid, tanh: gradient is very small for large inputs: slow updates\n\nWith ReLU, \\color{blue}{\\frac{\\partial a_{h_l}}{\\partial z_{h_l}}} = 1 if z>0, hence better against vanishing gradients\n\nProblem: for very negative inputs, the gradient is 0 and may never recover (dying ReLU)\n\nLeaky ReLU has a small (0.01) gradient there to allow recovery\n\n@interact\ndef plot_activations_derivative(function=functions):\n    fig, ax = plt.subplots(figsize=(6,2))\n    plot_activation(function, ax, derivative=True)\n    plt.legend(['original','derivative'], loc='upper center', \n               bbox_to_anchor=(0.5, 1.25), ncol=2)\n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(1,4, figsize=(10,2))\n    for function, ax in zip(functions,axes):\n        plot_activation(function, ax, derivative=True)\n    fig.legend(['original','derivative'], loc='upper center', \n               bbox_to_anchor=(0.5, 1.25), ncol=2)\n    plt.tight_layout();\n\n\n\n","type":"content","url":"/notebooks/neural-networks#effect-of-activation-functions-on-the-gradient","position":37},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"ReLU vs Tanh","lvl2":"Activation functions for hidden layers"},"type":"lvl4","url":"/notebooks/neural-networks#relu-vs-tanh","position":38},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"ReLU vs Tanh","lvl2":"Activation functions for hidden layers"},"content":"What is the effect of using non-smooth activation functions?\n\nReLU produces piecewise-linear boundaries, but allows deeper networks\n\nTanh produces smoother decision boundaries, but is slower\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom mglearn.plot_2d_separator import plot_2d_classification\nimport time\n\n@interact\ndef plot_boundary(nr_layers=(1,4,1)):\n    X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                        random_state=42)\n    \n    # Multi-Layer Perceptron with ReLU\n    mlp = MLPClassifier(solver='lbfgs', random_state=0,\n                        hidden_layer_sizes=[10]*nr_layers)\n    start = time.time()\n    mlp.fit(X_train, y_train)\n    relu_time = time.time() - start\n    relu_acc = mlp.score(X_test, y_test)\n\n    # Multi-Layer Perceptron with tanh\n    mlp_tanh = MLPClassifier(solver='lbfgs', activation='tanh',\n                             random_state=0, hidden_layer_sizes=[10]*nr_layers)\n    start = time.time()\n    mlp_tanh.fit(X_train, y_train)\n    tanh_time = time.time() - start\n    tanh_acc = mlp_tanh.score(X_test, y_test)\n\n    fig, axes = plt.subplots(1, 2, figsize=(8*fig_scale, 4*fig_scale))\n    axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', label=\"train\")\n    axes[0].set_title(\"ReLU, acc: {:.2f}, time: {:.2f} sec\".format(relu_acc, relu_time))\n    plot_2d_classification(mlp, X_train, fill=True, cm='bwr', alpha=.3, ax=axes[0])\n    axes[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', label=\"train\")\n    axes[1].set_title(\"tanh, acc: {:.2f}, time: {:.2f} sec\".format(tanh_acc, tanh_time))\n    plot_2d_classification(mlp_tanh, X_train, fill=True, cm='bwr', alpha=.3, ax=axes[1])\n    plt.show()\n\n\n\nif not interactive:\n    plot_boundary(nr_layers=2)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#relu-vs-tanh","position":39},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Activation functions for output layer","lvl2":"Activation functions for hidden layers"},"type":"lvl4","url":"/notebooks/neural-networks#activation-functions-for-output-layer","position":40},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Activation functions for output layer","lvl2":"Activation functions for hidden layers"},"content":"sigmoid converts output to probability in [0,1]\n\nFor binary classification\n\nsoftmax converts all outputs (aka ‘logits’) to probabilities that sum up to 1\n\nFor multi-class classification (k classes)\n\nCan cause over-confident models. If so, smooth the labels: y_{smooth} = (1-\\alpha)y + \\frac{\\alpha}{k}\n\\text{softmax}(\\mathbf{x},i) = \\frac{e^{x_i}}{\\sum_{j=1}^k e^{x_j}}\n\nFor regression, don’t use any activation function, let the model learn the exact target\n\noutput_functions = [\"sigmoid\",\"softmax\",\"none\"]\n\n@interact\ndef plot_output_activation(function=output_functions):\n    fig, ax = plt.subplots(figsize=(6,2))\n    plot_activation(function, ax)\n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(1,2, figsize=(8,2))\n    for function, ax in zip(output_functions[:2],axes):\n        plot_activation(function, ax)\n    plt.tight_layout();\n\n\n\n","type":"content","url":"/notebooks/neural-networks#activation-functions-for-output-layer","position":41},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Weight initialization"},"type":"lvl2","url":"/notebooks/neural-networks#weight-initialization","position":42},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Weight initialization"},"content":"Initializing weights to 0 is bad: all gradients in layer will be identical (symmetry)\n\nToo small random weights shrink activations to 0 along the layers (vanishing gradient)\n\nToo large random weights multiply along layers (exploding gradient, zig-zagging)\n\nIdeal: small random weights + variance of input and output gradients remains the same\n\nGlorot/Xavier initialization (for tanh): randomly sample from  N(0,\\sigma), \\sigma = \\sqrt{\\frac{2}{\\text{fan_in + fan_out}}}\n\nfan_in: number of input units, fan_out: number of output units\n\nHe initialization (for ReLU): randomly sample from  N(0,\\sigma), \\sigma = \\sqrt{\\frac{2}{\\text{fan_in}}}\n\nUniform sampling (instead of N(0,\\sigma)) for deeper networks (w.r.t. vanishing gradients)\n\nfig, ax = plt.subplots(1,1, figsize=(6*fig_scale, 3*fig_scale))\ndraw_neural_net(ax, [3, 5, 5, 5, 5, 5, 3], random_weights=True, figsize=(6, 3))\n\n\n\n","type":"content","url":"/notebooks/neural-networks#weight-initialization","position":43},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Weight initialization: transfer learning","lvl2":"Weight initialization"},"type":"lvl3","url":"/notebooks/neural-networks#weight-initialization-transfer-learning","position":44},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Weight initialization: transfer learning","lvl2":"Weight initialization"},"content":"Instead of starting from scratch, start from weights previously learned from similar tasks\n\nThis is, to a big extent, how humans learn so fast\n\nTransfer learning: learn weights on task T, transfer them to new network\n\nWeights can be frozen, or finetuned to the new data\n\nOnly works if the previous task is ‘similar’ enough\n\nGenerally, weights learned on very diverse data (e.g. ImageNet) transfer better\n\nMeta-learning: learn a good initialization across many related tasks\n\nimport tensorflow as tf\n#import tensorflow_addons as tfa\n\n# Toy surface\ndef f(x, y):\n    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n\n# Tensorflow optimizers\nclass CyclicalLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, initial_learning_rate, maximal_learning_rate, step_size):\n        self.initial_learning_rate = initial_learning_rate\n        self.maximal_learning_rate = maximal_learning_rate\n        self.step_size = step_size\n\n    def __call__(self, step):\n        cycle = tf.floor(1 + step / (2 * self.step_size))\n        x = tf.abs(step / self.step_size - 2 * cycle + 1)\n        return self.initial_learning_rate + (self.maximal_learning_rate - self.initial_learning_rate) * tf.maximum(0.0, (1 - x))\n\nclr_schedule = CyclicalLearningRate(initial_learning_rate=1e-4, \n                                    maximal_learning_rate=0.1, \n                                    step_size=100)\nsgd_cyclic = tf.keras.optimizers.SGD(learning_rate=clr_schedule)\nsgd = tf.optimizers.SGD(0.01)\nlr_schedule = tf.optimizers.schedules.ExponentialDecay(0.02,decay_steps=100,decay_rate=0.96)\nsgd_decay = tf.optimizers.SGD(learning_rate=lr_schedule)\nmomentum = tf.optimizers.SGD(0.005, momentum=0.9, nesterov=False)\nnesterov = tf.optimizers.SGD(0.005, momentum=0.9, nesterov=True)\nadagrad = tf.optimizers.Adagrad(0.4)\nrmsprop = tf.optimizers.RMSprop(learning_rate=0.1)\nadam = tf.optimizers.Adam(learning_rate=0.2, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\noptimizers = [sgd, sgd_decay, momentum, nesterov, adagrad, rmsprop, adam, sgd_cyclic]\nopt_names = ['sgd', 'sgd_decay', 'momentum', 'nesterov', 'adagrad', 'rmsprop', 'adam', 'sgd_cyclic']\ncmap = plt.cm.get_cmap('tab10')\ncolors = [cmap(x/10) for x in range(10)]\n\n# Training\nall_paths = []\nfor opt, name in zip(optimizers, opt_names):\n    x = tf.Variable(0.8)\n    y = tf.Variable(1.6)\n\n    x_history = []\n    y_history = []\n    loss_prev = 0.0\n    max_steps = 100\n    for step in range(max_steps):\n        with tf.GradientTape() as g:\n            loss = f(x, y)\n            x_history.append(x.numpy())\n            y_history.append(y.numpy())\n            grads = g.gradient(loss, [x, y])\n            opt.apply_gradients(zip(grads, [x, y]))\n\n    if np.abs(loss_prev - loss.numpy()) < 1e-6:\n        break\n    loss_prev = loss.numpy()\n    x_history = np.array(x_history)\n    y_history = np.array(y_history)\n    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n    all_paths.append(path)\n\n\n\n# Plotting\nnumber_of_points = 50\nmargin = 4.5\nminima = np.array([3., .5])\nminima_ = minima.reshape(-1, 1)\nx_min = 0. - 2\nx_max = 0. + 3.5\ny_min = 0. - 3.5\ny_max = 0. + 2\nx_points = np.linspace(x_min, x_max, number_of_points) \ny_points = np.linspace(y_min, y_max, number_of_points)\nx_mesh, y_mesh = np.meshgrid(x_points, y_points)\nz = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])\n\ndef plot_optimizers(ax, iterations, optimizers):\n    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=fig_scale, zorder=-1)\n    ax.plot(*minima, 'r*', markersize=20*fig_scale)\n    for name, path, color in zip(opt_names, all_paths, colors):\n        if name in optimizers:\n            p = path[:,:iterations]\n            ax.plot([], [], color=color, label=name, lw=3*fig_scale, linestyle='-')\n            ax.quiver(p[0,:-1], p[1,:-1], p[0,1:]-p[0,:-1], p[1,1:]-p[1,:-1], scale_units='xy', angles='xy', scale=1, color=color, lw=4)\n\n\n    ax.set_xlim((x_min, x_max))\n    ax.set_ylim((y_min, y_max))\n    ax.legend(loc='lower left', prop={'size': 15*fig_scale}) \n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.tight_layout()\n\n\n\nfrom decimal import *\nfrom matplotlib.colors import LogNorm\n\n# Training for momentum\nall_lr_paths = []\nlr_range = [0.005 * i for i in range(0,10)]\nfor lr in lr_range:\n    opt = tf.optimizers.SGD(lr, nesterov=False)\n\n    x_init = 0.8\n    x = tf.compat.v1.get_variable('x', dtype=tf.float32, initializer=tf.constant(x_init))\n    y_init = 1.6\n    y = tf.compat.v1.get_variable('y', dtype=tf.float32, initializer=tf.constant(y_init))\n\n    x_history = []\n    y_history = []\n    z_prev = 0.0\n    max_steps = 100\n    for step in range(max_steps):\n        with tf.GradientTape() as g:\n            z = f(x, y)\n            x_history.append(x.numpy())\n            y_history.append(y.numpy())\n            dz_dx, dz_dy = g.gradient(z, [x, y])\n            opt.apply_gradients(zip([dz_dx, dz_dy], [x, y]))\n\n    if np.abs(z_prev - z.numpy()) < 1e-6:\n        break\n    z_prev = z.numpy()\n    x_history = np.array(x_history)\n    y_history = np.array(y_history)\n    path = np.concatenate((np.expand_dims(x_history, 1), np.expand_dims(y_history, 1)), axis=1).T\n    all_lr_paths.append(path)\n    \n# Plotting\nnumber_of_points = 50\nmargin = 4.5\nminima = np.array([3., .5])\nminima_ = minima.reshape(-1, 1)\nx_min = 0. - 2\nx_max = 0. + 3.5\ny_min = 0. - 3.5\ny_max = 0. + 2\nx_points = np.linspace(x_min, x_max, number_of_points) \ny_points = np.linspace(y_min, y_max, number_of_points)\nx_mesh, y_mesh = np.meshgrid(x_points, y_points)\nz = np.array([f(xps, yps) for xps, yps in zip(x_mesh, y_mesh)])\n\ndef plot_learning_rate_optimizers(ax, iterations, lr):\n    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=fig_scale, zorder=-1)\n    ax.plot(*minima, 'r*', markersize=20*fig_scale)\n    for path, lrate in zip(all_lr_paths, lr_range):\n        if round(lrate,3) == lr:\n            p = path[:,:iterations]\n            ax.plot([], [], color='b', label=\"Learning rate {}\".format(lr), lw=3*fig_scale, linestyle='-')\n            ax.quiver(p[0,:-1], p[1,:-1], p[0,1:]-p[0,:-1], p[1,1:]-p[1,:-1], scale_units='xy', angles='xy', scale=1, color='b', lw=4)\n\n\n    ax.set_xlim((x_min, x_max))\n    ax.set_ylim((y_min, y_max))\n    ax.legend(loc='lower left', prop={'size': 15*fig_scale}) \n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.tight_layout()\n\n\n\n# Toy plot to illustrate nesterov momentum\n# TODO: replace with actual gradient computation?\ndef plot_nesterov(ax, method=\"Nesterov momentum\"):\n    ax.contour(x_mesh, y_mesh, z, levels=np.logspace(-0.5, 5, 25), norm=LogNorm(), cmap=plt.cm.jet, linewidths=fig_scale, zorder=-1)\n    ax.plot(*minima, 'r*', markersize=20*fig_scale)\n    \n    # toy example\n    ax.quiver(-0.8,-1.13,1,1.33, scale_units='xy', angles='xy', scale=1, color='k', alpha=0.5, lw=3, label=\"previous update\")\n    # 0.9 * previous update\n    ax.quiver(0.2,0.2,0.9,1.2, scale_units='xy', angles='xy', scale=1, color='g', lw=3, label=\"momentum step\")\n    if method == \"Momentum\":\n        ax.quiver(0.2,0.2,0.5,0, scale_units='xy', angles='xy', scale=1, color='r', lw=3, label=\"gradient step\")\n        ax.quiver(0.2,0.2,0.9*0.9+0.5,1.2, scale_units='xy', angles='xy', scale=1, color='b', lw=3, label=\"actual step\")\n    if method == \"Nesterov momentum\":\n        ax.quiver(1.1,1.4,-0.2,-1, scale_units='xy', angles='xy', scale=1, color='r', lw=3, label=\"'lookahead' gradient step\")\n        ax.quiver(0.2,0.2,0.7,0.2, scale_units='xy', angles='xy', scale=1, color='b', lw=3, label=\"actual step\")\n\n    ax.set_title(method)\n    ax.set_xlim((x_min, x_max))\n    ax.set_ylim((-2.5, y_max))\n    ax.legend(loc='lower right', prop={'size': 9*fig_scale})\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.tight_layout()\n\n\n\n","type":"content","url":"/notebooks/neural-networks#weight-initialization-transfer-learning","position":45},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Optimizers"},"type":"lvl2","url":"/notebooks/neural-networks#optimizers","position":46},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Optimizers"},"content":"","type":"content","url":"/notebooks/neural-networks#optimizers","position":47},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"SGD with learning rate schedules","lvl2":"Optimizers"},"type":"lvl3","url":"/notebooks/neural-networks#sgd-with-learning-rate-schedules","position":48},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"SGD with learning rate schedules","lvl2":"Optimizers"},"content":"Using a constant learning \\eta rate for weight updates \\mathbf{w}_{(s+1)} = \\mathbf{w}_s-\\eta\\nabla \\mathcal{L}(\\mathbf{w}_s) is not ideal\n\nYou would need to ‘magically’ know the right value\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\nfrom IPython.display import clear_output\n\n@interact\ndef plot_lr(iterations=(1,100,1), learning_rate=(0.005,0.04,0.005)):\n    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n    plot_learning_rate_optimizers(ax,iterations,round(learning_rate,3))\n    plt.show()\n\n\n\nif not interactive:\n    plot_lr(iterations=50, learning_rate=0.02)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#sgd-with-learning-rate-schedules","position":49},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"SGD with learning rate schedules","lvl2":"Optimizers"},"type":"lvl3","url":"/notebooks/neural-networks#sgd-with-learning-rate-schedules-1","position":50},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"SGD with learning rate schedules","lvl2":"Optimizers"},"content":"Learning rate decay/annealing with decay rate k\n\nE.g. exponential (\\eta_{s+1} = \\eta_{0}  e^{-ks}), inverse-time (\\eta_{s+1} = \\frac{\\eta_{0}}{1+ks}),...\n\nCyclical learning rates\n\nChange from small to large: hopefully in ‘good’ region long enough before diverging\n\nWarm restarts: aggressive decay + reset to initial learning rate\n\n@interact\ndef compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n    plot_optimizers(ax,iterations,[optimizer1,optimizer2])\n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(1,2, figsize=(10*fig_scale,4*fig_scale))\n    optimizers = ['sgd_decay', 'sgd_cyclic']\n    for function, ax in zip(optimizers,axes):\n        plot_optimizers(ax,100,function)\n    plt.tight_layout();\n\n\n\n","type":"content","url":"/notebooks/neural-networks#sgd-with-learning-rate-schedules-1","position":51},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Momentum","lvl2":"Optimizers"},"type":"lvl3","url":"/notebooks/neural-networks#momentum","position":52},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Momentum","lvl2":"Optimizers"},"content":"Imagine a ball rolling downhill: accumulates momentum, doesn’t exactly follow steepest descent\n\nReduces oscillation, follows larger (consistent) gradient of the loss surface\n\nAdds a velocity vector \\mathbf{v} with momentum \\gamma (e.g. 0.9, or increase from \\gamma=0.5 to \\gamma=0.99)\n\\mathbf{w}_{(s+1)} = \\mathbf{w}_{(s)} + \\mathbf{v}_{(s)} \\qquad \\text{with} \\qquad\n\\color{blue}{\\mathbf{v}_{(s)}} = \\color{green}{\\gamma \\mathbf{v}_{(s-1)}} - \\color{red}{\\eta \\nabla \\mathcal{L}(\\mathbf{w}_{(s)})}\n\nNesterov momentum: Look where momentum step would bring you, compute gradient there\n\nResponds faster (and reduces momentum) when the gradient changes\n\\color{blue}{\\mathbf{v}_{(s)}} = \\color{green}{\\gamma \\mathbf{v}_{(s-1)}} - \\color{red}{\\eta \\nabla \\mathcal{L}(\\mathbf{w}_{(s)} + \\gamma \\mathbf{v}_{(s-1)})}\n\nfig, axes = plt.subplots(1,2, figsize=(10*fig_scale,4*fig_scale))\nplot_nesterov(axes[0],method=\"Momentum\")\nplot_nesterov(axes[1],method=\"Nesterov momentum\")\n\n\n\n","type":"content","url":"/notebooks/neural-networks#momentum","position":53},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Momentum in practice","lvl3":"Momentum","lvl2":"Optimizers"},"type":"lvl4","url":"/notebooks/neural-networks#momentum-in-practice","position":54},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Momentum in practice","lvl3":"Momentum","lvl2":"Optimizers"},"content":"\n\n@interact\ndef compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n    plot_optimizers(ax,iterations,[optimizer1,optimizer2])\n    plt.show()\n\n\n\nif not interactive:\n    fig, axes = plt.subplots(1,2, figsize=(10*fig_scale,3.5*fig_scale))\n    optimizers = [['sgd','momentum'], ['momentum','nesterov']]\n    for function, ax in zip(optimizers,axes):\n        plot_optimizers(ax,100,function)\n    plt.tight_layout();\n\n\n\n","type":"content","url":"/notebooks/neural-networks#momentum-in-practice","position":55},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Adaptive gradients","lvl2":"Optimizers"},"type":"lvl3","url":"/notebooks/neural-networks#adaptive-gradients","position":56},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Adaptive gradients","lvl2":"Optimizers"},"content":"‘Correct’ the learning rate for each w_i based on specific local conditions (layer depth, fan-in,...)\n\nAdagrad: scale \\eta according to squared sum of previous gradients G_{i,(s)} = \\sum_{t=1}^s \\nabla \\mathcal{L}(w_{i,(t)})^2\n\nUpdate rule for w_i. Usually \\epsilon=10^{-7} (avoids division by 0), \\eta=0.001.\nw_{i,(s+1)} = w_{i,(s)} - \\frac{\\eta}{\\sqrt{G_{i,(s)}+\\epsilon}} \\nabla \\mathcal{L}(w_{i,(s)})\n\nRMSProp: use moving average of squared gradients m_{i,(s)} = \\gamma m_{i,(s-1)} + (1-\\gamma) \\nabla \\mathcal{L}(w_{i,(s)})^2\n\nAvoids that gradients dwindle to 0 as G_{i,(s)} grows. Usually \\gamma=0.9, \\eta=0.001\nw_{i,(s+1)} = w_{i,(s)}- \\frac{\\eta}{\\sqrt{m_{i,(s)}+\\epsilon}} \\nabla \\mathcal{L}(w_{i,(s)})\n\nif not interactive:\n    fig, axes = plt.subplots(1,2, figsize=(10*fig_scale,2.6*fig_scale))\n    optimizers = [['sgd','adagrad', 'rmsprop'], ['rmsprop','rmsprop_mom']]\n    for function, ax in zip(optimizers,axes):\n        plot_optimizers(ax,100,function)\n    plt.tight_layout();\n\n\n\n@interact\ndef compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n    plot_optimizers(ax,iterations,[optimizer1,optimizer2])\n    plt.show()\n\n\n\n","type":"content","url":"/notebooks/neural-networks#adaptive-gradients","position":57},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Adam (Adaptive moment estimation)","lvl2":"Optimizers"},"type":"lvl3","url":"/notebooks/neural-networks#adam-adaptive-moment-estimation","position":58},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Adam (Adaptive moment estimation)","lvl2":"Optimizers"},"content":"Adam: RMSProp + momentum. Adds moving average for gradients as well (\\gamma_2 = momentum):\n\nAdds a bias correction to avoid small initial gradients: \\hat{m}_{i,(s)} = \\frac{m_{i,(s)}}{1-\\gamma} and \\hat{g}_{i,(s)} = \\frac{g_{i,(s)}}{1-\\gamma_2}\ng_{i,(s)} = \\gamma_2 g_{i,(s-1)} + (1-\\gamma_2) \\nabla \\mathcal{L}(w_{i,(s)})\n\n\nw_{i,(s+1)} = w_{i,(s)}- \\frac{\\eta}{\\sqrt{\\hat{m}_{i,(s)}+\\epsilon}} \\hat{g}_{i,(s)}\n\nAdamax: Idem, but use max() instead of moving average: u_{i,(s)} = max(\\gamma u_{i,(s-1)}, |\\mathcal{L}(w_{i,(s)})|)w_{i,(s+1)} = w_{i,(s)}- \\frac{\\eta}{u_{i,(s)}} \\hat{g}_{i,(s)}\n\nif not interactive:\n    # fig, axes = plt.subplots(1,2, figsize=(10*fig_scale,2.6*fig_scale))\n    # optimizers = [['sgd','adam'], ['adam','adamax']]\n    # for function, ax in zip(optimizers,axes):\n    #     plot_optimizers(ax,100,function)\n    # plt.tight_layout();\n    \n    fig, axes = plt.subplots(1,1, figsize=(5*fig_scale,2.6*fig_scale))\n    optimizers = [['sgd','adam']]\n    plot_optimizers(axes,100,['sgd','adam'])\n    plt.tight_layout();\n\n\n\n@interact\ndef compare_optimizers(iterations=(1,100,1), optimizer1=opt_names, optimizer2=opt_names):\n    fig, ax = plt.subplots(figsize=(6*fig_scale,4*fig_scale))\n    plot_optimizers(ax,iterations,[optimizer1,optimizer2])\n    plt.show()\n\n\n\n","type":"content","url":"/notebooks/neural-networks#adam-adaptive-moment-estimation","position":59},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"SGD Optimizer Zoo","lvl2":"Optimizers"},"type":"lvl3","url":"/notebooks/neural-networks#sgd-optimizer-zoo","position":60},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"SGD Optimizer Zoo","lvl2":"Optimizers"},"content":"RMSProp often works well, but do try alternatives. For even more optimizers, \n\nsee here.\n\nif not interactive:\n    fig, ax = plt.subplots(1,1, figsize=(10*fig_scale,5.5*fig_scale))\n    plot_optimizers(ax,100,opt_names)\n\n\n\n@interact\ndef compare_optimizers(iterations=(1,100,1)):\n    fig, ax = plt.subplots(figsize=(10*fig_scale,6*fig_scale))\n    plot_optimizers(ax,iterations,opt_names)\n    plt.show()\n\n\n\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom numpy.random import seed\nfrom tensorflow.random import set_seed\nimport random\nimport os\n\n#Trying to set all seeds\nos.environ['PYTHONHASHSEED']=str(0)\nrandom.seed(0)\nseed(0)\nset_seed(0)\nseed_value= 0\n\n\n\n","type":"content","url":"/notebooks/neural-networks#sgd-optimizer-zoo","position":61},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Neural networks in practice"},"type":"lvl2","url":"/notebooks/neural-networks#neural-networks-in-practice","position":62},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Neural networks in practice"},"content":"There are many practical courses on training neural nets.\n\nE.g.: \n\nhttps://​pytorch​.org​/tutorials/, \n\nfast.ai course\n\nWe’ll use PyTorch in these examples and the labs.\n\nFocus on key design decisions, evaluation, and regularization\n\nRunning example: Fashion-MNIST\n\n28x28 pixel images of 10 classes of fashion items\n\n# Download FMINST data\nmnist = oml.datasets.get_dataset(40996)\nX, y, _, _ = mnist.get_data(target=mnist.default_target_attribute, dataset_format='array');\nX = X.reshape(70000, 28, 28)\nfmnist_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n                  6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n\n# Take some random examples\nfrom random import randint\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor i in range(5):\n    n = randint(0,70000)\n    axes[i].imshow(X[n], cmap=plt.cm.gray_r)\n    axes[i].set_xticks([])\n    axes[i].set_yticks([])\n    axes[i].set_xlabel(\"{}\".format(fmnist_classes[y[n]]))\nplt.show();\n\n\n\n","type":"content","url":"/notebooks/neural-networks#neural-networks-in-practice","position":63},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Preparing the data","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#preparing-the-data","position":64},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Preparing the data","lvl2":"Neural networks in practice"},"content":"We’ll use feed-forward networks first, so we flatten the input data\n\nCreate train-test splits to evaluate the model later\n\nConvert the data (numpy arrays) to PyTorch tensors# Flatten images, create train-test split\nX_flat = X.reshape(70000, 28 * 28)\nX_train, X_test, y_train, y_test = train_test_split(X_flat, y, stratify=y)\n\n# Convert numpy arrays to PyTorch tensors with correct types\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\nCreate data loaders to return data in batchesimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Create PyTorch datasets\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create DataLoaders\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Flatten images, create train-test split\nX_flat = X.reshape(70000, 28 * 28)\nX_train, X_test, y_train, y_test = train_test_split(X_flat, y, stratify=y)\n\n# Convert numpy arrays to PyTorch tensors with correct types\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n\n# Create PyTorch datasets\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True, pin_memory=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4, shuffle=False, pin_memory=False)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#preparing-the-data","position":65},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Building the network","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#building-the-network","position":66},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Building the network","lvl2":"Neural networks in practice"},"content":"PyTorch has a Sequential and Functional API. We’ll use the Sequential API first.\n\nInput layer: a flat vector of 28*28 = 784 nodes\n\nWe’ll see how to properly deal with images later\n\nTwo dense (Linear) hidden layers: 512 nodes each, ReLU activation\n\nOutput layer: 10 nodes (for 10 classes)\n\nSoftMax not needed, it will be done in the loss functionimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Linear(28 * 28, 512), # Layer 1: 28*28 inputs to 512 output nodes\n    nn.ReLU(),\n    nn.Linear(512, 512), # Layer 2: 512 inputs to 512 output nodes\n    nn.ReLU(),\n    nn.Linear(512, 10), # Layer 3: 512 inputs to output nodes\n)\n\nIn the Functional API, the same network looks like thisimport torch.nn.functional as F\n\nclass NeuralNetwork(nn.Module): # Class that defines your model\n    def __init__(self):\n        super(NeuralNetwork, self).__init__() # Components defined in __init__\n        self.fc1 = nn.Linear(28 * 28, 512)    # Fully connected layers\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 10)\n\n    def forward(self, x):        # Forward pass and structure of the network\n        x = F.relu(self.fc1(x))  # Layer 1: Input to FC1, then through ReLU\n        x = F.relu(self.fc2(x))  # Layer 2: Then though FC2, then ReLU\n        x = self.fc3(x)          # Layer 3: Then though FC3, then SoftMax\n        return x                 # Return output\n\nmodel = NeuralNetwork()\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass NeuralNetwork(nn.Module): # Class that defines your model\n    def __init__(self):\n        super(NeuralNetwork, self).__init__() # Main components often defined in __init__\n        self.fc1 = nn.Linear(28 * 28, 512)    # Fully connected layers\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 10)\n\n    def forward(self, x):                 # Forward pass and structure of the network\n        x = F.relu(self.fc1(x))           # Layer 1: Input to FC1, then through ReLU\n        x = F.relu(self.fc2(x))           # Layer 2: Then though FC2, then ReLU\n        x = self.fc3(x)                   # Layer 3: Then though FC3, then SoftMax\n        return x                          # Return output\n\nmodel = NeuralNetwork()\n\n\n\n","type":"content","url":"/notebooks/neural-networks#building-the-network","position":67},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Choosing loss, optimizer, metrics","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#choosing-loss-optimizer-metrics","position":68},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Choosing loss, optimizer, metrics","lvl2":"Neural networks in practice"},"content":"Loss function: Cross-entropy (log loss) for multi-class classification\n\nOptimizer: Any of the optimizers we discussed before. RMSprop/Adam usually work well.\n\nMetrics: To monitor performance during training and testing, e.g. accuracyimport torch.optim as optim\nimport torchmetrics\n\n# Loss function with label smoothing. Also applies softmax internally\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n\n# Optimizer. Note that we pass the model parameters at creation time.\noptimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.0)\n\n# Accuracy metric\naccuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\nimport torch.optim as optim\nimport torchmetrics\n\ncross_entropy = nn.CrossEntropyLoss(label_smoothing=0.01)\noptimizer = optim.RMSprop(model.parameters(), lr=0.001, momentum=0.0)\naccuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#choosing-loss-optimizer-metrics","position":69},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Training on GPU","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#training-on-gpu","position":70},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Training on GPU","lvl2":"Neural networks in practice"},"content":"The device is where the training is done. It’s “cpu” by default.\n\nThe model, tensors, and metric must all be moved to the same device.if torch.cuda.is_available():         # For CUDA based systems\n    device = torch.device(\"cuda\")\nif torch.backends.mps.is_available(): # For MPS (M1-M4 Mac) based systems\n    device = torch.device(\"mps\")\nprint(f\"Used device: {device}\")\n\n# Move models and metrics to `device`\nmodel.to(device)                      \naccuracy_metric = accuracy_metric.to(device)\n\n# Move batches one at a time (GPUs have limited memory)\nfor X_batch, y_batch in train_loader:\n    X_batch, y_batch = X_batch.to(device), y_batch.to(device)  \n\nif torch.cuda.is_available():         # For CUDA based systems\n    device = torch.device(\"cuda\")\nif torch.backends.mps.is_available(): # For MPS (M1-M4 Mac) based systems\n    device = torch.device(\"mps\")\nprint(f\"Used device: {device}\")\n\n# Move everything to `device`\nmodel.to(device)                      \naccuracy_metric = accuracy_metric.to(device)\nfor X_batch, y_batch in train_loader:  \n    X_batch, y_batch = X_batch.to(device), y_batch.to(device)  \n    break\n\n\n\n","type":"content","url":"/notebooks/neural-networks#training-on-gpu","position":71},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Training loop","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#training-loop","position":72},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Training loop","lvl2":"Neural networks in practice"},"content":"In pure PyTorch, you have to write the training loop yourself (as well as any code to print out progress)for epoch in range(10):\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device) # to GPU\n\n        # Forward pass + loss calculation\n        outputs = model(X_batch)\n        loss = cross_entropy(outputs, y_batch)\n\n        # Backward pass\n        optimizer.zero_grad() # Reset gradients (otherwise they accumulate)\n        loss.backward()       # Backprop. Computes all gradients\n        optimizer.step()      # Uses gradients to update weigths\n\nnum_epochs = 5\nfor epoch in range(5):\n    total_loss, correct, total = 0, 0, 0\n\n    for X_batch, y_batch in train_loader:\n        X_batch, y_batch = X_batch.to(device), y_batch.to(device) # to GPU\n\n        # Forward pass + loss calculation\n        outputs = model(X_batch)\n        loss = cross_entropy(outputs, y_batch)\n\n        # Backward pass: zero gradients, do backprop, do optimizer step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Compute training metrics\n        total_loss += loss.item()\n        correct += accuracy_metric(outputs, y_batch).item() * y_batch.size(0)\n        total += y_batch.size(0)\n\n    # Compute epoch metrics\n    avg_loss = total_loss / len(train_loader)\n    avg_acc = correct / total\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n\n\n\n","type":"content","url":"/notebooks/neural-networks#training-loop","position":73},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"loss.backward()","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#loss-backward","position":74},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"loss.backward()","lvl2":"Neural networks in practice"},"content":"Every time you perform a forward pass, PyTorch dynamically constructs a computational graph\n\nThis graph tracks tensors and operations involved in computing gradients (see next slide)\n\nThe loss returned is a tensor, and every tensor is part of the computational graph\n\nWhen you call .backward() on loss, PyTorch traverses this graph in reverse to compute all gradients\n\nThis process is called automatic differentiation\n\nStores intermediate values so no gradient component is calculated twice\n\nWhen backward() completes, the computational graph is discarded by default to free memory\n\nComputational graph for our model (Loss in green, weights/biases in blue)\n\nfrom torchviz import make_dot\nfrom IPython.display import Image\n\ntoy_X = X_train_tensor[0].to(device)\ntoy_y = y_train_tensor[0].to(device)\ntoy_out = model(toy_X)\nloss = cross_entropy(toy_out, toy_y)\n\ngraph = make_dot(loss, params=dict(model.named_parameters()))\ngraph.render(\"computational_graph\", format=\"png\", view=True)\nImage(\"computational_graph.png\")\n\n\n\n\n","type":"content","url":"/notebooks/neural-networks#loss-backward","position":75},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"In PyTorch Lightning","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#in-pytorch-lightning","position":76},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"In PyTorch Lightning","lvl2":"Neural networks in practice"},"content":"A high-level framework built on PyTorch that simplifies deep learning model training\n\nSame code, but extend pl.LightingModule instead of nn.Module\n\nHas a number of predefined functions. For instance:class NeuralNetwork(pl.LightningModule):\n    def __init__(self):\n        pass # Initialize model\n    \n    def forward(self, x): \n        pass # Forward pass, return output tensor\n    \n    def configure_optimizers(self):\n        pass # Configure optimizer (e.g. Adam)\n    \n    def training_step(self, batch, batch_idx):\n        pass # Return loss tensor\n    \n    def validation_step(self, batch, batch_idx):\n        pass # Return loss tensor\n        \n    def test_step(self, batch, batch_idx):\n        pass # Return loss tensor\n\nOur entire example now becomes:import pytorch_lightning as pl\n\nclass NeuralNetwork(pl.LightningModule):\n    def __init__(self):\n        super(LitNeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 10)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def training_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)\n        return self.criterion(outputs, y_batch)\n\n    def configure_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr=0.001, momentum=0.0)\n\nmodel = NeuralNetwork()    \n\nimport pytorch_lightning as pl\n\nclass NeuralNetwork(pl.LightningModule):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 10)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the image\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def training_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)                 # Logits (raw outputs)\n        loss = self.criterion(outputs, y_batch) # Loss\n        preds = torch.argmax(outputs, dim=1)    # Predictions\n        acc = self.accuracy(preds, y_batch)\n        self.log(\"train_loss\", loss, sync_dist=True)\n        self.log(\"train_acc\", acc, sync_dist=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)\n        loss = self.criterion(outputs, y_batch)\n        preds = torch.argmax(outputs, dim=1)\n        acc = self.accuracy(preds, y_batch)\n        self.log(\"val_loss\", loss, sync_dist=True, on_epoch=True)\n        self.log(\"val_acc\", acc, sync_dist=True, on_epoch=True)\n        return loss\n    \n    def on_train_epoch_end(self):\n        avg_loss = self.trainer.callback_metrics[\"train_loss\"].item()\n        avg_acc = self.trainer.callback_metrics[\"train_acc\"].item()\n        print(f\"Epoch {self.trainer.current_epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {avg_acc:.4f}\")\n\n    def configure_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr=0.001, momentum=0.0)\n\nmodel = NeuralNetwork()   \n\n\n\nWe can also get a nice model summary\n\nLots of parameters (weights and biases) to learn!\n\nhidden layer 1 : (28 * 28 + 1) * 512 = 401920\n\nhidden layer 2 : (512 + 1) * 512 = 262656\n\noutput layer: (512 + 1) * 10 = 5130ModelSummary(pl_model, max_depth=2)\n\nfrom pytorch_lightning.utilities.model_summary import ModelSummary\nModelSummary(model, max_depth=2)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#in-pytorch-lightning","position":77},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Training","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#training","position":78},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Training","lvl2":"Neural networks in practice"},"content":"To log results while training, we can extend the training methods:def training_step(self, batch, batch_idx):\n    X_batch, y_batch = batch\n    outputs = self(X_batch)                 # Logits (raw outputs)\n    loss = self.criterion(outputs, y_batch) # Loss\n    preds = torch.argmax(outputs, dim=1)    # Predictions\n    acc = self.accuracy(preds, y_batch)     # Metric\n    self.log(\"train_loss\", loss)            # self.log is the default\n    self.log(\"train_acc\", acc)              # TensorBoard logger\n    return loss\n\ndef on_train_epoch_end(self): # Runs at the end of every epoch\n    avg_loss = self.trainer.callback_metrics[\"train_loss\"].item()\n    avg_acc = self.trainer.callback_metrics[\"train_acc\"].item()\n    print(f\"Epoch {self.trainer.current_epoch}: Loss = {avg_loss:.4f}, Train accuracy = {avg_acc:.4f}\")\n\nWe also need to implement the validation steps if we want validation scores\n\nIdentical to training_step except for the loggingdef validation_step(self, batch, batch_idx):\n    X_batch, y_batch = batch\n    outputs = self(X_batch)\n    loss = self.criterion(outputs, y_batch)\n    preds = torch.argmax(outputs, dim=1)\n    acc = self.accuracy(preds, y_batch)\n    self.log(\"val_loss\", loss, on_epoch=True)\n    self.log(\"val_acc\", acc, on_epoch=True)\n    return loss\n\n","type":"content","url":"/notebooks/neural-networks#training","position":79},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Lightning Trainer","lvl2":"Neural networks in practice"},"type":"lvl3","url":"/notebooks/neural-networks#lightning-trainer","position":80},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Lightning Trainer","lvl2":"Neural networks in practice"},"content":"For training, we can now create a trainer and fit it. This will also automatically move everything to GPU.trainer = pl.Trainer(max_epochs=3, accelerator=\"gpu\") # Or 'cpu'\ntrainer.fit(model, train_loader)\n\nimport logging # Cleaner output\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n\naccelerator = \"cpu\"\nif torch.backends.mps.is_available():\n    accelerator = \"mps\"\nif torch.cuda.is_available():\n    accelerator = \"gpu\"\n\n\n\ntrainer = pl.Trainer(max_epochs=3, accelerator=accelerator)\ntrainer.fit(model, train_loader)\n\n\n\n\n\n","type":"content","url":"/notebooks/neural-networks#lightning-trainer","position":81},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Choosing training hyperparameters","lvl3":"Lightning Trainer","lvl2":"Neural networks in practice"},"type":"lvl4","url":"/notebooks/neural-networks#choosing-training-hyperparameters","position":82},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Choosing training hyperparameters","lvl3":"Lightning Trainer","lvl2":"Neural networks in practice"},"content":"Number of epochs: enough to allow convergence\n\nToo much: model starts overfitting (or levels off and just wastes time)\n\nBatch size: small batches (e.g. 32, 64,... samples) often preferred\n\n‘Noisy’ training data makes overfitting less likely\n\nLarge batches generalize less well (‘generalization gap’)\n\nRequires less memory (especially in GPUs)\n\nLarge batches do speed up training, may converge in fewer epochs\n\nBatch size interacts with learning rate\n\nInstead of shrinking the learning rate you can increase batch size\n\n# Same model, more silent\nclass NeuralNetwork(pl.LightningModule):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 10)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the image\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def training_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)                 # Logits (raw outputs)\n        loss = self.criterion(outputs, y_batch) # Loss\n        preds = torch.argmax(outputs, dim=1)    # Predictions\n        acc = self.accuracy(preds, y_batch)\n        self.log(\"train_loss\", loss, sync_dist=True)\n        self.log(\"train_acc\", acc, sync_dist=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)\n        loss = self.criterion(outputs, y_batch)\n        preds = torch.argmax(outputs, dim=1)\n        acc = self.accuracy(preds, y_batch)\n        self.log(\"val_loss\", loss, sync_dist=True, on_epoch=True)\n        self.log(\"val_acc\", acc, sync_dist=True, on_epoch=True)\n        return loss\n    \n    def on_train_epoch_end(self):\n        avg_loss = self.trainer.callback_metrics[\"train_loss\"].item()\n        avg_acc = self.trainer.callback_metrics[\"train_acc\"].item()\n        \n    def on_validation_epoch_end(self):\n        avg_loss = self.trainer.callback_metrics[\"val_loss\"].item()\n        avg_acc = self.trainer.callback_metrics[\"val_acc\"].item()\n\n    def configure_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr=0.001, momentum=0.0)\n\nmodel = NeuralNetwork() \n\n\n\nfrom pytorch_lightning.callbacks import Callback\nfrom IPython.display import clear_output\n\n# First try. Not very efficient yet.\nclass TrainingBatchPlotCallback(Callback):\n    def __init__(self, update_interval=100, smoothing=100):\n        \"\"\"\n        Callback to plot training and validation progress.\n\n        Args:\n            update_interval (int): Number of batches between updates.\n            smoothing (int): Window size for moving average.\n        \"\"\"\n        super().__init__()\n        self.update_interval = update_interval\n        self.smoothing = smoothing\n        self.losses = []\n        self.acc = []\n        self.val_losses = []\n        self.val_acc = []\n        self.val_x_positions = []  # Store x positions for validation scores\n        self.max_acc = 0\n        self.global_step = 0  # Tracks processed training batches\n        self.epoch_count = 0  # Tracks processed epochs\n        self.steps_per_epoch = 0  # Updated dynamically to track num_batches per epoch\n\n    def moving_average(self, values, window):\n        \"\"\"Computes a simple moving average for smoothing.\"\"\"\n        if len(values) < window:\n            return np.convolve(values, np.ones(len(values)) / len(values), mode=\"valid\")\n        return np.convolve(values, np.ones(window) / window, mode=\"valid\")\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Updates training loss and accuracy every `update_interval` batches.\"\"\"\n        self.global_step += 1\n        logs = trainer.callback_metrics\n\n        # Store training loss and accuracy\n        if \"train_loss\" in logs:\n            self.losses.append(logs[\"train_loss\"].item())\n        if \"train_acc\" in logs:\n            self.acc.append(logs[\"train_acc\"].item())\n\n        # Update plot every `update_interval` training batches\n        if self.global_step % self.update_interval == 0:\n            self.plot_progress()\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Updates validation loss and accuracy at the end of each epoch, scaling x-axis correctly.\"\"\"\n        logs = trainer.callback_metrics\n        self.epoch_count += 1\n        if self.steps_per_epoch == 0:\n            self.steps_per_epoch = self.global_step\n\n        self.val_x_positions.append(self.global_step)\n\n        # Retrieve validation metrics from trainer\n        if \"val_loss\" in logs:\n            self.val_losses.append(logs[\"val_loss\"].item())\n        if \"val_acc\" in logs:\n            val_acc = logs[\"val_acc\"].item()\n            self.val_acc.append(val_acc)\n            self.max_acc = max(self.max_acc, val_acc)\n            \n    def on_train_end(self, trainer, pl_module):\n        \"\"\"Ensures the final validation curve is plotted after the last epoch.\"\"\"\n        self.plot_progress()\n            \n    def plot_progress(self):\n        \"\"\"Plots training and validation metrics with moving averages.\"\"\"\n        clear_output(wait=True)\n        steps = np.arange(1, len(self.losses) + 1)\n\n        plt.figure(figsize=(8, 4))\n        plt.ylim(0, 1)  # Constrain y-axis between 0 and 1\n\n        # Training curves\n        plt.plot(steps / self.steps_per_epoch, self.losses, lw=0.2, alpha=0.3, c=\"b\", linestyle=\"-\")\n        plt.plot(steps / self.steps_per_epoch, self.acc, lw=0.2, alpha=0.3, c=\"r\", linestyle=\"-\")\n\n        # Moving average (thicker line)\n        if len(self.losses) >= self.smoothing:\n            plt.plot(steps[self.smoothing - 1:] / self.steps_per_epoch, self.moving_average(self.losses, self.smoothing),\n                     lw=1, c=\"b\", linestyle=\"-\", label=\"Train Loss\")\n        if len(self.acc) >= self.smoothing:\n            plt.plot(steps[self.smoothing - 1:] / self.steps_per_epoch, self.moving_average(self.acc, self.smoothing),\n                     lw=1, c=\"r\", linestyle=\"-\", label=\"Train Acc\")\n\n        # Validation curves (scaled to correct x-positions)\n        if self.val_losses:\n            plt.plot(np.array(self.val_x_positions) / self.steps_per_epoch, self.val_losses, c=\"b\", linestyle=\":\", label=\"Val Loss\", lw=2)\n        if self.val_acc:\n            plt.plot(np.array(self.val_x_positions) / self.steps_per_epoch, self.val_acc, c=\"r\", linestyle=\":\", label=\"Val Acc\", lw=2)\n\n        plt.xlabel(\"Epochs\", fontsize=12)\n        plt.ylabel(\"Loss/Accuracy\", fontsize=12)\n        plt.title(f\"Training Progress (Step {self.global_step}, Epoch {self.epoch_count}, Max Val Acc {self.max_acc:.4f})\", fontsize=12)\n        plt.xticks(fontsize=10)\n        plt.yticks(fontsize=10)\n        plt.legend(fontsize=10)\n        plt.show()\n\n\n\n","type":"content","url":"/notebooks/neural-networks#choosing-training-hyperparameters","position":83},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Model selection"},"type":"lvl2","url":"/notebooks/neural-networks#model-selection","position":84},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Model selection"},"content":"Train the neural net and track the loss after every iteration on a validation set\n\nYou can add a callback to the fit version to get info on every epoch\n\nBest model after a few epochs, then starts overfitting\n\ntrainer = pl.Trainer(\n    max_epochs=15,\n    enable_progress_bar=False,\n    accelerator=accelerator,\n    callbacks=[TrainingBatchPlotCallback()] # Attach the callback\n)\ntrainer.fit(model, train_loader, test_loader)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#model-selection","position":85},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Early stopping","lvl2":"Model selection"},"type":"lvl3","url":"/notebooks/neural-networks#early-stopping","position":86},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Early stopping","lvl2":"Model selection"},"content":"Stop training when the validation loss (or validation accuracy) no longer improves\n\nLoss can be bumpy: use a moving average or wait for k steps without improvement# Define early stopping callback\nearly_stopping = EarlyStopping(\n    monitor=\"val_loss\", mode=\"min\", # minimize validation loss\n    patience=3)                     # Number of epochs with no improvement before stopping\n          \n# Update the Trainer to include early stopping as a callback\ntrainer = pl.Trainer(\n    max_epochs=10, accelerator=accelerator,\n    callbacks=[TrainingPlotCallback(), early_stopping]  # Attach the callbacks\n)\n\n","type":"content","url":"/notebooks/neural-networks#early-stopping","position":87},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Regularization and memorization capacity","lvl2":"Model selection"},"type":"lvl3","url":"/notebooks/neural-networks#regularization-and-memorization-capacity","position":88},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Regularization and memorization capacity","lvl2":"Model selection"},"content":"The number of learnable parameters is called the model capacity\n\nA model with more parameters has a higher memorization capacity\n\nToo high capacity causes overfitting, too low causes underfitting\n\nIn the extreme, the training set can be ‘memorized’ in the weights\n\nSmaller models are forced it to learn a compressed representation that generalizes better\n\nFind the sweet spot: e.g. start with few parameters, increase until overfitting stars.\n\nExample: 256 nodes in first layer, 32 nodes in second layer, similar performance\n\nAvoid bottlenecks: layers so small that information is lostself.fc1 = nn.Linear(28 * 28, 256)\nself.fc2 = nn.Linear(256, 32)\nself.fc3 = nn.Linear(32, 10)\n\n","type":"content","url":"/notebooks/neural-networks#regularization-and-memorization-capacity","position":89},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Weight regularization (weight decay)","lvl3":"Regularization and memorization capacity","lvl2":"Model selection"},"type":"lvl4","url":"/notebooks/neural-networks#weight-regularization-weight-decay","position":90},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Weight regularization (weight decay)","lvl3":"Regularization and memorization capacity","lvl2":"Model selection"},"content":"We can also add weight regularization to our loss function (or invent your own)\n\nL1 regularization: leads to sparse networks with many weights that are 0\n\nL2 regularization: leads to many very small weightsdef training_step(self, batch, batch_idx):\n    X_batch, y_batch = batch\n    outputs = self(X_batch)                 \n    loss = self.criterion(outputs, y_batch)\n    l1_lambda = 1e-5 # L1 Regularization\n    l1_loss = sum(p.abs().sum() for p in self.parameters())\n    l2_lambda = 1e-4 # L2 Regularization\n    l2_loss = sum((p ** 2).sum() for p in self.parameters())\n    return loss + l2_lambda * l2_loss  # Using L2 only\n\nAlternative: set weight_decay in the optimizer (only for L2 loss)def configure_optimizers(self):\n    return optim.RMSprop(self.parameters(), lr=0.001, momentum=0.0, weight_decay=1e-4)\n\n","type":"content","url":"/notebooks/neural-networks#weight-regularization-weight-decay","position":91},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Dropout","lvl2":"Model selection"},"type":"lvl3","url":"/notebooks/neural-networks#dropout","position":92},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Dropout","lvl2":"Model selection"},"content":"Every iteration, randomly set a number of activations a_i to 0\n\nDropout rate : fraction of the outputs that are zeroed-out (e.g. 0.1 - 0.5)\n\nUse higher dropout rates for deeper networks\n\nUse higher dropout in early layers, lower dropout later\n\nEarly layers are usually larger, deeper layers need stability\n\nIdea: break up accidental non-significant learned patterns\n\nAt test time, nothing is dropped out, but the output values are scaled down by the dropout rate\n\nBalances out that more units are active than during training\n\nfig = plt.figure(figsize=(3*fig_scale, 3*fig_scale))\nax = fig.gca()\ndraw_neural_net(ax, [2, 3, 1], draw_bias=True, labels=True, \n                show_activations=True, activation=True)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#dropout","position":93},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Dropout layers","lvl3":"Dropout","lvl2":"Model selection"},"type":"lvl4","url":"/notebooks/neural-networks#dropout-layers","position":94},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Dropout layers","lvl3":"Dropout","lvl2":"Model selection"},"content":"Dropout is usually implemented as a special layerdef __init__(self):\n    super(NeuralNetwork, self).__init__()\n    self.fc1 = nn.Linear(28 * 28, 512)\n    self.dropout1 = nn.Dropout(p=0.2)  # 20% dropout\n    self.fc2 = nn.Linear(512, 512)\n    self.dropout2 = nn.Dropout(p=0.1)  # 10% dropout\n    self.fc3 = nn.Linear(512, 10)\n\ndef forward(self, x):\n    x = F.relu(self.fc1(x))\n    x = self.dropout1(x)  # Apply dropout\n    x = F.relu(self.fc2(x))\n    x = self.dropout2(x)  # Apply dropout\n    return self.fc3(x)\n\n","type":"content","url":"/notebooks/neural-networks#dropout-layers","position":95},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Batch Normalization","lvl3":"Dropout","lvl2":"Model selection"},"type":"lvl4","url":"/notebooks/neural-networks#batch-normalization","position":96},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"Batch Normalization","lvl3":"Dropout","lvl2":"Model selection"},"content":"We’ve seen that scaling the input is important, but what if layer activations become very large?\n\nSame problems, starting deeper in the network\n\nBatch normalization: normalize the activations of the previous layer within each batch\n\nWithin a batch, set the mean activation close to 0 and the standard deviation close to 1\n\nAcross badges, use exponential moving average of batch-wise mean and variance\n\nAllows deeper networks less prone to vanishing or exploding gradients\n\n","type":"content","url":"/notebooks/neural-networks#batch-normalization","position":97},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"BatchNorm layers","lvl3":"Dropout","lvl2":"Model selection"},"type":"lvl4","url":"/notebooks/neural-networks#batchnorm-layers","position":98},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl4":"BatchNorm layers","lvl3":"Dropout","lvl2":"Model selection"},"content":"Batch normalization is also usually implemented as a special layerdef __init__(self):\n    super(NeuralNetwork, self).__init__()\n    self.fc1 = nn.Linear(28 * 28, 512)\n    self.bn1 = nn.BatchNorm1d(512)  # Batch normalization after first layer\n    self.fc2 = nn.Linear(512, 265)\n    self.bn2 = nn.BatchNorm1d(265)  # Batch normalization after second layer\n    self.fc3 = nn.Linear(265, 10)\n\ndef forward(self, x):\n    x = x.view(x.size(0), -1)  # Flatten the image\n    x = F.relu(self.bn1(self.fc1(x)))  # Apply batch norm after linear layer\n    x = F.relu(self.bn2(self.fc2(x)))  # Apply batch norm after second layer\n    return self.fc3(x)\n\n","type":"content","url":"/notebooks/neural-networks#batchnorm-layers","position":99},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"New model","lvl2":"Model selection"},"type":"lvl3","url":"/notebooks/neural-networks#new-model","position":100},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"New model","lvl2":"Model selection"},"content":"class NeuralNetwork(pl.LightningModule):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 265)\n        self.bn1 = nn.BatchNorm1d(265)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(265, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(64, 32)\n        self.bn3 = nn.BatchNorm1d(32)\n        self.dropout3 = nn.Dropout(0.5)\n        self.fc4 = nn.Linear(32, 10)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.fc3(x)))\n        x = self.dropout3(x)\n        x = self.fc4(x)\n        return x\n\n","type":"content","url":"/notebooks/neural-networks#new-model","position":101},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"New model (Sequential API)","lvl2":"Model selection"},"type":"lvl3","url":"/notebooks/neural-networks#new-model-sequential-api","position":102},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"New model (Sequential API)","lvl2":"Model selection"},"content":"model = nn.Sequential(\n        nn.Linear(28 * 28, 265),\n        nn.BatchNorm1d(265),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(265, 64),\n        nn.BatchNorm1d(64),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(64, 32),\n        nn.BatchNorm1d(32),\n        nn.ReLU(),\n        nn.Dropout(0.5),\n        nn.Linear(32, 10))\n\nOur model now performs better and is improving.\n\nclass NeuralNetwork(pl.LightningModule):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 265)\n        self.bn1 = nn.BatchNorm1d(265)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(265, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(64, 32)\n        self.bn3 = nn.BatchNorm1d(32)\n        self.dropout3 = nn.Dropout(0.5)\n        self.fc4 = nn.Linear(32, 10)\n        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.01)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = self.dropout1(x)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.dropout2(x)\n        x = F.relu(self.bn3(self.fc3(x)))\n        x = self.dropout3(x)\n        x = self.fc4(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)                 # Logits (raw outputs)\n        loss = self.criterion(outputs, y_batch) # Loss\n        preds = torch.argmax(outputs, dim=1)    # Predictions\n        acc = self.accuracy(preds, y_batch)\n        self.log(\"train_loss\", loss, sync_dist=True)\n        self.log(\"train_acc\", acc, sync_dist=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        X_batch, y_batch = batch\n        outputs = self(X_batch)\n        loss = self.criterion(outputs, y_batch)\n        preds = torch.argmax(outputs, dim=1)\n        acc = self.accuracy(preds, y_batch)\n        self.log(\"val_loss\", loss, sync_dist=True, on_epoch=True)\n        self.log(\"val_acc\", acc, sync_dist=True, on_epoch=True)\n        return loss\n    \n    def on_train_epoch_end(self):\n        avg_loss = self.trainer.callback_metrics[\"train_loss\"].item()\n        avg_acc = self.trainer.callback_metrics[\"train_acc\"].item()\n        \n    def on_validation_epoch_end(self):\n        avg_loss = self.trainer.callback_metrics[\"val_loss\"].item()\n        avg_acc = self.trainer.callback_metrics[\"val_acc\"].item()\n\n    def configure_optimizers(self):\n        return optim.RMSprop(self.parameters(), lr=0.001, momentum=0.0)\n    \nmodel = NeuralNetwork() \n\ntrainer = pl.Trainer(\n    max_epochs=10,\n    enable_progress_bar=False,\n    accelerator=accelerator,\n    callbacks=[TrainingBatchPlotCallback()] # Attach the callback\n)\ntrainer.fit(model, train_loader, test_loader)\n\n\n\n","type":"content","url":"/notebooks/neural-networks#new-model-sequential-api","position":103},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Other logging tools","lvl2":"Model selection"},"type":"lvl3","url":"/notebooks/neural-networks#other-logging-tools","position":104},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl3":"Other logging tools","lvl2":"Model selection"},"content":"There is a lot more tooling to help you build good models\n\nE.g. TensorBoard is easy to integrate and offers a convenient dashboardlogger = pl.loggers.TensorBoardLogger(\"logs/\", name=\"my_experiment\")\ntrainer = pl.Trainer(max_epochs=2, logger=logger)\ntrainer.fit(lit_model, trainloader)\n\n","type":"content","url":"/notebooks/neural-networks#other-logging-tools","position":105},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/neural-networks#summary-1","position":106},{"hierarchy":{"lvl1":"Lecture 6. Neural Networks","lvl2":"Summary"},"content":"Neural architectures\n\nTraining neural nets\n\nForward pass: Tensor operations\n\nBackward pass: Backpropagation\n\nNeural network design:\n\nActivation functions\n\nWeight initialization\n\nOptimizers\n\nNeural networks in practice\n\nModel selection\n\nEarly stopping\n\nMemorization capacity and information bottleneck\n\nL1/L2 regularization\n\nDropout\n\nBatch normalization","type":"content","url":"/notebooks/neural-networks#summary-1","position":107},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks"},"type":"lvl1","url":"/notebooks/convolutional-neural-networks","position":0},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks"},"content":"Handling image data\n\nJoaquin Vanschoren, Eindhoven University of Technology\n\n","type":"content","url":"/notebooks/convolutional-neural-networks","position":1},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#overview","position":2},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Overview"},"content":"Image convolution\n\nConvolutional neural networks\n\nData augmentation\n\nReal-world CNNs\n\nModel interpretation\n\nUsing pre-trained networks (transfer learning)\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots \nif interactive:\n    fig_scale = 0.5\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.4\n    plt.rcParams.update(print_config)\n    \nHTML('''<style>.rise-enabled .reveal pre {font-size=75%} </style>''')\n\n\n\n%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n\n\n\nimport pickle \ndata_dir = '../data/cats-vs-dogs_small'\nmodel_dir = '../data/models'\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n    \nwith open(\"../data/histories.pkl\", \"rb\") as f:\n    histories = pickle.load(f)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#overview","position":3},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Convolutions"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#convolutions","position":4},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Convolutions"},"content":"Operation that transforms an image by sliding a smaller image (called a filter or kernel ) over the image and multiplying the pixel values\n\nSlide an n x n filter over n x n patches of the original image\n\nEvery pixel is replaced by the sum of the element-wise products of the values of the image patch around that pixel and the kernel# kernel and image_patch are n x n matrices\npixel_out = np.sum(kernel * image_patch)\n\nfrom __future__ import print_function\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual, Dropdown\nfrom skimage import color\n\n\n# Visualize convolution. See https://tonysyu.github.io/\ndef iter_pixels(image):\n    \"\"\" Yield pixel position (row, column) and pixel intensity. \"\"\"\n    height, width = image.shape[:2]\n    for i in range(height):\n        for j in range(width):\n            yield (i, j), image[i, j]\n            \n# Visualize result\ndef imshow_pair(image_pair, titles=('', ''), figsize=(8, 4), **kwargs):\n    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n    for ax, img, label in zip(axes.ravel(), image_pair, titles):\n        ax.imshow(img, **kwargs)\n        ax.set_title(label, fontdict={'fontsize':32*fig_scale})\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n# Visualize result\ndef imshow_triple(axes, image_pair, titles=('', '', ''), figsize=(8, 4), **kwargs):\n    for ax, img, label in zip(axes, image_pair, titles):\n        ax.imshow(img, **kwargs)\n        ax.set_title(label, fontdict={'fontsize':10*fig_scale})\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n# Zero-padding\ndef padding_for_kernel(kernel):\n    \"\"\" Return the amount of padding needed for each side of an image.\n\n    For example, if the returned result is [1, 2], then this means an\n    image should be padded with 1 extra row on top and bottom, and 2\n    extra columns on the left and right.\n    \"\"\"\n    # Slice to ignore RGB channels if they exist.\n    image_shape = kernel.shape[:2]\n    # We only handle kernels with odd dimensions so make sure that's true.\n    # (The \"center\" pixel of an even number of pixels is arbitrary.)\n    assert all((size % 2) == 1 for size in image_shape)\n    return [(size - 1) // 2 for size in image_shape]\ndef add_padding(image, kernel):\n    h_pad, w_pad = padding_for_kernel(kernel)\n    return np.pad(image, ((h_pad, h_pad), (w_pad, w_pad)),\n                  mode='constant', constant_values=0)\ndef remove_padding(image, kernel):\n    inner_region = []  # A 2D slice for grabbing the inner image region\n    for pad in padding_for_kernel(kernel):\n        slice_i = np.s_[:] if pad == 0 else np.s_[pad: -pad]\n        inner_region.append(slice_i)\n    return image # [inner_region] # Broken in numpy 1.24, doesn't seem necessary\n\n# Slice windows\ndef window_slice(center, kernel):\n    r, c = center\n    r_pad, c_pad = padding_for_kernel(kernel)\n    # Slicing is (inclusive, exclusive) so add 1 to the stop value\n    return np.s_[r-r_pad:r+r_pad+1, c-c_pad:c+c_pad+1]\n        \n\n# Apply convolution kernel to image patch\ndef apply_kernel(center, kernel, original_image):\n    image_patch = original_image[window_slice(center, kernel)]\n    # An element-wise multiplication followed by the sum\n    return np.sum(kernel * image_patch)\n\n# Move kernel over the image\ndef iter_kernel_labels(image, kernel):\n    original_image = image\n    image = add_padding(original_image, kernel)\n    i_pad, j_pad = padding_for_kernel(kernel)\n\n    for (i, j), pixel in iter_pixels(original_image):\n        # Shift the center of the kernel to ignore padded border.\n        i += i_pad\n        j += j_pad\n        mask = np.zeros(image.shape, dtype=int)  # Background = 0\n        mask[window_slice((i, j), kernel)] = kernel   # Kernel = 1\n        #mask[i, j] = 2                           # Kernel-center = 2\n        yield (i, j), mask\n\n# Visualize kernel as it moves over the image\ndef visualize_kernel(kernel_labels, image):\n    return kernel_labels + image #color.label2rgb(kernel_labels, image, bg_label=0)\n\ndef convolution_demo(image, kernels, **kwargs):\n    # Dropdown for selecting kernels\n    kernel_names = list(kernels.keys())\n    kernel_selector = Dropdown(options=kernel_names, description='Kernel:')\n    \n    def update_convolution(kernel_name):\n        kernel = kernels[kernel_name]  # Get the selected kernel\n        gen_kernel_labels = iter_kernel_labels(image, kernel)\n        \n        image_cache = []\n        image_padded = add_padding(image, kernel)\n        \n        def convolution_step(i_step=0):\n            while i_step >= len(image_cache):\n                filtered_prev = image_padded if i_step == 0 else image_cache[-1][1]\n                filtered = filtered_prev.copy()\n                \n                center, kernel_labels = next(gen_kernel_labels)\n                filtered[center] = apply_kernel(center, kernel, image_padded)\n                kernel_overlay = visualize_kernel(kernel_labels, image_padded)\n                \n                image_cache.append((kernel_overlay, filtered))\n                \n            image_pair = [remove_padding(each, kernel) for each in image_cache[i_step]]\n            imshow_pair(image_pair, **kwargs)\n            plt.show()\n        \n        interact(convolution_step, i_step=(0, image.size - 1, 1))\n    \n    interact(update_convolution, kernel_name=kernel_selector);\n\n# Full process\ndef convolution_full(ax, image, kernel, **kwargs):\n    # Initialize generator since we're only ever going to iterate over\n    # a pixel once. The cached result is used, if we step back.\n    gen_kernel_labels = iter_kernel_labels(image, kernel)\n\n    image_cache = []\n    image_padded = add_padding(image, kernel)\n    # Plot original image and kernel-overlay next to filtered image.\n\n    for i_step in range(image.size-1):\n\n        # For the first step (`i_step == 0`), the original image is the\n        # filtered image; after that we look in the cache, which stores\n        # (`kernel_overlay`, `filtered`).\n        filtered_prev = image_padded if i_step == 0 else image_cache[-1][1]\n        # We don't want to overwrite the previously filtered image:\n        filtered = filtered_prev.copy()\n\n        # Get the labels used to visualize the kernel\n        center, kernel_labels = next(gen_kernel_labels)\n        # Modify the pixel value at the kernel center\n        filtered[center] = apply_kernel(center, kernel, image_padded)\n        # Take the original image and overlay our kernel visualization\n        kernel_overlay = visualize_kernel(kernel_labels, image_padded)\n        # Save images for reuse.\n        image_cache.append((kernel_overlay, filtered))\n\n    # Remove padding we added to deal with boundary conditions\n    # (Loop since each step has 2 images)\n    image_triple = [remove_padding(each, kernel)\n                  for each in image_cache[i_step]]\n    image_triple.insert(1,kernel)\n    imshow_triple(ax, image_triple, **kwargs)\n\n\n\nDifferent kernels can detect different types of patterns in the image\n\nhorizontal_edge_kernel = np.array([[ 1,  2,  1],\n                                   [ 0,  0,  0],\n                                   [-1, -2, -1]])\ndiagonal_edge_kernel = np.array([[1, 0, 0],\n                                 [0, 1, 0],\n                                 [0, 0, 1]])\nedge_detect_kernel = np.array([[-1, -1, -1],\n                               [-1,  8, -1],\n                               [-1, -1, -1]])\nall_kernels = {\"horizontal\": horizontal_edge_kernel,\n               \"diagonal\": diagonal_edge_kernel,\n               \"edge_detect\":edge_detect_kernel}\n\n\n\nmnist_data = oml.datasets.get_dataset(554) # Download MNIST data\n# Get the predictors X and the labels y\nX_mnist, y_mnist, c, a = mnist_data.get_data(dataset_format='array', target=mnist_data.default_target_attribute); \nimage = X_mnist[1].reshape((28, 28))\nimage = (image - np.min(image))/np.ptp(image) # Normalize\n\nif interactive:\n    titles = ('Image and kernel', 'Filtered image')\n    convolution_demo(image, all_kernels, vmin=-4, vmax=4, titles=titles, cmap='gray_r');\n\n\n\nif not interactive:\n    fig, axs = plt.subplots(3,  3, figsize=(5*fig_scale, 5*fig_scale))\n    titles = ('Image and kernel', 'Hor. edge filter', 'Filtered image')\n    convolution_full(axs[0,:], image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Edge detect filter', 'Filtered image')\n    convolution_full(axs[1,:], image, edge_detect_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Diag. edge filter', 'Filtered image')\n    convolution_full(axs[2,:], image, diagonal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    plt.tight_layout()\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#convolutions","position":5},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Demonstration on Fashion-MNIST","lvl2":"Convolutions"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#demonstration-on-fashion-mnist","position":6},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Demonstration on Fashion-MNIST","lvl2":"Convolutions"},"content":"\n\nfmnist_data = oml.datasets.get_dataset(40996) # Download FMNIST data\n# Get the predictors X and the labels y\nX_fm, y_fm, _, _ = fmnist_data.get_data(dataset_format='array', target=fmnist_data.default_target_attribute)\nfm_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n              6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}\n\n\n\n# build a list of figures for plotting\ndef buildFigureList(fig, subfiglist, titles, length):\n    for i in range(0,length):\n        pixels = np.array(subfiglist[i], dtype='float')\n        pixels = pixels.reshape((28, 28))\n        a=fig.add_subplot(1,length,i+1)\n        imgplot =plt.imshow(pixels, cmap='gray_r')\n        a.set_title(fm_classes[titles[i]], fontsize=6)\n        a.axes.get_xaxis().set_visible(False)\n        a.axes.get_yaxis().set_visible(False)\n    return\n\nsubfiglist = []\ntitles=[]\n\nfor i in range(0,7):\n    subfiglist.append(X_fm[i])\n    titles.append(y_fm[i])\n\nbuildFigureList(plt.figure(1),subfiglist, titles, 7)\nplt.show()\n\n\n\nDemonstration of convolution with edge filters\n\ndef normalize_image(X):\n    image = X.reshape((28, 28))\n    return (image - np.min(image))/np.ptp(image) # Normalize\n\nif interactive:\n    image = normalize_image(X_fm[3])\n    demo2 = convolution_demo(image, all_kernels,\n                             vmin=-4, vmax=4, cmap='gray_r');\n\n\n\nif not interactive:\n    fig, axs = plt.subplots(3, 3, figsize=(5*fig_scale, 5*fig_scale))\n    titles = ('Image and kernel', 'Hor. edge filter', 'Filtered image')\n    convolution_full(axs[0,:], image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Diag. edge filter', 'Filtered image')\n    convolution_full(axs[1,:], image, diagonal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Edge detect filter', 'Filtered image')\n    convolution_full(axs[2,:], image, edge_detect_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    plt.tight_layout()\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#demonstration-on-fashion-mnist","position":7},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Image convolution in practice","lvl2":"Convolutions"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#image-convolution-in-practice","position":8},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Image convolution in practice","lvl2":"Convolutions"},"content":"How do we know which filters are best for a given image?\n\nFamilies of kernels (or filter banks ) can be run on every image\n\nGabor, Sobel, Haar Wavelets,...\n\nGabor filters: Wave patterns generated by changing:\n\nFrequency: narrow or wide ondulations\n\nTheta: angle (direction) of the wave\n\nSigma: resolution (size of the filter)\n\nDemonstration of Gabor filters\n\nfrom scipy import ndimage as ndi\nfrom skimage import data\nfrom skimage.util import img_as_float\nfrom skimage.filters import gabor_kernel\n\n# Gabor Filters\n@interact\ndef demoGabor(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n    plt.gray()\n    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest', extent=[-1, 1, -1, 1])\n    plt.title(f'freq: {round(frequency,2)}, theta: {round(theta,2)}, sigma: {round(sigma,2)}', fontdict={'fontsize':14*fig_scale})\n    plt.xticks([])\n    plt.yticks([])\n    plt.show();\n\n\n\nif not interactive:\n    plt.subplot(1, 3, 1)\n    demoGabor(frequency=0.16, theta=1.2, sigma=4.0)\n    plt.subplot(1, 3, 2)\n    demoGabor(frequency=0.31, theta=0, sigma=3.6)\n    plt.subplot(1, 3, 3)\n    demoGabor(frequency=0.36, theta=1.6, sigma=1.3)\n    plt.tight_layout()\n\n\n\nDemonstration on the Fashion-MNIST data\n\n# Calculate the magnitude of the Gabor filter response given a kernel and an imput image\ndef magnitude(image, kernel):\n    image = (image - image.mean()) / image.std() # Normalize images\n    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)\n\n\n\n@interact\ndef demoGabor2(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n    plt.subplot(131)\n    plt.title('Original', fontdict={'fontsize':24*fig_scale})\n    plt.imshow(image)\n    plt.xticks([])\n    plt.yticks([])\n    plt.subplot(132)\n    plt.title('Gabor kernel', fontdict={'fontsize':24*fig_scale})\n    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest')\n    plt.xticks([])\n    plt.yticks([])\n    plt.subplot(133)\n    plt.title('Response magnitude', fontdict={'fontsize':24*fig_scale})\n    plt.imshow(np.real(magnitude(image, gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))), interpolation='nearest')\n    plt.tight_layout()\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n\n\n\nif not interactive:\n    demoGabor2(frequency=0.16, theta=1.4, sigma=1.2)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#image-convolution-in-practice","position":9},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Filter banks","lvl2":"Convolutions"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#filter-banks","position":10},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Filter banks","lvl2":"Convolutions"},"content":"Different filters detect different edges, shapes,...\n\nNot all seem useful\n\n# More images\n# Fetch some Fashion-MNIST images\nboot = X_fm[0].reshape(28, 28)\nshirt = X_fm[1].reshape(28, 28)\ndress = X_fm[2].reshape(28, 28)\nimage_names = ('boot', 'shirt', 'dress')\nimages = (boot, shirt, dress)\n\ndef plot_filter_bank(images):\n    # Create a set of kernels, apply them to each image, store the results\n    results = []\n    kernel_params = []\n    for theta in (0, 1):\n        theta = theta / 4. * np.pi\n        for frequency in (0.1, 0.2):\n            for sigma in (1, 3):\n                kernel = gabor_kernel(frequency, theta=theta,sigma_x=sigma,sigma_y=sigma)\n                params = 'theta=%.2f,\\nfrequency=%.2f\\nsigma=%.2f' % (theta, frequency, sigma)\n                kernel_params.append(params)\n                results.append((kernel, [magnitude(img, kernel) for img in images]))\n\n    # Plotting\n    fig, axes = plt.subplots(nrows=4, ncols=9, figsize=(14*fig_scale, 8*fig_scale))\n    plt.gray()\n    #fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)\n    axes[0][0].axis('off')\n\n    for label, img, ax in zip(image_names, images, axes[1:]):\n        axs = ax[0]\n        axs.imshow(img)\n        axs.set_ylabel(label, fontsize=12*fig_scale)\n        axs.set_xticks([]) # Remove axis ticks \n        axs.set_yticks([])\n        \n    # Plot Gabor kernel\n    col = 1\n    for label, (kernel, magnitudes), ax_col in zip(kernel_params, results, axes[0][1:]):\n        ax_col.imshow(np.real(kernel), interpolation='nearest') # Plot kernel\n        ax_col.set_title(label, fontsize=10*fig_scale)\n        ax_col.axis('off')\n        \n        # Plot Gabor responses with the contrast normalized for each filter\n        vmin = np.min(magnitudes)\n        vmax = np.max(magnitudes)\n        for patch, ax in zip(magnitudes, axes.T[col][1:]):\n            ax.imshow(patch, vmin=vmin, vmax=vmax) # Plot convolutions\n            ax.axis('off')\n        col += 1\n    \n    plt.show()\n\nplot_filter_bank(images)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#filter-banks","position":11},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Convolutional neural nets"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#convolutional-neural-nets","position":12},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Convolutional neural nets"},"content":"Finding relationships between individual pixels and the correct class is hard\n\nSimplify the problem by decomposing it into smaller problems\n\nFirst, discover ‘local’ patterns (edges, lines, endpoints)\n\nRepresenting such local patterns as features makes it easier to learn from them\n\nDeeper layers will do that for us\n\nWe could use convolutions, but how to choose the filters?\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#convolutional-neural-nets","position":13},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Convolutional Neural Networks (ConvNets)","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#convolutional-neural-networks-convnets","position":14},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Convolutional Neural Networks (ConvNets)","lvl2":"Convolutional neural nets"},"content":"Instead of manually designing the filters, we can also learn them based on data\n\nChoose filter sizes (manually), initialize with small random weights\n\nForward pass: Convolutional layer slides the filter over the input, generates the output\n\nBackward pass: Update the filter weights according to the loss gradients\n\nIllustration for 1 filter:\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#convolutional-neural-networks-convnets","position":15},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Convolutional layers: Feature maps","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#convolutional-layers-feature-maps","position":16},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Convolutional layers: Feature maps","lvl2":"Convolutional neural nets"},"content":"One filter is not sufficient to detect all relevant patterns in an image\n\nA convolutional layer applies and learns d filters in parallel\n\nSlide d filters across the input image (in parallel) -> a (1x1xd) output per patch\n\nReassemble into a feature map with d ‘channels’, a (width x height x d) tensor.\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#convolutional-layers-feature-maps","position":17},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Border effects (zero padding)","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#border-effects-zero-padding","position":18},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Border effects (zero padding)","lvl2":"Convolutional neural nets"},"content":"Consider a 5x5 image and a 3x3 filter: there are only 9 possible locations, hence the output is a 3x3 feature map\n\nIf we want to maintain the image size, we use zero-padding, adding 0’s all around the input tensor. \n\n","type":"content","url":"/notebooks/convolutional-neural-networks#border-effects-zero-padding","position":19},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Undersampling (striding)","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#undersampling-striding","position":20},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Undersampling (striding)","lvl2":"Convolutional neural nets"},"content":"Sometimes, we want to downsample a high-resolution image\n\nFaster processing, less noisy (hence less overfitting)\n\nForces the model to summarize information in (smaller) feature maps\n\nOne approach is to skip values during the convolution\n\nDistance between 2 windows: stride length\n\nExample with stride length 2 (without padding):\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#undersampling-striding","position":21},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Max-pooling","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#max-pooling","position":22},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Max-pooling","lvl2":"Convolutional neural nets"},"content":"Another approach to shrink the input tensors is max-pooling :\n\nRun a filter with a fixed stride length over the image\n\nUsually 2x2 filters and stride lenght 2\n\nThe filter simply returns the max (or avg ) of all values\n\nAgressively reduces the number of weights (less overfitting)\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#max-pooling","position":23},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Receptive field","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#receptive-field","position":24},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Receptive field","lvl2":"Convolutional neural nets"},"content":"Receptive field: how much each output neuron ‘sees’ of the input image\n\nTranslation invariance: shifting the input does not affect the output\n\nLarge receptive field -> neurons can ‘see’ patterns anywhere in the input\n\nnxn convolutions only increase the receptive field by n+2 each layer\n\nMaxpooling doubles the receptive field without deepening the network\n\nimport matplotlib.patches as patches\n\ndef draw_grid(ax, size, offset):\n    \"\"\"Draws a grid without text labels\"\"\"\n    for i in range(size):\n        for j in range(size):\n            ax.add_patch(patches.Rectangle((j + offset[0], -i + offset[1]), 1, 1, \n                                           fill=False, edgecolor='gray', linewidth=1))\n\ndef highlight_region(ax, positions, offset, color, alpha=0.3):\n    \"\"\"Highlights a specific region in the grid\"\"\"\n    for x, y in positions:\n        ax.add_patch(patches.Rectangle((x + offset[0], -y + offset[1]), 1, 1, fill=True, color=color, alpha=alpha))\n\ndef draw_connection_hull(ax, points, color, alpha):\n    \"\"\"Draws a polygon representing the hull of connection lines\"\"\"\n    ax.add_patch(patches.Polygon(points, closed=True, facecolor=color, alpha=alpha, edgecolor=None))\n    \ndef add_titles(ax, option):\n    \"\"\"Adds titles above each matrix\"\"\"\n    titles = [\"Input\", option, \"Output_1\", \"Kernel_2\", \"Output_2\"]\n    positions = [(0, 1.5), (9, 1.5), (15, 1.5), (20, 1.5), (24, 1.5)]\n    \n    for title, (x, y) in zip(titles, positions):\n        ax.text(x, y, title, fontsize=12, fontweight='bold', ha='left')\n\nlayer_options = ['3x3 Kernel', '3x3 Kernel, Stride 2', '5x5 Kernel', 'MaxPool 2x2']\nlayer_options2 = ['3x3 Kernel', '3x3 Kernel, Dilation 2']\n\n@interact\ndef visualize_receptive_field(option=layer_options):\n    fig, ax = plt.subplots(figsize=(18, 6))\n    ax.set_xlim(-2, 26)\n    ax.set_ylim(-9, 2)\n    ax.axis('off')\n    add_titles(ax, option)\n    kernel_size = 0\n    \n    grids = [(8, (0, 0)), (4, (15, 0)), (3, (20, 0)), (2, (24, 0))]\n    \n    single_output_rf = [(0, 0)]\n    for size, offset in grids:\n        draw_grid(ax, size, offset)\n    \n    if option == 'MaxPool 2x2':\n        full_input_rf = [(x, y) for x in range(6) for y in range(6)]\n        highlight_region(ax, full_input_rf, (0, 0), 'green', alpha=0.3)\n    else:\n        kernel_size = 3 if option.startswith('3x3 Kernel') else 5\n        draw_grid(ax, kernel_size, (9, 0))\n        \n        input_highlight_size = kernel_size + 2\n        if option == '3x3 Kernel, Stride 2' or option == '3x3 Kernel, Dilation 2':\n            input_highlight_size = kernel_size + 4\n\n        full_input_rf = [(x, y) for x in range(input_highlight_size) for y in range(input_highlight_size)]\n        kernel_1 = [(x, y) for x in range(kernel_size) for y in range(kernel_size)]\n        kernel_rf = kernel_1\n        if option == '3x3 Kernel, Dilation 2':\n            kernel_rf = [(x*2, y*2) for x in range(kernel_size) for y in range(kernel_size)]\n\n        highlight_region(ax, full_input_rf, (0, 0), 'green')\n        highlight_region(ax, kernel_rf, (0, 0), 'blue')\n        highlight_region(ax, kernel_1, (9, 0), 'blue')\n        highlight_region(ax, single_output_rf, (15, 0), 'blue')\n    \n    kernel2_rf = [(x, y) for x in range(3) for y in range(3)]\n    \n    highlight_region(ax, kernel2_rf, (15, 0), 'green')\n    highlight_region(ax, kernel2_rf, (20, 0), 'green')\n    highlight_region(ax, single_output_rf, (24, 0), 'green')\n    \n    connection_hulls = [\n        ([(23, -2), (23, 1), (24, 1), (24, 0)], 'green', 0.1),\n        ([(18, -2), (18, 1), (20, 1), (20, -2)], 'green', 0.1)\n    ]\n    \n    kernel_fp = kernel_size * 2 - 1 if option == '3x3 Kernel, Dilation 2' else kernel_size\n\n    if option != 'MaxPool 2x2':\n        connection_hulls.extend([\n            ([(kernel_fp, 1-kernel_fp), (kernel_fp, 1), (9, 1), (9, 1-kernel_size)], 'blue', 0.1),\n            ([(9+kernel_size, 1-kernel_size), (9+kernel_size, 1), (15, 1), (15, 0)], 'blue', 0.1)\n        ])\n    else:\n        connection_hulls.extend([\n            ([(6, -5), (6, 1), (15, 1), (15, -2)], 'green', 0.1),\n        ])        \n    \n    for points, color, alpha in connection_hulls:\n        draw_connection_hull(ax, points, color, alpha)\n    \n    plt.show()\n\n\n\nif not interactive:\n    for option in layer_options[0::3]:\n        visualize_receptive_field(option=option)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#receptive-field","position":25},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Dilated convolutions","lvl2":"Convolutional neural nets"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#dilated-convolutions","position":26},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Dilated convolutions","lvl2":"Convolutional neural nets"},"content":"Downsample by introducing ‘gaps’ between filter elements by spacing them out\n\nIncreases the receptive field exponentially\n\nDoesn’t need extra parameters or computation (unlike larger filters)\n\nRetains feature map size (unlike pooling)\n\n@interact\ndef visualize_receptive_field2(option=layer_options2):\n    visualize_receptive_field(option)\n\n\n\nif not interactive:\n    visualize_receptive_field(option=layer_options2[1])\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#dilated-convolutions","position":27},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Convolutional nets in practice"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#convolutional-nets-in-practice","position":28},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Convolutional nets in practice"},"content":"Use multiple convolutional layers to learn patterns at different levels of abstraction\n\nFind local patterns first (e.g. edges), then patterns across those patterns\n\nUse MaxPooling layers to reduce resolution, increase translation invariance\n\nUse sufficient filters in the first layer (otherwise information gets lost)\n\nIn deeper layers, use increasingly more filters\n\nPreserve information about the input as resolution descreases\n\nAvoid decreasing the number of activations (resolution x nr of filters)\n\nFor very deep nets, add skip connections to preserve information (and gradients)\n\nSums up outputs of earlier layers to those of later layers (with same dimensions)\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#convolutional-nets-in-practice","position":29},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Example with PyTorch","lvl2":"Convolutional nets in practice"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#example-with-pytorch","position":30},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Example with PyTorch","lvl2":"Convolutional nets in practice"},"content":"Conv2d for 2D convolutional layers\n\nGrayscale image: 1 in_channels\n\n32 filters: 32 out_channels, 3x3 size\n\nDeeper layers use 64 filters\n\nReLU activation, no padding\n\nMaxPool2d for max-pooling, 2x2model = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n    nn.ReLU()\n)\n\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0),\n    nn.ReLU(), \n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU()\n)\n\n\n\nObserve how the input image on 1x28x28 is transformed to a 64x3x3 feature map\n\nIn pytorch, shapes are (batch_size, channels, height, width)\n\nConv2d parameters = (kernel size^2 × input channels + 1) × output channels\n\nNo zero-padding: every output is 2 pixels less in every dimension\n\nAfter every MaxPooling, resolution halved in every dimension\n\nfrom torchinfo import summary\nsummary(model, input_size=(1, 1, 28, 28))\n\n\n\nTo classify the images, we still need a linear and output layer.\n\nWe flatten the 3x3x64 feature map to a vector of size 576model = nn.Sequential(\n    ...\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(64 * 3 * 3, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10)\n)\n\nmodel = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(64 * 3 * 3, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10)\n)\n\n\n\nComplete model. Flattening adds a lot of weights!\n\nsummary(model, input_size=(1, 1, 28, 28))\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#example-with-pytorch","position":31},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Global Average Pooling (GAP)","lvl2":"Convolutional nets in practice"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#global-average-pooling-gap","position":32},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Global Average Pooling (GAP)","lvl2":"Convolutional nets in practice"},"content":"Instead of flattening, we do GAP: returns average of each activation map\n\nWe can drop the hidden dense layer: number of outputs > number of classesmodel = nn.Sequential(...\n    nn.AdaptiveAvgPool2d(1), # Global Average Pooling\n    nn.Flatten(),            # Convert (batch, 64, 1, 1) -> (batch, 64)\n    nn.Linear(64, 10))       # Output layer for 10 classes\n\nWith GlobalAveragePooling: much fewer weights to learn\n\nUse with caution: this destroys the location information learned by the CNN\n\nNot ideal for tasks such as object localization\n\nmodel = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1),  # Global Average Pooling (GAP)\n    nn.Flatten(),  # Convert (batch, 64, 1, 1) -> (batch, 64)\n    nn.Linear(64, 10)  # Output layer for 10 classes\n)\nsummary(model, input_size=(1, 1, 28, 28))\n\n\n\nRun the model on MNIST dataset\n\nTrain and test as usual: 99% accuracy\n\nCompared to 97,8% accuracy with the dense architecture\n\nFlatten and GlobalAveragePooling yield similar performance\n\nimport pytorch_lightning as pl\n\n# Keeps a history of scores to make plotting easier\nclass MetricTracker(pl.Callback):\n    def __init__(self):\n        super().__init__()\n        self.history = {\n            \"train_loss\": [],\n            \"train_acc\": [],\n            \"val_loss\": [],\n            \"val_acc\": []\n        }\n        self.first_validation = True  # Flag to ignore first validation step\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Collects training metrics at the end of each epoch\"\"\"\n        train_loss = trainer.callback_metrics.get(\"train_loss\")\n        train_acc = trainer.callback_metrics.get(\"train_acc\")\n\n        if train_loss is not None:\n            self.history[\"train_loss\"].append(train_loss.cpu().item())\n        if train_acc is not None:\n            self.history[\"train_acc\"].append(train_acc.cpu().item())\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Collects validation metrics at the end of each epoch\"\"\"\n        if self.first_validation:  \n            self.first_validation = False  # Skip first validation logging\n            return  \n\n        val_loss = trainer.callback_metrics.get(\"val_loss\")\n        val_acc = trainer.callback_metrics.get(\"val_acc\")\n\n        if val_loss is not None:\n            self.history[\"val_loss\"].append(val_loss.cpu().item())\n        if val_acc is not None:\n            self.history[\"val_acc\"].append(val_acc.cpu().item())\n            \ndef plot_training(history):\n    plt.figure(figsize=(12, 4))  # Increased figure size\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history[\"train_loss\"], label=\"Train Loss\", marker='o', lw=2)\n    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker='o', lw=2)\n    plt.xlabel(\"Epochs\", fontsize=14)  # Larger font size\n    plt.ylabel(\"Loss\", fontsize=14)\n    plt.title(\"Loss vs. Epochs\", fontsize=16, fontweight=\"bold\")\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12)\n\n    # Plot Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history[\"train_acc\"], label=\"Train Accuracy\", marker='o', lw=2)\n    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\", marker='o', lw=2)\n    plt.xlabel(\"Epochs\", fontsize=14)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.title(\"Accuracy vs. Epochs\", fontsize=16, fontweight=\"bold\")\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12)\n\n    plt.tight_layout()  # Adjust layout for readability\n    plt.show()\n\n\n\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics.functional import accuracy\n\n# Model in Pytorch Lightning\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, learning_rate=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(64, 10)\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    # Logging of loss and accuracy for later plotting\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = accuracy(logits, y, task=\"multiclass\", num_classes=10)\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = accuracy(logits, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n# Compute mean and std to normalize the data\n# Couldn't find a way to do this automatically in PyTorch :(\n# Normalization is not strictly needed, but speeds up convergence\ndataset = datasets.MNIST(root=\".\", train=True, transform=transforms.ToTensor(), download=True)\nloader = torch.utils.data.DataLoader(dataset, batch_size=1000, num_workers=4, shuffle=False)\nmean = torch.mean(torch.stack([batch[0].mean() for batch in loader]))\nstd = torch.mean(torch.stack([batch[0].std() for batch in loader]))\n\n# Loading the data. We'll discuss data loaders again soon.\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((mean,), (std,))  # Normalize MNIST. Make more general?\n        ])\n\n    def prepare_data(self):\n        datasets.MNIST(root=\".\", train=True, download=True)  # Downloads dataset\n\n    def setup(self, stage=None):\n        full_train = datasets.MNIST(root=\".\", train=True, transform=self.transform)\n        self.train, self.val = random_split(full_train, [55000, 5000])\n        self.test = datasets.MNIST(root=\".\", train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val, batch_size=self.batch_size, num_workers=4)\n\n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=self.batch_size, num_workers=4)\n\n    \n# Initialize data & model\npl.seed_everything(42)  # Ensure reproducibility\ndata_module = MNISTDataModule(batch_size=64)\nmodel = MNISTModel(learning_rate=0.001)\n\n# Trainer with logging & checkpointing\naccelerator = \"cpu\"\nif torch.backends.mps.is_available():\n    accelerator = \"mps\"\nif torch.cuda.is_available():\n    accelerator = \"gpu\"\n\nmetric_tracker = MetricTracker()  # Callback to track per-epoch metrics\n\ntrainer = pl.Trainer(\n    max_epochs=10,  # Train for 10 epochs\n    accelerator=accelerator,\n    devices=\"auto\",\n    log_every_n_steps=10,\n    deterministic=True,\n    callbacks=[metric_tracker]  # Attach callback to trainer\n)\n\nif histories and histories[\"mnist\"]:\n    history = histories[\"mnist\"]\nelse:\n    trainer.fit(model, datamodule=data_module)\n    history = metric_tracker.history\n\n# Test after training (sanity check)\n# trainer.test(model, datamodule=data_module)\n\n\n\nplot_training(history)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#global-average-pooling-gap","position":33},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Cats vs Dogs"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#cats-vs-dogs","position":34},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Cats vs Dogs"},"content":"A more realistic dataset: \n\nCats vs Dogs\n\nColored JPEG images, different sizes\n\nNot nicely centered, translation invariance is important\n\nPreprocessing\n\nDecode JPEG images to floating-point tensors\n\nRescale pixel values to [0,1]\n\nResize images to 150x150 pixels\n\nUncomment to run from scratch# TODO: upload dataset to OpenML so we can avoid the manual steps.\n\nimport os, shutil \n# Download data from https://www.kaggle.com/c/dogs-vs-cats/data\n# Uncompress `train.zip` into the `original_dataset_dir`\noriginal_dataset_dir = '../data/cats-vs-dogs_original'\n\n# The directory where we will\n# store our smaller dataset\ntrain_dir = os.path.join(data_dir, 'train')\nvalidation_dir = os.path.join(data_dir, 'validation')\n\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\n    os.mkdir(train_dir)\n    os.mkdir(validation_dir)\n    \ntrain_cats_dir = os.path.join(train_dir, 'cats')\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\n\nif not os.path.exists(train_cats_dir):\n    os.mkdir(train_cats_dir)\n    os.mkdir(train_dogs_dir)\n    os.mkdir(validation_cats_dir)\n    os.mkdir(validation_dogs_dir)\n\n# Copy first 2000 cat images to train_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# Copy next 1000 cat images to validation_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(2000, 3000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# Copy first 2000 dog images to train_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# Copy next 1000 dog images to validation_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(2000, 3000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n\nimport random\n\n# Set random seed for reproducibility\ndef seed_everything(seed=42):\n    pl.seed_everything(seed)  # Sets seed for PyTorch Lightning\n    torch.manual_seed(seed)  # PyTorch\n    torch.cuda.manual_seed_all(seed)  # CUDA (if available)\n    np.random.seed(seed)  # NumPy\n    random.seed(seed)  # Python random module\n    torch.backends.cudnn.deterministic = True  # Ensures reproducibility in CNNs\n    torch.backends.cudnn.benchmark = False  # Ensures consistency\n\nseed_everything(42)  # Set global seed\n\nclass CatDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size=20, img_size=(150, 150)):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_size = img_size\n\n        # Define image transformations\n        self.transform = transforms.Compose([\n            transforms.Resize(self.img_size),  # Resize to 150x150\n            transforms.ToTensor(),  # Convert to tensor (also scales 0-1)\n        ])\n\n    def setup(self, stage=None):\n        \"\"\"Load datasets\"\"\"\n        train_dir = os.path.join(self.data_dir, \"train\")\n        val_dir = os.path.join(self.data_dir, \"validation\")\n\n        self.train_dataset = datasets.ImageFolder(root=train_dir, transform=self.transform)\n        self.val_dataset = datasets.ImageFolder(root=val_dir, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n\n# ----------------------------\n# Load dataset and visualize a batch\n# ----------------------------\ndata_module = CatDataModule(data_dir=data_dir)\ndata_module.setup()\ntrain_loader = data_module.train_dataloader()\n\n\n\n# Get a batch of data\ndata_batch, labels_batch = next(iter(train_loader))\n\n# Visualize images\nplt.figure(figsize=(10, 5))\nfor i in range(7):\n    plt.subplot(1, 7, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(data_batch[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n    plt.title(\"Cat\" if labels_batch[i] == 0 else \"Dog\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#cats-vs-dogs","position":35},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Data loader","lvl2":"Cats vs Dogs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#data-loader","position":36},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Data loader","lvl2":"Cats vs Dogs"},"content":"We create a Pytorch Lightning DataModule to do preprocessing and data loadingclass ImageDataModule(pl.LightningDataModule):\n  def __init__(self, data_dir, batch_size=20, img_size=(150, 150)):\n    super().__init__()\n    self.transform = transforms.Compose([\n      transforms.Resize(self.img_size),  # Resize to 150x150\n      transforms.ToTensor()])  # Convert to tensor (also scales 0-1)\n  def setup(self, stage=None):\n    self.train_dataset = datasets.ImageFolder(root=train_dir, transform=self.transform)\n    self.val_dataset = datasets.ImageFolder(root=val_dir, transform=self.transform)  def train_dataloader(self):\n    return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n  def val_dataloader(self):\n    return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n\nfrom torchmetrics.classification import Accuracy\n\n# Model in PyTorch Lightning\nclass CatImageClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Define convolutional layers\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(128, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.AdaptiveAvgPool2d(1)  # GAP replaces Flatten()\n        )\n\n        # Fully connected layers\n        self.fc_layers = nn.Sequential(\n            nn.Linear(128, 512),  # GAP outputs (batch, 128, 1, 1) → Flatten to (batch, 128)\n            nn.ReLU(),\n            nn.Linear(512, 1)  # Binary classification (1 output neuron)\n        )\n\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = Accuracy(task=\"binary\")\n\n    def forward(self, x):\n        x = self.conv_layers(x)  # Convolutions + GAP\n        x = x.view(x.size(0), -1)  # Flatten from (batch, 128, 1, 1) → (batch, 128)\n        x = self.fc_layers(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)  # Remove extra dimension\n        loss = self.loss_fn(logits, y.float())  # BCE loss requires float labels\n\n        preds = torch.sigmoid(logits)  # Convert logits to probabilities\n        acc = self.accuracy(preds, y)\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)\n        loss = self.loss_fn(logits, y.float())\n\n        preds = torch.sigmoid(logits)\n        acc = self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#data-loader","position":37},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Model","lvl2":"Cats vs Dogs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#model","position":38},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Model","lvl2":"Cats vs Dogs"},"content":"Since the images are more complex, we add another convolutional layer and increase the number of filters to 128.\n\nmodel = CatImageClassifier()\nsummary(model, input_size=(1, 3, 150, 150))\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#model","position":39},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Training","lvl2":"Cats vs Dogs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#training","position":40},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Training","lvl2":"Cats vs Dogs"},"content":"We use a Trainer module (from PyTorch Lightning) to simplify trainingtrainer = pl.Trainer(\n    max_epochs=20,        # Train for 20 epochs\n    accelerator=\"gpu\",    # Move data and model to GPU\n    devices=\"auto\",       # Number of GPUs\n    deterministic=True,   # Set random seeds, for reproducibility\n    callbacks=[metric_tracker,      # Callback for logging loss and acc\n               checkpoint_callback] # Callback for logging weights\n)\ntrainer.fit(model, datamodule=data_module)\n\nTip: to store the best model weights, you can add a ModelCheckpoint callbackcheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",   # Save model with lowest val. loss\n    mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n    save_top_k=1,         # Keep only the best model\n    dirpath=\"weights/\",   # Directory to save checkpoints\n    filename=\"cat_model\", # File name pattern\n)\n\nThe model learns well for the first 20 epochs, but then starts overfitting a lot!\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# Train Cat model\npl.seed_everything(42)  # Ensure reproducibility\ndata_module = CatDataModule(data_dir, batch_size=64)\nmodel = CatImageClassifier(learning_rate=0.001)\nmetric_tracker = MetricTracker()  # Callback to track per-epoch metrics\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# Define checkpoint callback to save the best model\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",  # Saves model with lowest validation loss\n    mode=\"min\",  # \"min\" for loss, \"max\" for accuracy\n    save_top_k=1,  # Keep only the best model\n    dirpath=\"../data/checkpoints/\",  # Directory to save checkpoints\n    filename=\"cat_model\",  # File name pattern\n)\n\ntrainer = pl.Trainer(\n    max_epochs=50,  # Train for 20 epochs\n    accelerator=accelerator,\n    devices=\"auto\",\n    log_every_n_steps=10,\n    deterministic=True,\n    callbacks=[metric_tracker, checkpoint_callback]  # Attach callback to trainer\n)\n\nif histories and histories[\"cat\"]:\n    history_cat = histories[\"cat\"]\nelse:\n    trainer.fit(model, datamodule=data_module)\n    history_cat = metric_tracker.history\n\n\n\nplot_training(history_cat)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#training","position":41},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Solving overfitting in CNNs","lvl2":"Cats vs Dogs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#solving-overfitting-in-cnns","position":42},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Solving overfitting in CNNs","lvl2":"Cats vs Dogs"},"content":"There are various ways to further improve the model:\n\nGenerating more training data (data augmentation)\n\nRegularization (e.g. Dropout, L1/L2, Batch Normalization,...)\n\nUse pretrained rather than randomly initialized filters\n\nThese are trained on a lot more data\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#solving-overfitting-in-cnns","position":43},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Data augmentation","lvl2":"Cats vs Dogs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#data-augmentation","position":44},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Data augmentation","lvl2":"Cats vs Dogs"},"content":"Generate new images via image transformations (only on training data!)\n\nImages will be randomly transformed every epoch\n\nUpdate the transform in the data moduleself.train_transform = transforms.Compose([\n    transforms.Resize(self.img_size), # Resize to 150x150\n    transforms.RandomRotation(40),    # Rotations up to 40 degrees\n    transforms.RandomResizedCrop(self.img_size, \n                                 scale=(0.8, 1.2)), # Scale + crop, up to 20%\n    transforms.RandomHorizontalFlip(),              # Horizontal flip\n    transforms.RandomAffine(degrees=0, shear=20),   # Shear, up to 20%\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, \n                           saturation=0.2),         # Color jitter    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])\n\nclass CatDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size=20, img_size=(150, 150)):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_size = img_size\n\n        # Training Data Augmentation \n        self.train_transform = transforms.Compose([\n            transforms.Resize(self.img_size),\n            transforms.RandomRotation(40),\n            transforms.RandomResizedCrop(self.img_size, scale=(0.8, 1.2)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomAffine(degrees=0, shear=20),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n        # Test Data Transforms (NO augmentation, just resize + normalize)\n        self.val_transform = transforms.Compose([\n            transforms.Resize(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n    def setup(self, stage=None):\n        \"\"\"Load datasets with correct transforms\"\"\"\n        train_dir = os.path.join(self.data_dir, \"train\")\n        val_dir = os.path.join(self.data_dir, \"validation\")\n\n        # Apply augmentation only to training data\n        self.train_dataset = datasets.ImageFolder(root=train_dir, transform=self.train_transform)\n        self.val_dataset = datasets.ImageFolder(root=val_dir, transform=self.val_transform)\n\n    def train_dataloader(self):\n        \"\"\"Applies augmentation via the pre-defined transform\"\"\"\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        \"\"\"Loads validation data WITHOUT augmentation\"\"\"\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n\n\n\nAugmentation example\n\ndef show_augmented_images(data_module, num_images=8):\n    \"\"\"Visualize the same image with different random augmentations.\"\"\"\n    \n    train_dataset = data_module.train_dataset  # Get training dataset with augmentation\n    \n    # Select a random image (without augmentation)\n    idx = np.random.randint(len(train_dataset))\n    original_img, label = train_dataset[idx]  # This is already augmented\n\n    # Convert original image back to NumPy format\n    original_img_np = original_img.permute(1, 2, 0).numpy()  # Convert (C, H, W) → (H, W, C)\n    original_img_np = (original_img_np - original_img_np.min()) / (original_img_np.max() - original_img_np.min())  # Normalize\n\n    fig, axes = plt.subplots(2, 4, figsize=(10, 5))  # Create 4x2 grid\n    axes = axes.flatten()\n\n    for i in range(num_images):\n        # Apply new augmentation on the same image each time\n        img, _ = train_dataset[idx]  # Re-fetch the same image, but with a new random augmentation\n        \n        # Convert tensor image back to NumPy format\n        img = img.permute(1, 2, 0).numpy()  # Convert (C, H, W) → (H, W, C)\n        img = (img - img.min()) / (img.max() - img.min())  # Normalize\n\n        # Plot the augmented image\n        axes[i].imshow(img)\n        axes[i].set_xticks([])\n        axes[i].set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n\n# Load dataset and visualize augmented images\ndata_module = CatDataModule(data_dir)  # Set correct dataset path\ndata_module.setup()\ncat_data_module = data_module\nshow_augmented_images(data_module)\n\n\n\nWe also add Dropout before the Dense layer, and L2 regularization (‘weight decay’) in Adam\n\nclass CatImageClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Define convolutional layers (CNN)\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(128, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.AdaptiveAvgPool2d(1)  # GAP instead of Flatten\n        )\n\n        # Fully connected layers (FC) with Dropout\n        self.fc_layers = nn.Sequential(\n            nn.Linear(128, 512),  # GAP outputs (batch, 128, 1, 1) → Flatten to (batch, 128)\n            nn.ReLU(),\n            nn.Dropout(0.5),  # Dropout (same as Keras Dropout(0.5))\n            nn.Linear(512, 1)  # Binary classification (1 output neuron)\n        )\n\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = Accuracy(task=\"binary\")\n\n    def forward(self, x):\n        x = self.conv_layers(x)  # Convolutions + GAP\n        x = x.view(x.size(0), -1)  # Flatten from (batch, 128, 1, 1) → (batch, 128)\n        x = self.fc_layers(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)  # Remove extra dimension\n        loss = self.loss_fn(logits, y.float())  # BCE loss requires float labels\n\n        preds = torch.sigmoid(logits)  # Convert logits to probabilities\n        acc = self.accuracy(preds, y)\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)\n        loss = self.loss_fn(logits, y.float())\n\n        preds = torch.sigmoid(logits)\n        acc = self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-4)\n    \nmodel = CatImageClassifier()\nsummary(model, input_size=(1, 3, 150, 150))\n\n\n\nNo more overfitting!\n\npl.seed_everything(42)  # Ensure reproducibility\ndata_module = CatDataModule(data_dir, batch_size=64)\nmodel = CatImageClassifier(learning_rate=0.001)\nmetric_tracker = MetricTracker()  # Callback to track per-epoch metrics\n\ntrainer = pl.Trainer(\n    max_epochs=50,  # Train for 20 epochs\n    accelerator=accelerator,\n    devices=\"auto\",\n    log_every_n_steps=10,\n    deterministic=True,\n    callbacks=[metric_tracker, checkpoint_callback]  # Attach callback to trainer\n)\n\n# If previously trained, load history and weights\nif histories and histories[\"cat2\"]:\n    history_cat2 = histories[\"cat2\"]\n    model = CatImageClassifier.load_from_checkpoint(\"../data/checkpoints/cat_model.ckpt\")\nelse:\n    trainer.fit(model, datamodule=data_module)\n    history_cat2 = metric_tracker.history\n    \n# Set to evaluation mode so we don't update the weights\nmodel.eval()\n\n\n\n\n\nhistory_cat2 = histories[\"cat2\"]\ncat_model = CatImageClassifier.load_from_checkpoint(\"../data/checkpoints/cat_model.ckpt\")\n\n\n\nplot_training(history_cat2)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#data-augmentation","position":45},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Real-world CNNs"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#real-world-cnns","position":46},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Real-world CNNs"},"content":"","type":"content","url":"/notebooks/convolutional-neural-networks#real-world-cnns","position":47},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"VGG16","lvl2":"Real-world CNNs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#vgg16","position":48},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"VGG16","lvl2":"Real-world CNNs"},"content":"Deeper architecture (16 layers): allows it to learn more complex high-level features\n\nTextures, patterns, shapes,...\n\nSmall filters (3x3) work better: capture spatial information while reducing number of parameters\n\nMax-pooling (2x2): reduces spatial dimension, improves translation invariance\n\nLower resolution forces model to learn robust features (less sensitive to small input changes)\n\nOnly after every 2 layers, otherwise dimensions reduce too fast\n\nDownside: too many parameters, expensive to train\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#vgg16","position":49},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Inceptionv3","lvl2":"Real-world CNNs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#inceptionv3","position":50},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Inceptionv3","lvl2":"Real-world CNNs"},"content":"Inception modules: parallel branches learn features of different sizes and scales (3x3, 5x5, 7x7,...)\n\nAdd reduction blocks that reduce dimensionality via convolutions with stride 2\n\nFactorized convolutions: a 3x3 conv. can be replaced by combining 1x3 and 3x1, and is 33% cheaper\n\nA 5x5 can be replaced by combining 3x3 and 3x3, which can in turn be factorized as above\n\n1x1 convolutions, or Network-In-Network (NIN) layers help reduce the number of channels: cheaper\n\nAn auxiliary classifier adds an additional gradient signal deeper in the network\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#inceptionv3","position":51},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl4":"Factorized convolutions","lvl3":"Inceptionv3","lvl2":"Real-world CNNs"},"type":"lvl4","url":"/notebooks/convolutional-neural-networks#factorized-convolutions","position":52},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl4":"Factorized convolutions","lvl3":"Inceptionv3","lvl2":"Real-world CNNs"},"content":"A 3x3 conv. can be replaced by combining 1x3 and 3x1, and is 33% cheaper\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#factorized-convolutions","position":53},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"ResNet50","lvl2":"Real-world CNNs"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#resnet50","position":54},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"ResNet50","lvl2":"Real-world CNNs"},"content":"Residual (skip) connections: add earlier feature map to a later one (dimensions must match)\n\nInformation can bypass layers, reduces vanishing gradients, allows much deeper nets\n\nResidual blocks: skip small number or layers and repeat many times\n\nMatch dimensions though padding and 1x1 convolutions\n\nWhen resolution drops, add 1x1 convolutions with stride 2\n\nCan be combined with Inception blocks\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#resnet50","position":55},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Interpreting the model"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#interpreting-the-model","position":56},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Interpreting the model"},"content":"Let’s see what the convnet is learning exactly by observing the intermediate feature maps\n\nWe can do this easily by attaching a ‘hook’ to a layer so we can read it’s output (activation)# Create a hook to send outputs to a global variable (activation)\ndef hook_fn(module, input, output): \n    nonlocal activation\n    activation = output.detach()\n    \n# Add a hook to a specific layer\nhook = model.features[layer_id].register_forward_hook(hook_fn)# Do a forward pass without gradient computation\nwith torch.no_grad(): \n    model(image_tensor) \n\n# Access the global variable\nreturn activation\n\nResult for a specific filter (Layer 0, Filter 0)\n\nfrom PIL import Image\nimport os\n\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\ndef load_image(img_path, img_size=(150, 150)):\n    \"\"\"Load and preprocess image as a PyTorch tensor.\"\"\"\n    transform = transforms.Compose([\n        transforms.Resize(img_size),\n        transforms.ToTensor(),  # Converts image to tensor with values in [0,1]\n    ])\n    \n    img = Image.open(img_path).convert(\"RGB\")  # Ensure RGB format\n    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n    return img_tensor\n\ndef get_layer_activations(model, img_tensor, layer_idx=0, keep_gradients=False):\n    \"\"\"Extract activations from a specific layer.\"\"\"\n    activation = None\n    \n    def hook_fn(module, input, output):\n        nonlocal activation\n        if keep_gradients: # Only for gradient ascent (later)\n            activation = output\n        else:\n            activation = output.detach()\n\n    # Register hook to capture the activation\n    # Handles our custom model and more general models like VGG\n    layer = model.conv_layers[layer_idx] if hasattr(model, \"conv_layers\") else model[layer_idx]\n    hook = layer.register_forward_hook(hook_fn)    \n    \n    if keep_gradients:\n        model(img_tensor)  # Run the image through the model\n    else:\n        with torch.no_grad():\n            model(img_tensor)  # Idem but no grad\n    \n    hook.remove()  # Remove the hook after getting activations\n    return activation\n\ndef visualize_activations(model, img_tensor, layer_idx=0, filter_idx=0):\n    \"\"\"Visualize input image and activations of a selected filter.\"\"\"\n\n    # Get activations from the specified layer\n    activations = get_layer_activations(model, img_tensor, layer_idx)\n\n    # Convert activations to numpy for visualization\n    activation_np = activations.squeeze(0).cpu().numpy()  # Remove batch dim\n    \n    # Show input image\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4, 2))\n    \n    # Convert input tensor to NumPy\n    img_np = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()  # (H, W, C)\n    img_np = np.clip(img_np, 0, 1)  # Ensure values are in range [0,1]\n    \n    ax1.imshow(img_np)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n    ax1.set_xlabel(\"Input Image\", fontsize=8)\n    \n    # Visualize a specific filter's activation\n    ax2.imshow(activation_np[filter_idx], cmap=\"viridis\")\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    ax2.set_xlabel(f\"Activation of Filter {filter_idx}\", fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n# Load model and visualize activations\nimg_path = os.path.join(data_dir, \"train/cats/cat.1700.jpg\")  # Update path\nimg_tensor = load_image(img_path).to(accelerator)\n\nvisualize_activations(cat_model, img_tensor, layer_idx=0, filter_idx=0)\n\n\n\nThe same filter will highlight the same patterns in other inputs.\n\nimg_path_dog = os.path.join(data_dir, \"train/dogs/dog.1528.jpg\")\nimg_tensor_dog = load_image(img_path_dog).to(accelerator)\n\nvisualize_activations(cat_model, img_tensor_dog, layer_idx=0, filter_idx=0)\n\n\n\ndef visualize_all_filters(model, img_tensor, layer_idx=0, max_per_row=16):\n    \"\"\"Visualize all filters of a given layer as a grid of feature maps.\"\"\"\n    activations = get_layer_activations(model, img_tensor, layer_idx)\n    activation_np = activations.squeeze(0).cpu().numpy()\n    \n    num_filters = activation_np.shape[0]\n    num_cols = min(num_filters, max_per_row)\n    num_rows = (num_filters + num_cols - 1) // num_cols  # Ceiling division\n    \n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols, num_rows))\n    axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure it's a 2D array\n    \n    for i in range(num_rows * num_cols):\n        ax = axes[i // num_cols, i % num_cols]\n        \n        if i < num_filters:\n            ax.imshow(activation_np[i], cmap=\"viridis\")\n        \n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    plt.suptitle(f\"Activations of Layer {layer_idx}\", fontsize=16, y=1.0)\n    plt.tight_layout()\n    plt.show()\n\n\n\nAll filters for the first 2 convolutional layers: edges, colors, simple shapes\n\nEmpty filter activations occur:\n\nFilter is not interested in that input image (maybe it’s dog-specific)\n\nIncomplete training, Dying ReLU,...\n\nvisualize_all_filters(cat_model, img_tensor, layer_idx=0)\nvisualize_all_filters(cat_model, img_tensor, layer_idx=3)\n\n\n\n\n\n3rd convolutional layer: increasingly abstract: ears, nose, eyes\n\nvisualize_all_filters(cat_model, img_tensor, layer_idx=6)\n\n\n\nLast convolutional layer: more abstract patterns\n\nEach filter combines information from all filters in previous layer\n\nvisualize_all_filters(cat_model, img_tensor, layer_idx=9)\n\n\n\nSame layer, with dog image input: some filters react only to dogs or cats\n\nDeeper layers learn representations that separate the classes\n\nvisualize_all_filters(cat_model, img_tensor_dog, layer_idx=9)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#interpreting-the-model","position":57},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Spatial hierarchies","lvl2":"Interpreting the model"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#spatial-hierarchies","position":58},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Spatial hierarchies","lvl2":"Interpreting the model"},"content":"Deep convnets can learn spatial hierarchies of patterns\n\nFirst layer can learn very local patterns (e.g. edges)\n\nSecond layer can learn specific combinations of patterns\n\nEvery layer can learn increasingly complex abstractions\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#spatial-hierarchies","position":59},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Visualizing the learned filters","lvl2":"Interpreting the model"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#visualizing-the-learned-filters","position":60},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Visualizing the learned filters","lvl2":"Interpreting the model"},"content":"Visualize filters by finding the input image that they are maximally responsive to\n\nGradient ascent in input space: learn what input maximizes the activations for that filter\n\nStart from a random input image X, freeze the kernel\n\nLoss = mean activation of output layer A, backpropagate to optimize X\n\nX_{(i+1)} = X_{(i)} + \\frac{\\partial L(x, X_{(i)})}{\\partial X} * \\eta\n\nfrom scipy.signal import convolve2d\n\nimport random\n\n# Function to generate green color shades\ndef generate_shades(size, randomness, brightness, striped=False):\n    matrix = np.zeros((size, size))\n    for i in range(size):\n        for j in range(size):\n            base_shade = max(0, min(1, brightness + randomness * random.uniform(-1, 1)))\n            if striped and i % 2 == 0:\n                matrix[i, j] = base_shade  # Brighter green for even rows\n            else:\n                matrix[i, j] = 0.5 * base_shade  # Dimmer green or normal shade\n                \n    return matrix\n\n# Function to highlight regions with green shades\ndef highlight_region_matrix(ax, cells, offset, color_matrix):\n    for (x, y) in cells:\n        color_value = color_matrix[y, x]  # Get green intensity value\n        color = (0, color_value, 0)  # Convert to RGB (only green channel)\n        ax.add_patch(plt.Rectangle((offset[0] + x, offset[1] + y), 1, 1, color=color, ec='black', lw=0.5))\n\n@interact\ndef visualize_gradient_ascent(step=(1, 100, 1)):\n    fig, ax = plt.subplots(figsize=(18, 6))\n    ax.set_xlim(-2, 26)\n    ax.set_ylim(-9, 2)\n    ax.axis('off')\n\n    # Define grid sizes and positions\n    grids = [(6, (0, 0)), (3, (9, 0)), (4, (15, 0))]\n\n    # Adjust randomness and brightness based on step\n    input_randomness = 1 / step\n    input_brightness = 0.5 + 0.5 * (step / 100)\n\n    # Generate color matrices (single green intensity values)\n    input_colors = generate_shades(6, input_randomness, input_brightness, striped=True)\n    kernel_colors = np.array([[0.25, 0.5, 0.25],[0.5, 1.0, 0.5],[0.25, 0.5, 0.25]])  \n    kernel_colors = kernel_colors / np.sum(kernel_colors)\n    output_colors = convolve2d(input_colors, kernel_colors,  mode='valid') # Convolution\n    kernel_colors = (kernel_colors - kernel_colors.min()) / (kernel_colors.max() - kernel_colors.min())\n\n    # Draw grids\n    for size, offset in grids:\n        draw_grid(ax, size, offset)\n\n    # Highlight regions with shades\n    highlight_region_matrix(ax, [(x, y) for x in range(6) for y in range(6)], (0, -5), input_colors)\n    highlight_region_matrix(ax, [(x, y) for x in range(3) for y in range(3)], (9, -2), kernel_colors)\n    highlight_region_matrix(ax, [(x, y) for x in range(4) for y in range(4)], (15, -3), output_colors)\n\n    # Titles\n    titles = [\"Input X\", \"Kernel (Frozen)\", \"Activations A\"]\n    positions = [(0, 1.5), (9, 1.5), (15, 1.5)]\n    for title, (x, y) in zip(titles, positions):\n        ax.text(x, y, title, fontsize=12, fontweight=\"bold\", ha=\"left\")\n\n    plt.show()\n\n\n\nVisualization: initialization (top) and after 100 optimization steps (bottom)\n\nInput image will show patterns that the filter responds to most\n\nif not interactive:\n    visualize_gradient_ascent(1)\n    visualize_gradient_ascent(100)\n\n\n\nGradient Ascent in input space in PyTorch\n\n# Create a random input tensor and tell Adam to optimize the pixels\nimg = np.random.uniform(150, 180, (sz, sz, 3)) / 255\nimg_tensor = torch.from_numpy(img.transpose(2, 0, 1)).to(self.device)\nimg_tensor.requires_grad_()\noptimizer = optim.Adam([img_tensor], lr=lr, weight_decay=1e-6)for _ in range(steps): \n    # Add our hook on the layer of interest to get the activations\n    hook = layer.register_forward_hook(hook_fn) \n    \n    # Run the input through the model\n    model(img_tensor)     # Loss = Avg Activation of specific filter\n    loss = -activations[0, filter_idx].mean()\n    \n    # Update inputs to maximize activation\n    loss.backward() \n    optimizer.step()\n\nimport cv2\n\nclass FilterVisualizer():\n    def __init__(self, model, size=56, upscaling_steps=12, upscaling_factor=1.2, device=None):\n        self.size = size\n        self.upscaling_steps = upscaling_steps\n        self.upscaling_factor = upscaling_factor\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = model.features if hasattr(model, 'features') else model\n        self.model = self.model.to(self.device).eval()\n        self.hook = None\n        self.activations = None\n\n        # Get indices of all Conv2d layers\n        self.conv_layers = [layer for layer in self.model.modules() if isinstance(layer, nn.Conv2d)]\n\n    def hook_fn(self, module, input, output):\n        self.activations = output\n\n    def register_hook(self, conv_layer_index):\n        if self.hook is not None:\n            self.hook.remove()\n        layer = self.conv_layers[conv_layer_index]\n        self.hook = layer.register_forward_hook(self.hook_fn)\n\n    def visualize(self, conv_layer_index, filter_idx, lr=0.1, opt_steps=20, blur=None):\n        sz = self.size\n        img = np.random.uniform(150, 180, (sz, sz, 3)) / 255  # Random noise image\n\n        self.register_hook(conv_layer_index)  # Attach hook\n\n        for _ in range(self.upscaling_steps):  # Iteratively upscale\n            img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).unsqueeze(0).float().to(self.device)\n            img_tensor.requires_grad_()\n            optimizer = optim.Adam([img_tensor], lr=lr, weight_decay=1e-6)\n\n            for _ in range(opt_steps):  # Optimize pixel values\n                optimizer.zero_grad()\n                self.model(img_tensor)\n                loss = -self.activations[0, filter_idx].mean()\n                loss.backward()\n                optimizer.step()\n\n            img = img_tensor.detach().cpu().numpy()[0].transpose(1, 2, 0)\n            self.output = img\n            sz = int(self.upscaling_factor * sz)  # Increase image size\n            img = cv2.resize(img, (sz, sz), interpolation=cv2.INTER_CUBIC)  # Upscale image\n            if blur is not None:\n                img = cv2.GaussianBlur(img, (blur, blur), 0)  # Apply blur to reduce noise\n\n        self.hook.remove()  # Remove hook after use\n        return self.output\n\n    def visualize_filters(self, conv_layer_index, num_filters=None, blur=None, filters=None):\n        filter_images = []\n\n        if filters:\n            for filter_idx in filters:\n                img = self.visualize(conv_layer_index, filter_idx, blur=blur)\n                filter_images.append(img)\n            num_filters = len(filters)\n        else:\n            # Visualize first to get activations and number of filters\n            img = self.visualize(conv_layer_index, 0, blur=blur)\n            filter_images.append(img)\n            if self.activations is not None:\n                total_filters = self.activations.shape[1]\n                num_filters = num_filters or total_filters\n                for filter_idx in range(1, num_filters):\n                    img = self.visualize(conv_layer_index, filter_idx, blur=blur)\n                    filter_images.append(img)\n            else:\n                raise RuntimeError(\"Failed to get layer activations.\")\n\n        self.show_filters(filter_images, num_filters, conv_layer_index)\n\n    def show_filters(self, filter_images, num_filters, layer_id):\n        cols = min(10, num_filters)  # Limit to max 10 columns\n        rows = (num_filters // cols) + int(num_filters % cols > 0)\n\n        fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n        axes = np.array(axes).flatten()  # Flatten in case of single row/col\n\n        for i, img in enumerate(filter_images):\n            axes[i].imshow(np.clip(img, 0, 1))\n            axes[i].axis('off')\n\n        # Remove empty subplots\n        for i in range(len(filter_images), len(axes)):\n            fig.delaxes(axes[i])\n\n        fig.subplots_adjust(wspace=0, hspace=0)\n        plt.tight_layout(pad=0, rect=[0.05, 0, 1, 1])  # Leave room on the left (x=0.05)\n        fig.supylabel(f\"Layer {layer_id}\", fontsize=12)\n        plt.show()\n\n\n\nFirst layers respond mostly to colors, horizontal/diagonal edges\n\nDeeper layer respond to circular, triangular, stripy,... patterns\n\nvisualizer = FilterVisualizer(cat_model, size=64, upscaling_steps=10, upscaling_factor=1.2, device=accelerator)\nvisualizer.visualize_filters(conv_layer_index=1, filters=[0,2,5,10,14,16,17,19])\nvisualizer.visualize_filters(conv_layer_index=2, filters=[11,13,17,20,23,27,36,39])\nvisualizer.visualize_filters(conv_layer_index=3, filters=[0,1,3,5,10,13,16,17])\n\n\n\n\n\n\n\nWe need to go deeper and train for much longer.\n\nLet’s do this again for the VGG16 network pretrained on ImageNetfrom torchvision.models import vgg16, VGG16_Weights\nmodel = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n\nfrom torchvision.models import vgg16, VGG16_Weights\n\n# Load VGG16 pretrained on ImageNet\nvgg16_model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n\n# Remove the fully connected layers (equivalent to include_top=False in Keras)\nvgg16_model_feat = vgg16_model.features\n\n# Set model to evaluation mode and move to GPU\nvgg16_model_feat.eval();\n\n\n\n# Print model summary\nsummary(vgg16_model_feat, input_size=(1, 3, 224, 224))  # Pretraining images where 224x224\n\n\n\nFirst layers: very clear color and edge detectors\n\n3rd layer responds to arcs, circles, sharp corners\n\nvisualizer = FilterVisualizer(vgg16_model, size=64, upscaling_steps=10, upscaling_factor=1.2, device=accelerator)\nvisualizer.visualize_filters(conv_layer_index=0, filters=[0,1,4,6,11,19,20,21])\nvisualizer.visualize_filters(conv_layer_index=1, filters=[0,1,2,3,4,5,14,21])\nvisualizer.visualize_filters(conv_layer_index=2, filters=[1,2,4,5,9,15,21,22])\n\n\n\n\n\n\n\nDeeper: more intricate patterns in different colors emerge\n\nSwirls, arches, boxes, circles,...\n\nvisualizer.visualize_filters(conv_layer_index=3, filters=[0,4,5,9,14,16,21,22])\nvisualizer.visualize_filters(conv_layer_index=4, filters=[5,6,7,8,15,16,27,28])\nvisualizer.visualize_filters(conv_layer_index=5, filters=[0,7,12,13,15,17,22,28])\n\n\n\n\n\n\n\nDeeper: Filters specialize in all kinds of natural shapes\n\nMore complex patterns (waves, landscapes, eyes) seem to appear\n\nvisualizer.visualize_filters(conv_layer_index=6, filters=[0,1,2,8,13,17,25,28])\nvisualizer.visualize_filters(conv_layer_index=7, filters=[4,5,6,12,14,16,20,27])\nvisualizer.visualize_filters(conv_layer_index=8, filters=[3,6,10,15,16,20,26,29])\n\n\n\n\n\n\n\nDeepest layers have 512 filters each, each responding to very different patterns\n\nThis 512-dimensional embedding separates distinct classes of images in ‘feature space’\n\nvisualizer.visualize_filters(conv_layer_index=10, filters=[10,12,14,20,22,25,26,28])\nvisualizer.visualize_filters(conv_layer_index=11, filters=[1,9,12,14,15,23,24,25])\nvisualizer.visualize_filters(conv_layer_index=12, filters=[0,11,18,21,34,48,52,84])\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#visualizing-the-learned-filters","position":61},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Visualizing class activation","lvl2":"Interpreting the model"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#visualizing-class-activation","position":62},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Visualizing class activation","lvl2":"Interpreting the model"},"content":"We can also visualize which pixels of the input had the greatest influence on the final classification. Helps to interpret what the model is paying attention to.\n\nClass activation maps : produces a heatmap over the input image\n\nChoose a convolution layer, do Global Average Pooling (GAP) to get one output per channel\n\nGet the weights between those outputs and the class of interest\n\nCompute the weighted sum of all filter activations: combines what each filter is responding to and how much this affects the class prediction\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#visualizing-class-activation","position":63},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Implementing gradCAM"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#implementing-gradcam","position":64},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Implementing gradCAM"},"content":"    # Hooks to capture activations and gradients\n    def forward_hook(module, input, output):\n        activations = output\n    def backward_hook(module, grad_input, grad_output):\n        gradients = grad_output[0]\n    target_layer.register_forward_hook(forward_hook)\n    target_layer.register_full_backward_hook(backward_hook)    # Forward pass + get predicted class\n    pred_class = model(img_tensor).argmax(dim=1).item()\n\n    # For that class, do a backward pass to get gradients\n    model.zero_grad()\n    output[:, pred_class].backward()\n\n    # Compute Grad-CAM heatmap\n    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)  # GAP layer\n    heatmap = torch.sum(weights * activations, dim=1).squeeze()\n\nResNet50 model, image of class Elephant, top-8 channels (highest weighst)\n\nfrom torchvision import models\n\ndef gradCAM(img_path, show_channels=True, top_k=8):\n    model = models.resnet50(pretrained=True)\n    model.eval()\n    target_layer = model.layer4[-1]\n\n    preprocess = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    original_img = cv2.imread(img_path)\n    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n    display_img = original_img.copy()\n    img_tensor = preprocess(original_img).unsqueeze(0)\n\n    activations = None\n    gradients = None\n\n    def forward_hook(module, input, output):\n        nonlocal activations\n        activations = output\n\n    def backward_hook(module, grad_input, grad_output):\n        nonlocal gradients\n        gradients = grad_output[0]\n\n    target_layer.register_forward_hook(forward_hook)\n    target_layer.register_full_backward_hook(backward_hook)\n\n    output = model(img_tensor)\n    pred_class = output.argmax(dim=1).item()\n    model.zero_grad()\n    output[:, pred_class].backward()\n\n    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)  # [B, C, 1, 1]\n    activations = activations.detach().squeeze(0)              # [C, H, W]\n    weights = weights.detach().squeeze(0).squeeze(-1).squeeze(-1)  # [C]\n\n    # Visualize top-k channels\n    if show_channels:\n        # Get top-k channel indices by absolute weight\n        _, top_idxs = torch.topk(weights.abs(), k=top_k)\n        fig, axes = plt.subplots(1, top_k, figsize=(2.5 * top_k, 2.5))\n        for i, idx in enumerate(top_idxs):\n            channel_img = activations[idx].cpu().numpy()\n            channel_img = np.maximum(channel_img, 0)\n            channel_img /= channel_img.max() + 1e-10\n            channel_img = cv2.resize(channel_img, (original_img.shape[1], original_img.shape[0]))\n\n            weighted_img = channel_img * weights[idx].item()\n            weighted_img = np.clip(weighted_img, 0, 1)\n\n            axes[i].imshow(channel_img, cmap='viridis')\n            axes[i].axis('off')\n            axes[i].set_title(f\"w={weights[idx].item():.4f}\", fontsize=12)\n\n            #axes[1, i].imshow(weighted_img, cmap='inferno')\n            #axes[1, i].axis('off')\n            #axes[1, i].set_title(f\"w × ch {idx.item()}\", fontsize=12)\n\n        plt.suptitle(\"Top Conv Channels & Weights\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n\n    # Final Grad-CAM map\n    heatmap = torch.sum(weights[:, None, None] * activations, dim=0).cpu().numpy()\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max() + 1e-10\n    heatmap_resized = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(original_img, 0.6, heatmap_colored, 0.4, 0)\n\n    # Show original + Grad-CAM\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    axs[0].imshow(display_img)\n    axs[0].axis(\"off\")\n    axs[0].set_title(\"Original Image\", fontsize=12)\n\n    axs[1].imshow(superimposed_img)\n    axs[1].axis(\"off\")\n    axs[1].set_title(\"Grad-CAM\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\nimg_path = \"../notebooks/images/10_elephants.jpg\"\ngradCAM(img_path, show_channels=True, top_k=8)\n\n\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#implementing-gradcam","position":65},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Transfer learning"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#transfer-learning","position":66},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Transfer learning"},"content":"We can re-use pretrained networks instead of training from scratch\n\nLearned features can be a useful generic representation of the visual world\n\nGeneral approach:\n\nRemove the original classifier head and add a new one\n\nFreeze the pretrained weights (backbone), then train as usual\n\nOptionally unfreeze (and re-learn) part of the network\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#transfer-learning","position":67},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Using pre-trained networks: 3 ways","lvl2":"Transfer learning"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#using-pre-trained-networks-3-ways","position":68},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Using pre-trained networks: 3 ways","lvl2":"Transfer learning"},"content":"Fast feature extraction (for similar task, little data)\n\nRun data through convolutional base to build new features\n\nUse embeddings as input to a dense layer (or another algorithm)\n\nEnd-to-end finetuning (for similar task, lots of data + data augmentation)\n\nExtend the convolutional base model with a new dense layer\n\nTrain it end to end on the new data\n\nPartial fine-tuning (for somewhat different task)\n\nUnfreeze a few of the top convolutional layers, and retrain\n\nUpdate only the deeper (more task-specific layers)\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#using-pre-trained-networks-3-ways","position":69},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Fast feature extraction","lvl2":"Transfer learning"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#fast-feature-extraction","position":70},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Fast feature extraction","lvl2":"Transfer learning"},"content":"Pretrained ResNet18 architecture, remove fully-connected layers\n\nAdd new classification head, freeze all pretrained weightsdef __init__(self):\n    resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n    self.feature_dim = resnet.fc.in_features\n    self.resnet.fc = nn.Identity()                   # Remove old head\n    self.classifier = nn.Linear(self.feature_dim, 1) # New head\n    for param in self.backbone.parameters():  # Freeze backbone \n        param.requires_grad = False# Train\ndef forward(self, x):\n    features = self.resnet(x)\n    logits = self.classifier(features)\n    return logits.squeeze(1)\n\n# Faster data loader, but needs to be stored on file to be pickled\ndataset_code = \"\"\"\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset\n\nclass AlbumentationsImageDataset(Dataset):\n    def __init__(self, image_folder_dataset, transform=None):\n        self.image_folder = image_folder_dataset\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img_path, label = self.image_folder.samples[idx]\n        image = cv2.imread(img_path)\n        if image is None:\n            raise RuntimeError(f\"Failed to load image at {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            image = self.transform(image=image)[\"image\"]\n\n        return image, label\n\n    def __len__(self):\n        return len(self.image_folder)\n\"\"\"\n\nwith open(\"fast_dataset.py\", \"w\") as f:\n    f.write(dataset_code.strip())\n\n\n\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A\nimport multiprocessing\nfrom fast_dataset import AlbumentationsImageDataset\n\nnum_workers = 4 #multiprocessing.cpu_count()\n\n# Much faster version of the data module\n# Uses Albumentations to do data augmentation in GPU instead of CPU\nclass FastCatDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size=20, img_size=(150, 150), num_workers=None):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.num_workers = num_workers if num_workers is not None else os.cpu_count()\n\n        self.train_transform = A.Compose([\n            A.RandomScale(scale_limit=(0.0, 0.25), p=1.0),  # Random zoom-in up to 125%\n            A.PadIfNeeded(min_height=self.img_size[0], min_width=self.img_size[1], border_mode=0),  # pad if needed\n            A.RandomCrop(height=self.img_size[0], width=self.img_size[1]),\n            A.HorizontalFlip(),\n            A.Rotate(limit=40),\n            A.Affine(shear=20),\n            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n            ToTensorV2()\n        ])\n\n        self.val_transform = A.Compose([\n            A.Resize(height=self.img_size[0], width=self.img_size[1]),\n            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n            ToTensorV2()\n        ])\n\n    def setup(self, stage=None):\n        train_dir = os.path.join(self.data_dir, \"train\")\n        val_dir = os.path.join(self.data_dir, \"validation\")\n\n        base_train_dataset = datasets.ImageFolder(train_dir)\n        base_val_dataset = datasets.ImageFolder(val_dir)\n\n        self.train_dataset = AlbumentationsImageDataset(base_train_dataset, transform=self.train_transform)\n        self.val_dataset = AlbumentationsImageDataset(base_val_dataset, transform=self.val_transform)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n\n\n\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch\n\nclass TransferCatClassifier(pl.LightningModule):\n    def __init__(self, finetune_mode=\"head\", learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Load full pre-trained ResNet18\n        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.feature_dim = self.resnet.fc.in_features\n\n        # Replace FC head with your own\n        self.resnet.fc = nn.Identity()  # We'll do classification separately\n        self.classifier = nn.Linear(self.feature_dim, 1)\n\n        # Apply fine-tuning strategy\n        self.finetune_mode = finetune_mode\n        self.freeze_layers()\n\n        # Loss and metric\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = Accuracy(task=\"binary\")\n\n    def freeze_layers(self):\n        # Freeze all\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        if self.finetune_mode == \"last_block\":\n            # Unfreeze last block (layer4)\n            for param in self.resnet.layer4.parameters():\n                param.requires_grad = True\n        elif self.finetune_mode == \"full\":\n            for param in self.resnet.parameters():\n                param.requires_grad = True\n        # else 'head': leave everything except classifier frozen\n\n    def forward(self, x):\n        features = self.resnet(x)\n        logits = self.classifier(features)\n        return logits.squeeze(1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y.float())\n        acc = self.accuracy(logits, y.int())\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", acc, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y.float())\n        acc = self.accuracy(logits, y.int())\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        if self.finetune_mode == \"full\": # Keep LR small for backbone\n            return torch.optim.Adam([\n                {\"params\": self.resnet.parameters(), \"lr\": self.hparams.learning_rate * 0.1},\n                {\"params\": self.classifier.parameters(), \"lr\": self.hparams.learning_rate}\n            ])\n        else:\n            return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\ndef transfer_trainer(mode):\n    print(f\"Training mode: {mode}\")\n    data_module = FastCatDataModule(data_dir, batch_size=64)\n    model = TransferCatClassifier(finetune_mode=mode, learning_rate=1e-3)\n    metric_tracker = MetricTracker()\n\n    trainer = pl.Trainer(\n        max_epochs=30,\n        accelerator=accelerator,\n        devices=\"auto\",\n        log_every_n_steps=3,\n        deterministic=True,\n        callbacks=[metric_tracker]\n    )\n    trainer.fit(model, datamodule=data_module)\n    return metric_tracker.history\n\n\n\n# Only train the head\nif histories and \"cat_head\" in histories:\n    history_cat_head = histories[\"cat_head\"]\nelse:\n    history_cat_head = transfer_trainer(\"head\")\n\n\n\nFast feature extraction for cats and dogs\n\nTraining from scratch (see earlier): 85% accuracy after 30 epochs\n\nWith transfer learning: 94% in a few epochs\n\nHowever, not much capacity for learning more (weights are frozen)\n\nWe could add more dense layers, but are stuck with the conv layers\n\n512 trainable parameters\n\nplot_training(history_cat_head)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#fast-feature-extraction","position":71},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Partial finetuning","lvl2":"Transfer learning"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#partial-finetuning","position":72},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"Partial finetuning","lvl2":"Transfer learning"},"content":"Freeze backbone as before\n\nUnfreeze the last convolutional block# Freeze conv layers \nfor param in self.backbone.parameters():\n    param.requires_grad = False\n    \n# Unfreeze last block \nfor param in self.resnet.layer4.parameters():\n    param.requires_grad = True\n\n# Unfreeze last block\nif histories and \"cat_unfreeze\" in histories:\n    history_cat_unfreeze = histories[\"cat_unfreeze\"]\nelse:\n    history_cat_unfreeze = transfer_trainer(\"last_block\")\n\n\n\nPartial finetuning for cats and dogs\n\nPerformance increases slightly to 95% accuracy in few epochs\n\nNo further increase with longer training\n\n8.4 million trainable parameters\n\nplot_training(history_cat_unfreeze)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#partial-finetuning","position":73},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"End-to-end finetuning","lvl2":"Transfer learning"},"type":"lvl3","url":"/notebooks/convolutional-neural-networks#end-to-end-finetuning","position":74},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl3":"End-to-end finetuning","lvl2":"Transfer learning"},"content":"Freeze backbone as before. Unfreeze the last convolutional block\n\nUse a gentler initial learning rate for the backbone (to avoid destruction)# Make conv layers trainable\nfor param in self.resnet.parameters():\n    param.requires_grad = True\n\n# Use a gentle learning rate for the backbone\ndef configure_optimizers(self):\n    return torch.optim.Adam([\n        {\"params\": self.resnet.parameters(), \"lr\": self.hparams.learning_rate * 0.1},\n        {\"params\": self.classifier.parameters(), \"lr\": self.hparams.learning_rate}])\n\n# Full\nif histories and \"cat_full\" in histories:\n    history_cat_full = histories[\"cat_full\"]\nelse:\n    history_cat_full = transfer_trainer(\"full\")\n\n\n\nEnd-to-end finetuning for cats and dogs\n\nVery similar behavior, but more noisy\n\nOur dataset is likely too small to learn a better embedding\n\n11.2 million trainable parameters\n\nplot_training(history_cat_full)\n\n\n\nimport pickle \n\nhistories = {\"mnist\":history,\"cat\":history_cat,\"cat2\":history_cat2,\n             \"cat_head\":history_cat_head,\"cat_unfreeze\":history_cat_unfreeze,\"cat_full\":history_cat_full}\nwith open(\"../data/histories.pkl\", \"wb\") as f:\n    pickle.dump(histories, f)\n\n\n\n","type":"content","url":"/notebooks/convolutional-neural-networks#end-to-end-finetuning","position":75},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Take-aways"},"type":"lvl2","url":"/notebooks/convolutional-neural-networks#take-aways","position":76},{"hierarchy":{"lvl1":"Lecture 7: Convolutional Neural Networks","lvl2":"Take-aways"},"content":"2D Convnets are ideal for addressing image-related problems.\n\n1D convnets are sometimes used for text, signals,...\n\nCompositionality: learn a hierarchy of simple to complex patterns\n\nTranslation invariance: deeper networks are more robust to data variance\n\nData augmentation helps fight overfitting (for small datasets)\n\nRepresentations are easy to inspect: visualize activations, filters, GradCAM\n\nTransfer learning: pre-trained embeddings can often be effectively reused to learn deep models for small datasets","type":"content","url":"/notebooks/convolutional-neural-networks#take-aways","position":77},{"hierarchy":{"lvl1":"Lecture 8. Transformers"},"type":"lvl1","url":"/notebooks/transformers","position":0},{"hierarchy":{"lvl1":"Lecture 8. Transformers"},"content":"Maybe attention is all you need\n\nJoaquin Vanschoren\n\n# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots \nif interactive:\n    fig_scale = 0.5\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.4\n    plt.rcParams.update(print_config)\n    \nHTML('''<style>.rise-enabled .reveal pre {font-size=75%} </style>''')\n\n\n\n","type":"content","url":"/notebooks/transformers","position":1},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Overview"},"type":"lvl2","url":"/notebooks/transformers#overview","position":2},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Overview"},"content":"Basics: word embeddings\n\nWord2Vec, FastText, GloVe\n\nSequence-to-sequence and autoregressive models\n\nSelf-attention and transformer models\n\nVision Transformers\n\n","type":"content","url":"/notebooks/transformers#overview","position":3},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Bag of word representation"},"type":"lvl2","url":"/notebooks/transformers#bag-of-word-representation","position":4},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Bag of word representation"},"content":"First, build a vocabulary of all occuring words. Maps every word to an index.\n\nRepresent each document as an N dimensional vector (top-N most frequent words)\n\nOne-hot (sparse) encoding: 1 if the word occurs in the document\n\nDestroys the order of the words in the text (hence, a ‘bag’ of words)\n\n","type":"content","url":"/notebooks/transformers#bag-of-word-representation","position":5},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Text preprocessing pipelines","lvl2":"Bag of word representation"},"type":"lvl3","url":"/notebooks/transformers#text-preprocessing-pipelines","position":6},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Text preprocessing pipelines","lvl2":"Bag of word representation"},"content":"Tokenization: how to you split text into words / tokens?\n\nStemming: naive reduction to word stems. E.g. ‘the meeting’ to ‘the meet’\n\nLemmatization: NLP-based reduction, e.g. distinguishes between nouns and verbs\n\nDiscard stop words (‘the’, ‘an’,...)\n\nOnly use N (e.g. 10000) most frequent words, or a hash function\n\nn-grams: Use combinations of n adjacent words next to individual words\n\ne.g. 2-grams: “awesome movie”, “movie with”, “with creative”, ...\n\nCharacter n-grams: combinations of n adjacent letters: ‘awe’, ‘wes’, ‘eso’,...\n\nSubword tokenizers: graceful splits “unbelievability” -> un, believ, abil, ity\n\nUseful libraries: \n\nnltk, \n\nspaCy, \n\ngensim, \n\nHuggingFace tokenizers,...\n\n","type":"content","url":"/notebooks/transformers#text-preprocessing-pipelines","position":7},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Scaling","lvl2":"Bag of word representation"},"type":"lvl3","url":"/notebooks/transformers#scaling","position":8},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Scaling","lvl2":"Bag of word representation"},"content":"Only for classical models, LLMs use subword tokenizers and dense tokens from embedding layers (see later)\n\nL2 Normalization (vector norm): sum of squares of all word values equals 1\n\nNormalized Euclidean distance is equivalent to cosine distance\n\nWorks better for distance-based models (e.g. kNN, SVM,...)\nt_i = \\frac{t_i}{\\| t\\|_2 }\n\nTerm Frequency - Inverted Document Frequency (TF-IDF)\n\nScales value of words by how frequently they occur across all N documents\n\nWords that only occur in few documents get higher weight, and vice versat_i = t_i \\cdot log(\\frac{N}{|\\{d \\in D : t_i \\in d\\}|})\n\n","type":"content","url":"/notebooks/transformers#scaling","position":9},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Neural networks on bag of words"},"type":"lvl2","url":"/notebooks/transformers#neural-networks-on-bag-of-words","position":10},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Neural networks on bag of words"},"content":"We can build neural networks on bag-of-word vectors\n\nDo a one-hot-encoding with 10000 most frequent words\n\nSimple model with 2 dense layers, ReLU activation, dropoutself.model = nn.Sequential(\n    nn.Linear(10000, 16),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(16, 16),\n    nn.ReLU(),\n    nn.Dropout(0.5),\n    nn.Linear(16, 1)\n)\n\n","type":"content","url":"/notebooks/transformers#neural-networks-on-bag-of-words","position":11},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Evaluation","lvl2":"Neural networks on bag of words"},"type":"lvl3","url":"/notebooks/transformers#evaluation","position":12},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Evaluation","lvl2":"Neural networks on bag of words"},"content":"IMDB dataset of movie reviews (label is ‘positive’ or ‘negative’)\n\nTake a validation set of 10,000 samples from the training set\n\nWorks prety well (88% Acc), but overfits easily\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom collections import Counter\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom keras.datasets import imdb\nfrom IPython.display import clear_output\n\n# Load data with top 10,000 words\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\n# Vectorize sequences into one-hot encoded vectors\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension), dtype=np.float32)\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.0\n    return results\n\n# One-hot encode\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')\n\nclass IMDBVectorizedDataset(Dataset):\n    def __init__(self, features, labels):\n        self.x = torch.tensor(features, dtype=torch.float32)\n        self.y = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n    \n# Validation split like in Keras: first 10k for val\nx_val, x_partial_train = x_train[:10000], x_train[10000:]\ny_val, y_partial_train = y_train[:10000], y_train[10000:]\n\ntrain_dataset = IMDBVectorizedDataset(x_partial_train, y_partial_train)\nval_dataset = IMDBVectorizedDataset(x_val, y_val)\ntest_dataset = IMDBVectorizedDataset(x_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=512)\ntest_loader = DataLoader(test_dataset, batch_size=512)\n\nclass LivePlotCallback(pl.Callback):\n    def __init__(self):\n        self.train_losses = []\n        self.train_accs = []\n        self.val_losses = []\n        self.val_accs = []\n        self.max_acc = 0\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        metrics = trainer.callback_metrics\n\n        train_loss = metrics.get(\"train_loss\")\n        train_acc = metrics.get(\"train_acc\")\n        val_loss = metrics.get(\"val_loss\")\n        val_acc = metrics.get(\"val_acc\")\n\n        if all(v is not None for v in [train_loss, train_acc, val_loss, val_acc]):\n            self.train_losses.append(train_loss.item())\n            self.train_accs.append(train_acc.item())\n            self.val_losses.append(val_loss.item())\n            self.val_accs.append(val_acc.item())\n            self.max_acc = max(self.max_acc, val_acc.item())\n\n            if len(self.train_losses) > 1:\n                clear_output(wait=True)\n                N = np.arange(0, len(self.train_losses))\n                plt.figure(figsize=(10, 4))\n                plt.plot(N, self.train_losses, label='train_loss', lw=2, c='r')\n                plt.plot(N, self.train_accs, label='train_acc', lw=2, c='b')\n                plt.plot(N, self.val_losses, label='val_loss', lw=2, linestyle=\":\", c='r')\n                plt.plot(N, self.val_accs, label='val_acc', lw=2, linestyle=\":\", c='b')\n                plt.title(f\"Training Loss and Accuracy [Max Val Acc: {self.max_acc:.4f}]\", fontsize=12)\n                plt.xlabel(\"Epoch\", fontsize=12)\n                plt.ylabel(\"Loss / Accuracy\", fontsize=12)\n                plt.tick_params(axis='both', labelsize=12)\n                plt.legend(fontsize=12)\n                plt.grid(True)\n                plt.show()\n            \nclass IMDBClassifier(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10000, 16)\n        self.dropout1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(16, 16)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc3 = nn.Linear(16, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = torch.sigmoid(self.fc3(x))\n        return x.squeeze()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy(y_hat, y)\n        acc = ((y_hat > 0.5) == y.bool()).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        val_loss = F.binary_cross_entropy(y_hat, y)\n        val_acc = ((y_hat > 0.5) == y.bool()).float().mean()\n        self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", val_acc, on_epoch=True, prog_bar=True)\n\n    def configure_optimizers(self):\n        return torch.optim.RMSprop(self.parameters())\n    \nmodel = IMDBClassifier()\ntrainer = pl.Trainer(max_epochs=15, callbacks=[LivePlotCallback()], logger=False, enable_checkpointing=False)\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/transformers#evaluation","position":13},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Predictions","lvl3":"Evaluation","lvl2":"Neural networks on bag of words"},"type":"lvl4","url":"/notebooks/transformers#predictions","position":14},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Predictions","lvl3":"Evaluation","lvl2":"Neural networks on bag of words"},"content":"Let’s look at a few predictions. Why is the last one so negative?\n\n# 1. Get the trained model into eval mode\nmodel.eval()\n\n# 2. Disable gradient tracking\nwith torch.no_grad():\n    # Convert entire test set to a tensor if not already\n    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n\n    # Get predictions\n    predictions = model(x_test_tensor).numpy()\n\n# Get word index from Keras\nword_index = imdb.get_word_index()\nreverse_word_index = {value + 3: key for key, value in word_index.items()}\n\n# Add special tokens\nreverse_word_index[0] = '[PAD]'\nreverse_word_index[1] = '[START]'\nreverse_word_index[2] = '[UNK]'\nreverse_word_index[3] = '[UNUSED]'\n\ndef encode_review(text, word_index, num_words=10000):\n    # Basic preprocessing\n    words = text.lower().split()\n    encoded = [1]  # 1 is the index for [START]\n\n    for word in words:\n        index = word_index.get(word, 2)  # 2 is [UNK]\n        if index < num_words:\n            encoded.append(index)\n    return encoded\n\n# Function to decode a review\ndef decode_review(encoded_review):\n    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])\n\nprint(\"Review 0:\\n\", decode_review(test_data[0]))\nprint(\"Predicted positiveness:\", predictions[0])\n\nprint(\"\\nReview 16:\\n\", decode_review(test_data[16]))\nprint(\"Predicted positiveness:\", predictions[16])\n\n# New sentence\nsentence = 'the restaurant is not too terrible'\nencoded = encode_review(sentence, word_index)\nvectorized = vectorize_sequences([encoded])  # Note: wrap in list to get shape (1, 10000)\nmodel.eval() \nwith torch.no_grad():\n    input_tensor = torch.tensor(vectorized, dtype=torch.float32)\n    prediction = model(input_tensor).item()\n\nprint(\"\\nReview X:\\n\", \"[START]\",sentence)\nprint(f\"Predicted positiveness: {prediction:.4f}\")\n\n\n\n","type":"content","url":"/notebooks/transformers#predictions","position":15},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Word Embeddings"},"type":"lvl2","url":"/notebooks/transformers#word-embeddings","position":16},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Word Embeddings"},"content":"A word embedding is a numeric vector representation of a word\n\nCan be manual or learned from an existing representation (e.g. one-hot)\n\n","type":"content","url":"/notebooks/transformers#word-embeddings","position":17},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Learning embeddings from scratch","lvl2":"Word Embeddings"},"type":"lvl3","url":"/notebooks/transformers#learning-embeddings-from-scratch","position":18},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Learning embeddings from scratch","lvl2":"Word Embeddings"},"content":"Input layer uses fixed length documents (with 0-padding).\n\nAdd an embedding layer to learn the embedding\n\nCreate n-dimensional one-hot encoding.\n\nTo learn an m-dimensional embedding, use m hidden nodes. Weight matrix W^{n x m}\n\nLinear activation function: \\mathbf{X}_{embed} = W \\mathbf{X}_{orig}.\n\nCombine all word embeddings into a document embedding (e.g. global pooling).\n\nAdd layers to map word embeddings to the output. Learn embedding weights from data.\n\nLet’s try this:max_length = 100 # pad documents to a maximum number of words\nvocab_size = 10000 # vocabulary size\nembedding_length = 20 # embedding length (more would be better)\n\nself.model = nn.Sequential(\n    nn.Embedding(vocab_size, embedding_length),\n    nn.AdaptiveAvgPool1d(1),  # global average pooling over sequence\n    nn.Linear(embedding_length, 1),\n)\n\nTraining on the IMDB dataset: slightly worse than using bag-of-words?\n\nEmbedding of dim 20 is very small, should be closer to 100 (or 300)\n\nWe don’t have enough data to learn a really good embedding from scratch\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\nclass IMDBVectorizedDataset(Dataset):\n    def __init__(self, features, labels):\n        self.x = torch.tensor(features, dtype=torch.long) # Needs long\n        self.y = torch.tensor(labels, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\nclass IMDBEmbeddingModel(pl.LightningModule):\n    def __init__(self, vocab_size=10000, embedding_length=20, max_length=100):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_length)\n        self.pooling = nn.AdaptiveAvgPool1d(1)  # GlobalAveragePooling1D equivalent\n        self.fc = nn.Linear(embedding_length, 1)\n\n    def forward(self, x):\n        # x: (batch, max_length)\n        embedded = self.embedding(x)  # (batch, max_length, embedding_length)\n        embedded = embedded.permute(0, 2, 1)  # for AdaptiveAvgPool1d → (batch, embed_dim, seq_len)\n        pooled = self.pooling(embedded).squeeze(-1)  # → (batch, embed_dim)\n        output = torch.sigmoid(self.fc(pooled))  # → (batch, 1)\n        return output.squeeze()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy(y_hat, y)\n        acc = ((y_hat > 0.5) == y.bool()).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        val_loss = F.binary_cross_entropy(y_hat, y)\n        val_acc = ((y_hat > 0.5) == y.bool()).float().mean()\n        self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", val_acc, on_epoch=True, prog_bar=True)\n\n    def configure_optimizers(self):\n        return torch.optim.RMSprop(self.parameters())\n\n# Build padded sequences\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Parameters\nvocab_size = 10000\nmax_length = 100\n\n# Load and preprocess\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\nx_train = pad_sequences(train_data, maxlen=max_length)\nx_test = pad_sequences(test_data, maxlen=max_length)\n\ny_train = train_labels\ny_test = test_labels\n\n# Split training/validation like in Keras example\nx_val, x_partial_train = x_train[:10000], x_train[10000:]\ny_val, y_partial_train = y_train[:10000], y_train[10000:]\n\nfrom torch.utils.data import DataLoader\n\ntrain_dataset = IMDBVectorizedDataset(x_partial_train, y_partial_train)\nval_dataset = IMDBVectorizedDataset(x_val, y_val)\ntest_dataset = IMDBVectorizedDataset(x_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=512)\ntest_loader = DataLoader(test_dataset, batch_size=512)\n\nmodel = IMDBEmbeddingModel(vocab_size=vocab_size, embedding_length=20, max_length=max_length)\n\ntrainer = pl.Trainer(\n    max_epochs=15,\n    logger=False,\n    enable_checkpointing=False,\n    callbacks=[LivePlotCallback()]  # optional\n)\n\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n\n\n\n\n\n","type":"content","url":"/notebooks/transformers#learning-embeddings-from-scratch","position":19},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Pre-trained embeddings","lvl2":"Word Embeddings"},"type":"lvl3","url":"/notebooks/transformers#pre-trained-embeddings","position":20},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Pre-trained embeddings","lvl2":"Word Embeddings"},"content":"With more data we can build better embeddings, but we also need more labels\n\nSolution: transfer learning! Learn embedding on auxiliary task that doesn’t require labels\n\nE.g. given a word, predict the surrounding words.\n\nAlso called self-supervised learning. Supervision is provided by data itself\n\nFreeze embedding weights to produce simple word embeddings, or finetune to a new tasks\n\nMost common approaches:\n\nWord2Vec: Learn neural embedding for a word based on surrounding words\n\nFastText: learns embedding for character n-grams\n\nCan also produce embeddings for new, unseen words\n\nGloVe (Global Vector): Count co-occurrences of words in a matrix\n\nUse a low-rank approximation to get a latent vector representation\n\n","type":"content","url":"/notebooks/transformers#pre-trained-embeddings","position":21},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Word2Vec","lvl3":"Pre-trained embeddings","lvl2":"Word Embeddings"},"type":"lvl4","url":"/notebooks/transformers#word2vec","position":22},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Word2Vec","lvl3":"Pre-trained embeddings","lvl2":"Word Embeddings"},"content":"Move a window over text to get C context words (V-dim one-hot encoded)\n\nAdd embedding layer with N linear nodes, global average pooling, and softmax layer(s)\n\nCBOW: predict word given context, use weights of last layer W^{'}_{NxV} as embedding\n\nSkip-Gram: predict context given word, use weights of first layer W^{T}_{VxN} as embedding\n\nScales to larger text corpora, learns relationships between words better\n\n","type":"content","url":"/notebooks/transformers#word2vec","position":23},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Word2Vec properties","lvl3":"Pre-trained embeddings","lvl2":"Word Embeddings"},"type":"lvl4","url":"/notebooks/transformers#word2vec-properties","position":24},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Word2Vec properties","lvl3":"Pre-trained embeddings","lvl2":"Word Embeddings"},"content":"Word2Vec happens to learn \n\ninteresting relationships between words\n\nSimple vector arithmetic can map words to plurals, conjugations, gender analogies,...\n\ne.g. Gender relationships: vec_{king} - vec_{man} + vec_{woman} \\sim vec_{queen}\n\nPCA applied to embeddings shows Country - Capital relationship\n\nCareful: embeddings can capture \n\ngender and other biases present in the data.\n\nImportant unsolved problem!\n\n","type":"content","url":"/notebooks/transformers#word2vec-properties","position":25},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Doc2Vec","lvl2":"Word Embeddings"},"type":"lvl3","url":"/notebooks/transformers#doc2vec","position":26},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Doc2Vec","lvl2":"Word Embeddings"},"content":"Alternative way to combine word embeddings (instead of global pooling)\n\nAdds a paragraph (or document) embedding: learns how paragraphs (or docs) relate to each other\n\nCaptures document-level semantics: context and meaning of entire document\n\nCan be used to determine semantic similarity between documents.\n\n","type":"content","url":"/notebooks/transformers#doc2vec","position":27},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"FastText","lvl3":"Doc2Vec","lvl2":"Word Embeddings"},"type":"lvl4","url":"/notebooks/transformers#fasttext","position":28},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"FastText","lvl3":"Doc2Vec","lvl2":"Word Embeddings"},"content":"Limitations of Word2Vec:\n\nCannot represent new (out-of-vocabulary) words\n\nSimilar words are learned independently: less efficient (no parameter sharing)\n\nE.g. ‘meet’ and ‘meeting’\n\nFastText: same model, but uses character n-grams\n\nWords are represented by all character n-grams of length 3 to 6\n\n“football” 3-grams: <fo, foo, oot, otb, tba, bal, all, ll>\n\nBecause there are so many n-grams, they are hashed (dimensionality = bin size)\n\nRepresentation of word “football” is sum of its n-gram embeddings\n\nNegative sampling: also trains on random negative examples (out-of-context words)\n\nWeights are updated so that they are less likely to be predicted\n\n","type":"content","url":"/notebooks/transformers#fasttext","position":29},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Global Vector model (GloVe)","lvl3":"Doc2Vec","lvl2":"Word Embeddings"},"type":"lvl4","url":"/notebooks/transformers#global-vector-model-glove","position":30},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl4":"Global Vector model (GloVe)","lvl3":"Doc2Vec","lvl2":"Word Embeddings"},"content":"Builds a co-occurence matrix \\mathbf{X}: counts how often 2 words occur in the same context\n\nLearns a k-dimensional embedding W through matrix factorization with rank k\n\nActually learns 2 embeddings W and W' (differ in random initialization)\n\nMinimizes loss \\mathcal{L}, where b_i and b'_i are bias terms and f is a weighting function\\mathcal{L} = \\sum_{i,j=1}^{V} f(\\mathbf{X}_{ij}) (\\mathbf{w_i} \\mathbf{w'_j} + b_i + b'_j - log(\\mathbf{X}_{ij}))^2\n\nLet’s try this\n\nDownload the \n\nGloVe embeddings trained on Wikipedia\n\nWe can now get embeddings for 400,000 English words\n\nE.g. ‘queen’ (50 first values of 300-dim embedding)\n\n# To find the original data files, see\n# http://nlp.stanford.edu/data/glove.6B.zip\n# http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n\n# Build an index so that we can later easily compose the embedding matrix\ndata_dir = '../data'\nembeddings_index = {}\nwith open(os.path.join(data_dir, 'glove.txt')) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\n\n\nembeddings_index['queen'][0:50]\n\n\n\nSame simple model, but with frozen GloVe embeddings: much worse!\n\nLinear layer is too simple. We need something more complex -> transformers :)embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\nself.model = nn.Sequential(\n    nn.Embedding.from_pretrained(embedding_tensor, freeze=True),\n    nn.AdaptiveAvgPool1d(1),\n    nn.Linear(embedding_tensor.shape[1], 1))\n\n# Load GloVe (assumes file is like 'glove.6B.300d.txt')\nembedding_dim = 300\nglove_path = \"../data/glove.txt\"\n\nembeddings_index = {}\nwith open(glove_path, encoding='utf-8') as f:\n    for line in f:\n        values = line.strip().split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = vector\n        \nvocab_size = 10000\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nmissing = 0\n\nfor word, i in word_index.items():\n    if i < vocab_size:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n        else:\n            missing += 1\n\nprint(f\"{missing} words not found in GloVe.\")\n\nclass Permute(nn.Module):\n    def __init__(self, *dims):\n        super().__init__()\n        self.dims = dims\n\n    def forward(self, x):\n        return x.permute(*self.dims)\n\nclass Squeeze(nn.Module):\n    def __init__(self, dim=-1):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        return x.squeeze(self.dim)\n\nclass FrozenGloVeModel(pl.LightningModule):\n    def __init__(self, embedding_matrix, max_length=100):\n        super().__init__()\n        embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n\n        self.model = nn.Sequential(\n            nn.Embedding.from_pretrained(embedding_tensor, freeze=True),\n            Permute(0, 2, 1),\n            nn.AdaptiveAvgPool1d(1),\n            Squeeze(dim=-1),\n            nn.Linear(embedding_tensor.shape[1], 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x).squeeze()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.binary_cross_entropy(y_hat, y)\n        acc = ((y_hat > 0.5) == y.bool()).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        val_loss = F.binary_cross_entropy(y_hat, y)\n        val_acc = ((y_hat > 0.5) == y.bool()).float().mean()\n        self.log(\"val_loss\", val_loss, on_epoch=True)\n        self.log(\"val_acc\", val_acc, on_epoch=True)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters())\n    \nmodel = FrozenGloVeModel(embedding_matrix=embedding_matrix, max_length=100)\n\ntrainer = pl.Trainer(\n    max_epochs=30,\n    logger=False,\n    enable_checkpointing=False,\n    callbacks=[LivePlotCallback()]  # optional\n)\n\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n        \n        \n        \n        \n        \n        \n\n\n\n\n\n","type":"content","url":"/notebooks/transformers#global-vector-model-glove","position":31},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl2","url":"/notebooks/transformers#sequence-to-sequence-seq2seq-models","position":32},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Global average pooling or flattening destroys the word order\n\nWe need to model sequences explictly, e.g.:\n\n1D convolutional models: run a 1D filter over the input data\n\nFast, but can only look at small part of the sentence\n\nRecurrent neural networks (RNNs)\n\nCan look back at the entire previous sequence\n\nMuch slower to train, have limited memory in practice\n\nAttention-based networks (Transformers)\n\nBest of both worlds: fast and very long memory\n\n","type":"content","url":"/notebooks/transformers#sequence-to-sequence-seq2seq-models","position":33},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"seq2seq models","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#seq2seq-models","position":34},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"seq2seq models","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Produce a series of output given a series of inputs over time\n\nCan handle sequences of different lengths\n\nLabel-to-sequence, Sequence-to-label, seq2seq,...\n\nAutoregressive models (e.g. predict the next character, unsupervised)\n\n","type":"content","url":"/notebooks/transformers#seq2seq-models","position":35},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"1D convolutional networks","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#id-1d-convolutional-networks","position":36},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"1D convolutional networks","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Similar to 2D convnets, but moves only in 1 direction (time)\n\nExtract local 1D patch, apply filter (kernel) to every patch\n\nPattern learned can later be recognized elsewhere (translation invariance)\n\nLimited memory: only sees a small part of the sequence (receptive field)\n\nYou can use multiple layers, dilations,... but becomes expensive\n\nLooks at ‘future’ parts of the series, but can be made to look only at the past\n\nKnown as ‘causal’ models (not related to causality)\n\nSame embedding, but add 2 Conv1D layers and MaxPooling1D. Better!model = nn.Sequential(\n    nn.Embedding(num_embeddings=10000, embedding_dim=embedding_dim),\n    nn.Conv1d(in_channels=embedding_dim, out_channels=32, kernel_size=7),\n    nn.ReLU(),\n    nn.MaxPool1d(kernel_size=5),\n    nn.Conv1d(in_channels=32, out_channels=32, kernel_size=7),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool1d(1),  # GAP\n    nn.Flatten(),             # (batch, 32, 1) → (batch, 32)\n    nn.Linear(32, 1)\n)\n\nmodel = nn.Sequential(\n    nn.Embedding(num_embeddings=10000, embedding_dim=embedding_dim),  # embedding_layer\n    nn.Conv1d(in_channels=embedding_dim, out_channels=32, kernel_size=7),\n    nn.ReLU(),\n    nn.MaxPool1d(kernel_size=5),\n    nn.Conv1d(in_channels=32, out_channels=32, kernel_size=7),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool1d(1),  # equivalent to GlobalAveragePooling1D\n    nn.Flatten(),             # flatten (batch, 32, 1) → (batch, 32)\n    nn.Linear(32, 1),\n    nn.Sigmoid()\n)\n\n\n\n","type":"content","url":"/notebooks/transformers#id-1d-convolutional-networks","position":37},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Recurrent neural networks (RNNs)","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#recurrent-neural-networks-rnns","position":38},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Recurrent neural networks (RNNs)","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Recurrent connection: concats output to next input\n{\\color{orange} h_t} = \\sigma \\left( {\\color{orange} W } \\left[ \\begin{array}{c} {\\color{blue}x}_t \\\\ {\\color{orange} h}_{t-1} \\end{array} \\right] + b \\right)\n\nUnbounded memory, but training requires backpropagation through time\n\nRequires storing previous network states (slow + lots of memory)\n\nVanishing gradients strongly limit practical memory\n\nImproved with gating: learn what to input, forget, output (LSTMs, GRUs,...)\n\n","type":"content","url":"/notebooks/transformers#recurrent-neural-networks-rnns","position":39},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#simple-self-attention","position":40},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Maps a set of inputs to a set of outputs (without learned weigths)\n\n","type":"content","url":"/notebooks/transformers#simple-self-attention","position":41},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#simple-self-attention-1","position":42},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Compute dot product of input vector x_i with every x_j (including itself): {\\color{Orange} w_{ij}}\n\nCompute softmax over all these weights (positive, sum to 1)\n\nMultiply by each input vector, and sum everything up\n\nCan be easily vectorized: {\\color{green} Y}^T = {\\color{orange} W}{\\color{blue} X^T}, {\\color{orange} W} = \\textrm{softmax}( {\\color{blue} X}^T {\\color{blue}X} )\n\nFor each output, we mix information from all inputs according to how ‘similar’ they are\n\nThe set of weights {\\color{Orange} w_{i}} for a given token is called the attention vector\n\nIt says how much ‘attention’ each token gives to other tokens\n\nDoesn’t learn (no parameters), the embedding of {\\color{blue} X} defines self-attention\n\nWe’ll learn how to transform the embeddings later\n\nThat way we can learn different relationships (not just similarity)\n\nHas no problem looking very far back in the sequence\n\nOperates on sets (permutation invariant): allows img-to-set, set-to-set,... tasks\n\nIf the token order matters, we’ll have to encode it in the token embedding\n\n","type":"content","url":"/notebooks/transformers#simple-self-attention-1","position":43},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Scaled dot products","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#scaled-dot-products","position":44},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Scaled dot products","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Self-attention is powerful because it’s mostly a linear operation\n\n{\\color{green} Y}^T = {\\color{orange} W}{\\color{blue} X^T} is linear, there are no vanishing gradients\n\nThe softmax function only applies to {\\color{orange} W} = \\textrm{softmax}( {\\color{blue} X}^T {\\color{blue}X} ), not to {\\color{green} Y}^T\n\nNeeded to make the attention values sum up nicely to 1 without exploding\n\nThe dot products do get larger as the embedding dimension k gets larger (by a factor \\sqrt{k})\n\nWe therefore normalize the dot product by the input dimension k: {\\color{orange}w^{'}_{ij}} = \\frac{{\\color{blue} x_i}^T \\color{blue} x_j}{\\sqrt{k}}\n\nThis also makes training more stable: large softmas values lead to ‘sharp’ outputs, making some gradients very large and others very small\n\n","type":"content","url":"/notebooks/transformers#scaled-dot-products","position":45},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention layer","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#simple-self-attention-layer","position":46},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention layer","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Let’s add a simple self-attention layer to our movie sentiment model\n\nWithout self-attention, every word would contribute independently (bag of words)\n\nThe word terrible will likely result in a negative prediction\n\nNow, we can freeze the embedding, take output {\\color{gray}Y}, obtain a loss, and do backpropagation so that the self-attention layer can learn that ‘not’ should invert the meaning of ‘terrible’\n\n","type":"content","url":"/notebooks/transformers#simple-self-attention-layer","position":47},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention layer","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#simple-self-attention-layer-1","position":48},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Simple self-attention layer","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"Through training, we want the self-attention to learn how certain tokens (e.g. ‘not’) can affect other tokens / words.\n\nE.g. we need to learn to change the representations of v_{not} and v_{terrible} so that they produce a ‘correct’ (low loss) output\n\nFor that, we do need to add some trainable parameters.\n\n","type":"content","url":"/notebooks/transformers#simple-self-attention-layer-1","position":49},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Standard self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#standard-self-attention","position":50},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Standard self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"We add 3 weight matrices (K, Q, V) and biases to change each vector:\n\nk_i = K x_i + b_k\n\nq_i = Q x_i + b_q\n\nv_i = V x_i + b_v\n\nThe same K, Q, V are used for all tokens depending on whether they are the input token (v), the token we are currently looking at (q), or the token we’re comparing with (k)\n\n","type":"content","url":"/notebooks/transformers#standard-self-attention","position":51},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Sidenote on terminology","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#sidenote-on-terminology","position":52},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Sidenote on terminology","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"View the set of tokens as a dictionary\ns = {a: v_a, b: v_b, c: v_c}\n\nIn a dictionary, the third output (for key c) would simple be s[c] = v_c\n\nIn a soft dictionary, it’s a weighted sum: s[c] = w_a * v_a + w_b * v_b + w_c * v_c\n\nIf w_i are dot products: s[c] = (k_a\\cdot q_c) * v_a + (k_b\\cdot q_c) * v_b + (k_b\\cdot q_c) * v_c\n\nWe blend the influence of every token based on their learned relations with other tokens\n\n\n","type":"content","url":"/notebooks/transformers#sidenote-on-terminology","position":53},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Intuition","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#intuition","position":54},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Intuition","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"We blend the influence of every token based on their learned ‘relations’ with other tokens\n\nSay that we need to learn how ‘negation’ works\n\nThe ‘query’ vector could be trained (via Q) to say something like ‘are there any negation words?’\n\nA token (e.g. ‘not’), transformed by K, could then respond very positively if it is\n\n","type":"content","url":"/notebooks/transformers#intuition","position":55},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Single-head self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#single-head-self-attention","position":56},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Single-head self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"There are different relations to model within a sentence.\n\nThe same input token, e.g. v_{terrible} can relate completely differently to other kinds of tokens\n\nBut we only have one set of K, V, and Q matrices\n\nTo better capture multiple relationships, we need multiple self-attention operations (expensive)\n\n","type":"content","url":"/notebooks/transformers#single-head-self-attention","position":57},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Multi-head self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"type":"lvl3","url":"/notebooks/transformers#multi-head-self-attention","position":58},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Multi-head self-attention","lvl2":"Sequence-to-sequence (seq2seq) models"},"content":"What if we project the input embeddings to a lower-dimensional embedding k?\n\nThen we could learn multiple self-attention operations in parallel\n\nEffectively, we split the self-attention in multiple heads\n\nEach applies a separate low-dimensional self attention (with K^{kxk},Q^{kxk},V^{kxk})\n\nAfter running them (in parallel), we concatenate their outputs.\n\n","type":"content","url":"/notebooks/transformers#multi-head-self-attention","position":59},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Transformer model"},"type":"lvl2","url":"/notebooks/transformers#transformer-model","position":60},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Transformer model"},"content":"Repeat self-attention multiple times in controlled fashion\n\nWorks for sequences, images, graphs,... (learn how sets of objects interact)\n\nModels consist of multiple transformer blocks, usually:\n\nLayer normalization (every input is normalized independently)\n\nSelf-attention layer (learn interactions)\n\nResidual connections (preserve gradients in deep networks)\n\nFeed-forward layer (learn mappings)\n\n","type":"content","url":"/notebooks/transformers#transformer-model","position":61},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Positional encoding","lvl2":"Transformer model"},"type":"lvl3","url":"/notebooks/transformers#positional-encoding","position":62},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Positional encoding","lvl2":"Transformer model"},"content":"We need some way to tell the self-attention layer about position in the sequence\n\nRepresent position by vectors, using some easy-to-learn predictable pattern\n\nAdd these encodings to vector embeddings\n\nGives information on how far one input is from the others\n\nOther techniques exist (e.g. relative positioning)\n\n","type":"content","url":"/notebooks/transformers#positional-encoding","position":63},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Autoregressive models","lvl2":"Transformer model"},"type":"lvl3","url":"/notebooks/transformers#autoregressive-models","position":64},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Autoregressive models","lvl2":"Transformer model"},"content":"Models that predict future values based on past values of the same stream\n\nOutput token is mapped to list of probabilities, sampled with softmax (with temperature)\n\nProblem: self-attention can simply look ahead in the stream\n\nWe need to make the transformer blocks causal\n\n","type":"content","url":"/notebooks/transformers#autoregressive-models","position":65},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Masked self-attention","lvl2":"Transformer model"},"type":"lvl3","url":"/notebooks/transformers#masked-self-attention","position":66},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Masked self-attention","lvl2":"Transformer model"},"content":"Simple solution: simply mask out any attention weights from current to future tokens\n\nReplace with -infinity, so that after softmax they will be 0\n\n","type":"content","url":"/notebooks/transformers#masked-self-attention","position":67},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Famous transformers","lvl2":"Transformer model"},"type":"lvl3","url":"/notebooks/transformers#famous-transformers","position":68},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Famous transformers","lvl2":"Transformer model"},"content":"“Attention is all you need”: first paper to use attention without CNNs or RNNs\n\nEncoder-Decoder architecture for translation: (k, q) to source attention layer\n\nWe’ll reproduce this (partly) in the Lab 6 tutorial :)\n\n","type":"content","url":"/notebooks/transformers#famous-transformers","position":69},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"GPT 3","lvl2":"Transformer model"},"type":"lvl3","url":"/notebooks/transformers#gpt-3","position":70},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"GPT 3","lvl2":"Transformer model"},"content":"Decoder-only, single stack of 96 transformer blocks (and 96 heads)\n\nSequence size 2048, input dimensionality 12,288, 175B parameters\n\nTrained on entire common crawl dataset (1 epoch)\n\nAdditional training on high-quality data (Wikipedia,...)\n\nExcellent animation by 1b3b\n\nGPT from scratch by A. Karpathy\n\n","type":"content","url":"/notebooks/transformers#gpt-3","position":71},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"GPT 4","lvl2":"Transformer model"},"type":"lvl3","url":"/notebooks/transformers#gpt-4","position":72},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"GPT 4","lvl2":"Transformer model"},"content":"Likely a ‘mixtures of experts’ model\n\nRouter (small MLP) selects which subnetworks (e.g. 2) to use given input\n\nPredictions get ensembled\n\nAllows scaling up parameter count without proportionate (inference) cost\n\nAlso better data, more human-in-the-loop training (RLHF),...\n\n","type":"content","url":"/notebooks/transformers#gpt-4","position":73},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Vision transformers"},"type":"lvl2","url":"/notebooks/transformers#vision-transformers","position":74},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Vision transformers"},"content":"Same principle: split up into patches, embed into tokens, add position encoding\n\nFor classification: add an extra (random) input token -> [CLS] output token\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\n\n## Torchvision\nimport torchvision\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\n\ndevice = \"cpu\"\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nprint(\"Device:\", device)\n\nDATASET_PATH = \"../data\"\nCHECKPOINT_PATH = \"../data/checkpoints\"\n\n\n\n","type":"content","url":"/notebooks/transformers#vision-transformers","position":75},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Demonstration","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#demonstration","position":76},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Demonstration","lvl2":"Vision transformers"},"content":"We’ll experiment with the CIFAR-10 datasets\n\nViTs are quite expensive on large images.\n\nThis ViT takes about an hour to train (we’ll run it from a checkpoint)\n\npl.seed_everything(42)\n\n\n\n\n\n# Downloads CIFAR10 en creates train/val/test loaders\n\ntest_transform = transforms.Compose([transforms.ToTensor(),\n                                     transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n                                     ])\n# For training, we add some augmentation. Networks are too powerful and would overfit.\ntrain_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n                                     ])\n# Loading the training dataset. We need to split it into a training and validation part\n# We need to do a little trick because the validation set should not use the augmentation.\ntrain_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\nval_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\ntrain_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n\n# Loading the test set\ntest_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n\n# We define a set of data loaders that we can use for various purposes later.\ntrain_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\nval_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\ntest_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n\n# Visualize some examples\nNUM_IMAGES = 4\nCIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\nimg_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\nimg_grid = img_grid.permute(1, 2, 0)\n\nplt.figure(figsize=(8,8))\nplt.title(\"Image examples of the CIFAR10 dataset\")\nplt.imshow(img_grid)\nplt.axis('off')\nplt.show()\nplt.close()\n\n\n\n","type":"content","url":"/notebooks/transformers#demonstration","position":77},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Patchify","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#patchify","position":78},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Patchify","lvl2":"Vision transformers"},"content":"Split N\\times N image into (N/M)^2 patches of size M\\times M.    B, C, H, W = x.shape  # Batch size, Channels, Height, Width\n    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n\ndef img_to_patch(x, patch_size, flatten_channels=True):\n    \"\"\"\n    Inputs:\n        x - torch.Tensor representing the image of shape [B, C, H, W]\n        patch_size - Number of pixels per dimension of the patches (integer)\n        flatten_channels - If True, the patches will be returned in a flattened format\n                           as a feature vector instead of a image grid.\n    \"\"\"\n    B, C, H, W = x.shape\n    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n    if flatten_channels:\n        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n    return x\n\nimg_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n\nfig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14, 2))\nfor i in range(CIFAR_images.shape[0]):\n    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=32, normalize=True, pad_value=1)\n    img_grid = img_grid.permute(1, 2, 0)\n    ax[i].imshow(img_grid)\n    ax[i].axis('off')\n\nplt.subplots_adjust(hspace=0)  # Reduce vertical spacing between rows\nplt.show()\nplt.close()\n\n\n\n","type":"content","url":"/notebooks/transformers#patchify","position":79},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Self-attention","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#self-attention","position":80},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Self-attention","lvl2":"Vision transformers"},"content":"First, we need to implement a (scaled) dot-product\n\n","type":"content","url":"/notebooks/transformers#self-attention","position":81},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Self-attention","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#self-attention-1","position":82},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Self-attention","lvl2":"Vision transformers"},"content":"First, we need to implement a (scaled) dot-productdef scaled_dot_product(q, k, v):\n    attn_logits = torch.matmul(q, k.transpose(-2, -1)) # dot prod\n    attn_logits = attn_logits / math.sqrt(q.size()[-1])# scaling\n    attention = F.softmax(attn_logits, dim=-1)         # softmax\n    values = torch.matmul(attention, v)                # dot prod\n    return values, attention\n\ndef scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention\n\n\n\n","type":"content","url":"/notebooks/transformers#self-attention-1","position":83},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Multi-head attention (simplified)","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#multi-head-attention-simplified","position":84},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Multi-head attention (simplified)","lvl2":"Vision transformers"},"content":"Project input to lower-dimensional embeddings\n\nStack them so we can feed them through self-attention at once\n\nUnstack and project back to original dimensions    qkv = nn.Linear(input_dim, 3*embed_dim)(x) # project to embed_dim\n    qkv = qkv.reshape(batch_size, seq_length, num_heads, 3*head_dim)\n    q, k, v = qkv.chunk(3, dim=-1) \n\n    values, attention = scaled_dot_product(q, k, v, mask=mask) # self-att\n    values = values.reshape(batch_size, seq_length, embed_dim)\n    out = nn.Linear(embed_dim, input_dim) # project back\n\ndef expand_mask(mask):\n    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim < 4:\n        mask = mask.unsqueeze(0)\n    return mask\n\nclass MultiheadAttention(nn.Module):\n    \n    def __init__(self, input_dim, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n        self.o_proj = nn.Linear(embed_dim, input_dim)\n        \n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n        \n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n        \n        if return_attention:\n            return o, attention\n        else:\n            return o\n\n\n\n","type":"content","url":"/notebooks/transformers#multi-head-attention-simplified","position":85},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Attention block","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#attention-block","position":86},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Attention block","lvl2":"Vision transformers"},"content":"The attention block is quite straightforward\n\n","type":"content","url":"/notebooks/transformers#attention-block","position":87},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Attention block","lvl2":"Vision transformers"},"type":"lvl3","url":"/notebooks/transformers#attention-block-1","position":88},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Attention block","lvl2":"Vision transformers"},"content":"def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n    self.layer_norm_1 = nn.LayerNorm(embed_dim)\n    self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n    self.layer_norm_2 = nn.LayerNorm(embed_dim)\n    self.linear = nn.Sequential( # Feed-forward layer\n        nn.Linear(embed_dim, hidden_dim),\n        nn.GELU(), nn.Dropout(dropout),\n        nn.Linear(hidden_dim, embed_dim),\n        nn.Dropout(dropout)\n    )def forward(self, x):\n    inp_x = self.layer_norm_1(x)\n    x = x + self.attn(inp_x, inp_x, inp_x)[0] # self-att + res\n    x = x + self.linear(self.layer_norm_2(x)) # feed-fw + res\n    return x\n\nclass AttentionBlock(nn.Module):\n    \n    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionality of input and attention feature vectors\n            hidden_dim - Dimensionality of hidden layer in feed-forward network \n                         (usually 2-4x larger than embed_dim)\n            num_heads - Number of heads to use in the Multi-Head Attention block\n            dropout - Amount of dropout to apply in the feed-forward network\n        \"\"\"\n        super().__init__()\n        \n        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, \n                                          dropout=dropout)\n        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n        self.linear = nn.Sequential(\n            nn.Linear(embed_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n        \n        \n    def forward(self, x):\n        inp_x = self.layer_norm_1(x)\n        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n        x = x + self.linear(self.layer_norm_2(x))\n        return x\n\n\n\n","type":"content","url":"/notebooks/transformers#attention-block-1","position":89},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Vision transformer"},"type":"lvl2","url":"/notebooks/transformers#vision-transformer","position":90},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Vision transformer"},"content":"Final steps:\n\nLinear projection (embedding) to map patches to vector\n\nAdd classification token to input\n\n2D positional encoding\n\nSmall MLP head to map CLS token to prediction\n\n","type":"content","url":"/notebooks/transformers#vision-transformer","position":91},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Positional encoding","lvl2":"Vision transformer"},"type":"lvl3","url":"/notebooks/transformers#positional-encoding-1","position":92},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Positional encoding","lvl2":"Vision transformer"},"content":"We implement this pattern and run in across a 2D grid:\nPE_{(pos,i)} = \\begin{cases}\n  \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n  \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n\\end{cases}\n\nimport math\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x\n    \nencod_block = PositionalEncoding(d_model=48, max_len=96)\npe = encod_block.pe.squeeze().T.cpu().numpy()\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\npos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\nfig.colorbar(pos, ax=ax)\nax.set_xlabel(\"Position in sequence\")\nax.set_ylabel(\"Hidden dimension\")\nax.set_title(\"Positional encoding over hidden dimensions\")\nax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\nax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\nplt.show()\n\n\n\nclass VisionTransformer(nn.Module):\n    \n    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n        \"\"\"\n        Inputs:\n            embed_dim - Dimensionality of the input feature vectors to the Transformer\n            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n                         within the Transformer\n            num_channels - Number of channels of the input (3 for RGB)\n            num_heads - Number of heads to use in the Multi-Head Attention block\n            num_layers - Number of layers to use in the Transformer\n            num_classes - Number of classes to predict\n            patch_size - Number of pixels that the patches have per dimension\n            num_patches - Maximum number of patches an image can have\n            dropout - Amount of dropout to apply in the feed-forward network and \n                      on the input encoding\n        \"\"\"\n        super().__init__()\n        \n        self.patch_size = patch_size\n        \n        # Layers/Networks\n        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, num_classes)\n        )\n        self.dropout = nn.Dropout(dropout)\n        \n        # Parameters/Embeddings\n        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n    \n    \n    def forward(self, x):\n        # Preprocess input\n        x = img_to_patch(x, self.patch_size)\n        B, T, _ = x.shape\n        x = self.input_layer(x)\n        \n        # Add CLS token and positional encoding\n        cls_token = self.cls_token.repeat(B, 1, 1)\n        x = torch.cat([cls_token, x], dim=1)\n        x = x + self.pos_embedding[:,:T+1]\n        \n        # Apply Transforrmer\n        x = self.dropout(x)\n        x = x.transpose(0, 1)\n        x = self.transformer(x)\n        \n        # Perform classification prediction\n        cls = x[0]\n        out = self.mlp_head(cls)\n        return out\n\n\n\nclass ViT(pl.LightningModule):\n    \n    def __init__(self, model_kwargs, lr):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = VisionTransformer(**model_kwargs)\n        self.example_input_array = next(iter(train_loader))[0]\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n        return [optimizer], [lr_scheduler]   \n    \n    def _calculate_loss(self, batch, mode=\"train\"):\n        imgs, labels = batch\n        preds = self.model(imgs)\n        loss = F.cross_entropy(preds, labels)\n        acc = (preds.argmax(dim=-1) == labels).float().mean()\n        \n        self.log(f'{mode}_loss', loss)\n        self.log(f'{mode}_acc', acc)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._calculate_loss(batch, mode=\"train\")\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"val\")\n\n    def test_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"test\")\n\n\n\ndef train_model(**kwargs):\n    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"), \n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=180,\n                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n                                    LearningRateMonitor(\"epoch\")])\n    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n\n    # Check whether pretrained model exists. If yes, load it and skip training\n    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n    if os.path.isfile(pretrained_filename):\n        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n    else:\n        pl.seed_everything(42) # To be reproducable\n        model = ViT(**kwargs)\n        trainer.fit(model, train_loader, val_loader)\n        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n\n    # Test best model on validation and test set\n    val_result = trainer.test(model, val_loader, verbose=False)\n    test_result = trainer.test(model, test_loader, verbose=False)\n    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n\n    return model, result\n\n\n\n","type":"content","url":"/notebooks/transformers#positional-encoding-1","position":93},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Results","lvl2":"Vision transformer"},"type":"lvl3","url":"/notebooks/transformers#results","position":94},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl3":"Results","lvl2":"Vision transformer"},"content":"ResNet outperforms ViT\n\nInductive biases of CNNs win out if you have limited data/compute\n\nTransformers have very little inductive bias\n\nMore flexible, but also more data hungry\n\nmodel, results = train_model(model_kwargs={\n                                'embed_dim': 256,\n                                'hidden_dim': 512,\n                                'num_heads': 8,\n                                'num_layers': 6,\n                                'patch_size': 4,\n                                'num_channels': 3,\n                                'num_patches': 64,\n                                'num_classes': 10,\n                                'dropout': 0.2\n                            },\n                            lr=3e-4)\nprint(\"ViT results\", results)\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/notebooks/transformers#results","position":95},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Summary"},"type":"lvl2","url":"/notebooks/transformers#summary","position":96},{"hierarchy":{"lvl1":"Lecture 8. Transformers","lvl2":"Summary"},"content":"Tokenization\n\nFind a good way to split data into tokens\n\nWord/Image embeddings (for initial embeddings)\n\nFor text: Word2Vec, FastText, GloVe\n\nFor images: MLP, CNN,...\n\nSequence-to-sequence models\n\n1D convolutional nets (fast, limited memory)\n\nRNNs (slow, also quite limited memory)\n\nTransformers\n\nSelf-attention (allows very large memory)\n\nPositional encoding\n\nAutoregressive models\n\nVision transformers\n\nUseful if you have lots of data (and compute)\n\nAcknowledgement\n\nSeveral figures came from the excellent \n\nVU Deep Learning course.","type":"content","url":"/notebooks/transformers#summary","position":97},{"hierarchy":{"lvl1":"Python for data analysis"},"type":"lvl1","url":"/notebooks/tutorial-1-python","position":0},{"hierarchy":{"lvl1":"Python for data analysis"},"content":"For those who are new to using Python for scientific work, we first provide a short introduction to Python and the most useful packages for data analysis.\n\n","type":"content","url":"/notebooks/tutorial-1-python","position":1},{"hierarchy":{"lvl1":"Python for data analysis","lvl2":"Python"},"type":"lvl2","url":"/notebooks/tutorial-1-python#python","position":2},{"hierarchy":{"lvl1":"Python for data analysis","lvl2":"Python"},"content":"Disclaimer: We can only cover some of the basics here. If you are completely new to Python, we recommend to take an introductory online course, such as the \n\nDefinite Guide to Python, or the \n\nWhirlwind Tour of Python. If you like a step-by-step approach, try the \n\nDataCamp Intro to Python for Data Science.\n\nTo practice your skills, try the \n\nHackerrank challenges.\n\n","type":"content","url":"/notebooks/tutorial-1-python#python","position":3},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Hello world","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#hello-world","position":4},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Hello world","lvl2":"Python"},"content":"Printing is done with the print() function.\n\nEverything after # is considered a comment.\n\nYou don’t need to end commands with ‘;’.\n\n# This is a comment\nprint(\"Hello world\")\nprint(5 / 8)\n5/8 # This only prints in IPython notebooks and shells.\n\n\n\n\n\nNote: In these notebooks we’ll use Python interactively to avoid having to type print() every time.\n\n","type":"content","url":"/notebooks/tutorial-1-python#hello-world","position":5},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Basic data types","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#basic-data-types","position":6},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Basic data types","lvl2":"Python"},"content":"Python has all the \n\nbasic data types and operations: int, float, str, bool, None.Variables are dynamically typed: you need to give them a value upon creation, and they will have the data type of that value. If you redeclare the same variable, if will have the data type of the new value.You can use type() to get a variable’s type.\n\ns = 5\ntype(s)\ns > 3 # Booleans: True or False\ns = \"The answer is \"\ntype(s)\n\n\n\nPython is also strongly typed: it won’t implicitly change a data type, but throw a TypeError instead. You will have to convert data types explictly, e.g. using str() or int().Exception: Arithmetic operations will convert to the most general type.\n\n1.0 + 2     # float + int -> float\ns + str(42) # string + string\n# s + 42    # Bad: string + int\n\n\n\n","type":"content","url":"/notebooks/tutorial-1-python#basic-data-types","position":7},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Complex types","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#complex-types","position":8},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Complex types","lvl2":"Python"},"content":"The main complex data types are lists, tuples, sets, and dictionaries (dicts).\n\nl = [1,2,3,4,5,6]       # list\nt = (1,2,3,4,5,6)       # tuple: like a list, but immutable\ns = set((1,2,3,4,5,6))  # set: unordered, you need to use add() to add new elements \nd = {2: \"a\",            # dict: has key - value pairs\n     3: \"b\",\n     \"foo\": \"c\",\n     \"bar\": \"d\"}\n\nl  # Note how each of these is printed\nt\ns \nd\n\n\n\nYou can use indices to return a value (except for sets, they are unordered)\n\nl\nl[2]\nt\nt[2]\nd\nd[2]\nd[\"foo\"]\n\n\n\nYou can assign new values to elements, except for tuples\n\nl\nl[2] = 7 # Lists are mutable\nl\n# t[2] = 7 # Tuples are not -> TypeError\n\n\n\nPython allows convenient tuple packing / unpacking\n\nb = (\"Bob\", 19, \"CS\")      # tuple packing\n(name, age, studies) = b   # tuple unpacking\nname\nage\nstudies\n\n\n\n","type":"content","url":"/notebooks/tutorial-1-python#complex-types","position":9},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Strings","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#strings","position":10},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Strings","lvl2":"Python"},"content":"Strings are \n\nquite powerful.They can be used as lists, e.g. retrieve a character by index.They can be formatted with the format operator (%), e.g. %s for strings, %d for decimal integers, %f for floats.\n\ns = \"The %s is %d\" % ('answer', 42)\ns\ns[0]\ns[4:10]\n'%.2f' % (3.14159265) # defines number of decimal places in a float\n\n\n\nThey also have a format() function for \n\nmore complex formatting\n\nl = [1,2,3,4,5,6]\n\"{}\".format(l) \n\"%s\" % l       # This is identical\n\"{first} {last}\".format(**{'first': 'Hodor', \n                           'last': 'Hodor!'})  \n\n\n\n","type":"content","url":"/notebooks/tutorial-1-python#strings","position":11},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"For loops, If statements","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#for-loops-if-statements","position":12},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"For loops, If statements","lvl2":"Python"},"content":"For-loops and if-then-else statements are written like this.Indentation defines the scope, not brackets.\n\nl = [1,2,3]\nd = {\"foo\": \"c\", \"bar\": \"d\"}\n\nfor i in l:\n    print(i)\n    \nfor k, v in d.items(): # Note how key-value pairs are extracted\n    print(\"%s : %s\" % (k,v))\n    \nif len(l) > 3:\n    print('Long list')\nelse:\n    print('Short list')\n\n\n\n","type":"content","url":"/notebooks/tutorial-1-python#for-loops-if-statements","position":13},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Functions","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#functions","position":14},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Functions","lvl2":"Python"},"content":"Functions are defined and called like this:\n\ndef myfunc(a, b):\n    return a + b\n\nmyfunc(2, 3)\n\n\n\nFunction arguments (parameters) can be:\n\nvariable-length (indicated with *)\n\na dictionary of keyword arguments (indicated with **).\n\ngiven a default value, in which case they are not required (but have to come last)\n\ndef func(*argv, **kwarg):\n    print(\"func argv: %s\" % str(argv))\n    print(\"func kwarg: %s\" % str(kwarg))\n    \nfunc(2, 3, a=4, b=5)\n\ndef func(a=2):\n    print(a * a)\n\nfunc(3)\nfunc()\n\n\n\nFunctions can have any number of outputs.\n\ndef func(*argv):\n    return sum(argv[0:2]), sum(argv[2:4])\n\nsum1, sum2 = func(2, 3, 4, 5)\nsum1, sum2\n\ndef squares(limit):\n    r = 0\n    ret = []\n    \n    while r < limit:\n        ret.append(r**2)\n        r += 1\n    \n    return ret\n\nfor i in squares(4):\n    print(i)\n\n\n\nFunctions can be passed as arguments to other functions\n\ndef greet(name):\n    return \"Hello \" + name \n\ndef call_func(func):\n    other_name = \"John\"\n    return func(other_name)  \n\ncall_func(greet)\n\n\n\nFunctions can return other functions\n\ndef compose_greet_func():\n    def get_message():\n        return \"Hello there!\"\n\n    return get_message\n\ngreet = compose_greet_func()\ngreet()\n\n\n\n","type":"content","url":"/notebooks/tutorial-1-python#functions","position":15},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Classes","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#classes","position":16},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Classes","lvl2":"Python"},"content":"Classes are defined like this\n\nclass TestClass(object): # TestClass inherits from object.\n    myvar = \"\"\n     \n    def __init__(self, myString): # optional constructor, returns nothing\n        self.myvar = myString # 'self' is used to store instance properties\n    \n    def say(self, what): # you need to add self as the first argument \n        return self.myvar + str(what)\n\na = TestClass(\"The new answer is \")\na.myvar # You can retrieve all properties of self\na.say(42)\n\n\n\nStatic functions need the  decorator\n\nclass TestClass(object):\n    myvar = \"\"\n    \n    def __init__(self, myString): \n        self.myvar = myString\n    \n    def say(self, what): # you need to add self as the first argument \n        return self.myvar + str(what)\n    \n    @staticmethod\n    def sayStatic(what): # or declare the function static \n        return \"The answer is \" + str(what)\n\na = TestClass(\"The new answer is \")\na.say(42)\na.sayStatic(42)\n\n\n\n","type":"content","url":"/notebooks/tutorial-1-python#classes","position":17},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Functional Python","lvl2":"Python"},"type":"lvl3","url":"/notebooks/tutorial-1-python#functional-python","position":18},{"hierarchy":{"lvl1":"Python for data analysis","lvl3":"Functional Python","lvl2":"Python"},"content":"You can write complex procedures in a few elegant lines of code using \n\nbuilt-in functions and libraries such as functools, itertools, operator.\n\ndef square(num):\n    return num ** 2\n\n# map(function, iterable) applies a given function to every element of a list\nlist(map(square, [1,2,3,4]))\n\n# a lambda function is an anonymous function created on the fly\nlist(map(lambda x: x**2, [1,2,3,4])) \nmydata = list(map(lambda x: x if x>2 else 0, [1,2,3,4])) \nmydata\n\n# reduce(function, iterable ) applies a function with two arguments cumulatively to every element of a list\nfrom functools import reduce\nreduce(lambda x,y: x+y, [1,2,3,4]) \nmydata\n\n\n\n# filter(function, iterable)) extracts every element for which the function returns true\nlist(filter(lambda x: x>2, [1,2,3,4]))\n\n# zip([iterable,...]) returns tuples of corresponding elements of multiple lists\nlist(zip([1,2,3,4],[5,6,7,8,9]))\n\n\n\nlist comprehensions can create lists as follows:[statement for var in iterable if condition]  \n\ngenerators do the same, but are lazy: they don’t create the list until it is needed:(statement for var in list if condition)\n\na = [2, 3, 4, 5]\n\nlc = [ x for x in a if x >= 4 ] # List comprehension. Square brackets\nlg = ( x for x in a if x >= 4 ) # Generator. Round brackets\n\na.extend([6,7,8,9])\n\nfor i in lc:\n    print(\"%i \" % i, end=\"\") # end tells the print function not to end with a newline\nprint(\"\\n\")\nfor i in lg:\n    print(\"%i \" % i, end=\"\")\n\n\n\ndict comprehensions are possible in Python 3:{key:value for (key,value) in dict.items() if condition}  \n\n# Quick dictionary creation\n\nnumbers = range(10)\n{n:n**2 for n in numbers if n%2 == 0}\n\n\n\n# Powerful alternative to replace lambda functions\n# Convert Fahrenheit to Celsius\nfahrenheit = {'t1': -30,'t2': 0,'t3': 32,'t4': 100}\n{k:(float(5)/9)*(v-32) for (k,v) in fahrenheit.items()}\n\n","type":"content","url":"/notebooks/tutorial-1-python#functional-python","position":19},{"hierarchy":{"lvl1":"Python for scientific computing"},"type":"lvl1","url":"/notebooks/tutorial-2-python-for-data-analysis","position":0},{"hierarchy":{"lvl1":"Python for scientific computing"},"content":"Python has extensive packages to help with data analysis:\n\nnumpy: matrices, linear algebra, Fourier transform, pseudorandom number generators\n\nscipy: advanced linear algebra and maths, signal processing, statistics\n\npandas: DataFrames, data wrangling and analysis\n\nmatplotlib: visualizations such as line charts, histograms, scatter plots.\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis","position":1},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"NumPy"},"type":"lvl2","url":"/notebooks/tutorial-2-python-for-data-analysis#numpy","position":2},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"NumPy"},"content":"NumPy is the fundamental package required for high performance scientific computing in Python. It provides:\n\nndarray: fast and space-efficient n-dimensional numeric array with vectorized arithmetic operations\n\nFunctions for fast operations on arrays without having to write loops\n\nLinear algebra, random number generation, Fourier transform\n\nIntegrating code written in C, C++, and Fortran (for faster operations)\n\npandas provides a richer, simpler interface to many operations. We’ll focus on using ndarrays here because they are heavily used in scikit-learn.\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#numpy","position":3},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"ndarrays","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#ndarrays","position":4},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"ndarrays","lvl2":"NumPy"},"content":"There are several ways to create numpy arrays.\n\n# Convert normal Python array to 1-dimensional numpy array\nnp.array((1, 2, 53))\n\n\n\n# Convert sequences of sequences of sequences ... to n-dim array\nnp.array([(1.5, 2, 3), (4, 5, 6)])\n\n\n\n# Define element type at creation time\nnp.array([[1, 2], [3, 4]], dtype=complex)\n\n\n\nUseful properties of ndarrays:\n\nmy_array = np.array([[1, 0, 3], [0, 1, 2]])\nmy_array.ndim     # number of dimensions (axes), also called the rank\nmy_array.shape    # a matrix with n rows and m columns has shape (n,m)\nmy_array.size     # the total number of elements of the array\nmy_array.dtype    # type of the elements in the array\nmy_array.itemsize # the size in bytes of each element of the array\n\n\n\nQuick array creation.It is cheaper to create an array with placeholders than extending it later.\n\nnp.ones(3) # Default type is float64\nnp.zeros([2, 2]) \nnp.empty([2, 2]) # Fills the array with whatever sits in memory\nnp.random.random((2,3))\nnp.random.randint(5, size=(2, 4))\n\n\n\nCreate sequences of numbers\n\nnp.linspace(0, 1, num=6)   # Linearly distributed numbers between 0 and 1\nnp.arange(0, 1, step=0.3)  # Fixed step size\nnp.arange(12).reshape(3,4) # Create and reshape\nnp.eye(4)                  # Identity matrix\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#ndarrays","position":5},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Basic Operations","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#basic-operations","position":6},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Basic Operations","lvl2":"NumPy"},"content":"Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result. Some operations, such as += and *=, act in place to modify an existing array rather than create a new one.\n\na = np.array([20, 30, 40, 50])\nb = np.arange(4)\na, b    # Just printing\na-b\nb**2\na > 32\na += 1\na\n\n\n\nThe product operator * operates elementwise.The matrix product can be performed using dot()\n\nA, B = np.array([[1,1], [0,1]]), np.array([[2,0], [3,4]]) # assign multiple variables in one line\nA\nB\nA * B\nnp.dot(A, B)\n\n\n\nUpcasting: Operations with arrays of different types choose the more general/precise one.\n\na = np.ones(3, dtype=int) # initialize to integers\nb = np.linspace(0, np.pi, 3) # default type is float\na.dtype, b.dtype, (a + b).dtype\n\n\n\nndarrays have most unary operations (max,min,sum,...) built in\n\na = np.random.random((2,3))\na\na.sum(), a.min(), a.max()\n\n\n\nBy specifying the axis parameter you can apply an operation along a specified axis of an array\n\nb = np.arange(12).reshape(3,4)\nb\nb.sum()\nb.sum(axis=0) \nb.sum(axis=1) \n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#basic-operations","position":7},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Universal Functions","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#universal-functions","position":8},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Universal Functions","lvl2":"NumPy"},"content":"NumPy provides familiar mathematical functions such as sin, cos, exp, sqrt, floor,... In NumPy, these are called “universal functions” (ufunc), and operate elementwise on an array, producing an array as output.\n\nnp.sin(np.arange(0, 10))\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#universal-functions","position":9},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Shape Manipulation","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#shape-manipulation","position":10},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Shape Manipulation","lvl2":"NumPy"},"content":"Transpose, flatten, reshape,...\n\na = np.floor(10*np.random.random((3,4)))\na\na.transpose()\nb = a.ravel() # flatten array\nb\nb.reshape(3, -1) # reshape in 3 rows (and as many columns as needed)\n\n\n\nArrays can be split and stacked together\n\na = np.floor(10*np.random.random((2,6)))\na\nb, c = np.hsplit(a, 2) # Idem: vsplit for vertical splits \nb\nc\nnp.hstack((b, c)) # Idenm: vstack for vertical stacks\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#shape-manipulation","position":11},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Indexing and Slicing","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#indexing-and-slicing","position":12},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Indexing and Slicing","lvl2":"NumPy"},"content":"Arrays can be indexed and sliced using [start:stop:stepsize]. Defaults are [0:ndim:1]\n\na = np.arange(10)**2\na\n\n\n\na[2]\n\n\n\na[3:10:2]\n\n\n\na[::-1] # Defaults are used if indices not stated\n\n\n\na[::2]\n\n\n\nFor multi-dimensional arrays, axes are comma-separated: [x,y,z].\n\nb = np.arange(16).reshape(4,4)\nb\nb[2,3] # row 2, column 3\n\n\n\nb[0:3,1] # Values 0 to 3 in column 1 \nb[ : ,1] # The whole column 1 \n\n\n\nb[1:3, : ] # Rows 1:3, all columns\n\n\n\n# Return the last row\nb[-1]   \n\n\n\nNote: dots (...) represent as many colons (:) as needed\n\nx[1,2,...] = x[1,2,:,:,:]\n\nx[...,3] = x[:,:,:,:,3]\n\nx[4,...,5,:] = x[4,:,:,5,:]\n\nArrays can also be indexed by arrays of integers and booleans.\n\na = np.arange(12)**2         \ni = np.array([ 1,1,3,8,5 ])\na\na[i]\n\n\n\nA matrix of indices returns a matrix with the corresponding values.\n\nj = np.array([[ 3, 4], [9, 7]])\na[j]\n\n\n\nWith boolean indices we explicitly choose which items in the array we want and which ones we don’t.\n\na = np.arange(12).reshape(3,4)\na\na[np.array([False,True,True]), :]\nb = a > 4\nb\na[b]\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#indexing-and-slicing","position":13},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Iterating","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#iterating","position":14},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Iterating","lvl2":"NumPy"},"content":"Iterating is done with respect to the first axis:\n\nfor row in b:\n    print(row)\n\n\n\nOperations on each element can be done by flattening the array (or nested loops)\n\nfor element in b.flat: # flat returns an iterator \n    print(element) \n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#iterating","position":15},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Copies and Views (or: how to shoot yourself in a foot)","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#copies-and-views-or-how-to-shoot-yourself-in-a-foot","position":16},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Copies and Views (or: how to shoot yourself in a foot)","lvl2":"NumPy"},"content":"Assigning an array to another variable does NOT create a copy\n\na = np.arange(12)\nb = a\na\n\n\n\nb[0] = -100\nb\n\n\n\na\n\n\n\nThe view() method creates a NEW array object that looks at the same data.\n\na = np.arange(12)\na\nc = a.view()\nc.resize((2, 6))\nc\n\n\n\na[0] = 123\nc # c is also changed now\n\n\n\nSlicing an array returns a view of it.\n\nc\ns = c[ : , 1:3]  \ns[:] = 10\ns\nc \n\n\n\nThe copy() method makes a deep copy of the array and its data.\n\nd = a.copy()      \nd[0] = -42\nd\n\n\n\na\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#copies-and-views-or-how-to-shoot-yourself-in-a-foot","position":17},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Numpy: further reading","lvl2":"NumPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#numpy-further-reading","position":18},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Numpy: further reading","lvl2":"NumPy"},"content":"Numpy Tutorial: \n\nhttp://​wiki​.scipy​.org​/Tentative​_NumPy​_Tutorial\n\n“Python for Data Analysis” by Wes McKinney (O’Reilly)\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#numpy-further-reading","position":19},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"SciPy"},"type":"lvl2","url":"/notebooks/tutorial-2-python-for-data-analysis#scipy","position":20},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"SciPy"},"content":"SciPy is a collection of packages for scientific computing, among others:\n\nscipy.integrate: numerical integration and differential equation solvers\n\nscipy.linalg: linear algebra routines and matrix decompositions\n\nscipy.optimize: function optimizers (minimizers) and root finding algorithms\n\nscipy.signal: signal processing tools\n\nscipy.sparse: sparse matrices and sparse linear system solvers\n\nscipy.stats: probability distributions, statistical tests, descriptive statistics\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#scipy","position":21},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Sparse matrices","lvl2":"SciPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#sparse-matrices","position":22},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Sparse matrices","lvl2":"SciPy"},"content":"Sparse matrices are used in scikit-learn for (large) arrays that contain mostly zeros. You can convert a dense (numpy) matrix to a sparse matrix.\n\nfrom scipy import sparse\neye = np.eye(4)\neye\nsparse_matrix = sparse.csr_matrix(eye) # Compressed Sparse Row matrix\nsparse_matrix\nprint(\"{}\".format(sparse_matrix))  \n\n\n\nWhen the data is too large, you can create a sparse matrix by passing the values and coordinates (COO format).\n\ndata = np.ones(4)                         # [1,1,1,1]\nrow_indices = col_indices = np.arange(4)  # [0,1,2,3]\ncol_indices = np.arange(4) * 2\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\nprint(\"{}\".format(eye_coo))\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#sparse-matrices","position":23},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Further reading","lvl2":"SciPy"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#further-reading","position":24},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Further reading","lvl2":"SciPy"},"content":"Check the \n\nSciPy reference guide for tutorials and examples of all SciPy capabilities.\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#further-reading","position":25},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"pandas"},"type":"lvl2","url":"/notebooks/tutorial-2-python-for-data-analysis#pandas","position":26},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"pandas"},"content":"pandas is a Python library for data wrangling and analysis. It provides:\n\nDataFrame: a table, similar to an R DataFrame that holds any structured data\n\nEvery column can have its own data type (strings, dates, floats,...)\n\nA great range of methods to apply to this table (sorting, querying, joining,...)\n\nImports data from a wide range of data formats (CSV, Excel) and databases (e.g. SQL)\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#pandas","position":27},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Series","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#series","position":28},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Series","lvl2":"pandas"},"content":"A one-dimensional array of data (of any numpy type), with indexed values. It can be created by passing a Python list or dict, a numpy array, a csv file,...\n\nimport pandas as pd\npd.Series([1,3,np.nan]) # Default integers are integers\npd.Series([1,3,5], index=['a','b','c'])\npd.Series({'a' : 1, 'b': 2, 'c': 3 }) # when given a dict, the keys will be used for the index\npd.Series({'a' : 1, 'b': 2, 'c': 3 }, index = ['b', 'c', 'd']) # this will try to match labels with keys\n\n\n\nFunctions like a numpy array, however with index labels as indices\n\na = pd.Series({'a' : 1, 'b': 2, 'c': 3 })\na\na['b']       # Retrieves a value\na[['a','b']] # and can also be sliced\n\n\n\nnumpy array operations on Series preserve the index value\n\na\na[a > 1]\na * 2 \nnp.sqrt(a)\n\n\n\nOperations over multiple Series will align the indices\n\na = pd.Series({'John' : 1000, 'Mary': 2000, 'Andre': 3000 })\nb = pd.Series({'John' : 100, 'Andre': 200, 'Cecilia': 300 })\na + b\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#series","position":29},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"DataFrame","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#dataframe","position":30},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"DataFrame","lvl2":"pandas"},"content":"A DataFrame is a tabular data structure with both a row and a column index. It can be created by passing a dict of arrays, a csv file,...\n\ndata = {'state': ['Ohio', 'Ohio', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2001, 2002],\n'pop': [1.5, 1.7, 2.4, 2.9]}\npd.DataFrame(data)\npd.DataFrame(data, columns=['year', 'state', 'pop', 'color']) # Will match indices  \n\n\n\nIt can be composed with a numpy array and row and column indices, and decomposed\n\ndates = pd.date_range('20130101',periods=4)\ndf = pd.DataFrame(np.random.randn(4,4),index=dates,columns=list('ABCD'))\ndf\n\n\n\ndf.index\ndf.columns\ndf.values\n\n\n\nDataFrames can easily read/write data from/to files\n\nread_csv(source): load CSV data from file or url\n\nread_table(source, sep=','): load delimited data with separator\n\ndf.to_csv(target): writes the DataFrame to a file\n\ndf.to_csv('data.csv', index=False) # Don't export the row index\ndfs = pd.read_csv('data.csv')\ndfs\ndfs.at[0, 'A'] = 10 # Set value in row 0, column 'A' to '10'\ndfs.to_csv('data.csv', index=False)\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#dataframe","position":31},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Simple operations","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#simple-operations","position":32},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Simple operations","lvl2":"pandas"},"content":"\n\ndf.head() # First 5 rows\ndf.tail() # Last 5 rows\n\n\n\n# Quick stats\ndf.describe()\n\n\n\n# Transpose\ndf.T\n\n\n\ndf\ndf.sort_index(axis=1, ascending=False) # Sort by index labels\ndf.sort_values(by='B') # Sort by values\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#simple-operations","position":33},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Selecting and slicing","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#selecting-and-slicing","position":34},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Selecting and slicing","lvl2":"pandas"},"content":"\n\ndf['A'] # Get single column by label\ndf.A    # Shorthand \n\n\n\ndf[0:2]          # Get rows by index number\ndf.iloc[0:2,0:2] # Get rows and columns by index number\ndf['20130102':'20130103']                # or row label  \ndf.loc['20130101':'20130103', ['A','B']] # or row and column label\n\n\n\nquery() retrieves data matching a boolean expression\n\ndf\ndf.query('A > -0.4') # Identical to df[df.A > 0.4]\ndf.query('A > B')   # Identical to df[df.A > df.B]\n\n\n\nNote: similar to NumPy, indexing and slicing returns a view on the data. Use copy() to make a deep copy.\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#selecting-and-slicing","position":35},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Operations","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#operations","position":36},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Operations","lvl2":"pandas"},"content":"DataFrames offer a \n\nwide range of operations: max, mean, min, sum, std,...\n\ndf.mean()       # Mean of all values per column\ndf.mean(axis=1) # Other axis: means per row \n\n\n\nAll of numpy’s universal functions also work with dataframes\n\nnp.abs(df)\n\n\n\nOther (custom) functions can be applied with apply(funct)\n\ndf\ndf.apply(np.max)\ndf.apply(lambda x: x.max() - x.min())\n\n\n\nData can be aggregated with groupby()\n\ndf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar'], 'B' : ['one', 'one', 'two', 'three'],\n                   'C' : np.random.randn(4), 'D' : np.random.randn(4)})\ndf\ndf.groupby('A').sum()\ndf.groupby(['A','B']).sum()\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#operations","position":37},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Data wrangling (some examples)","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#data-wrangling-some-examples","position":38},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Data wrangling (some examples)","lvl2":"pandas"},"content":"Merge: combine two dataframes based on common keys\n\ndf1 = pd.DataFrame({'key': ['b', 'b', 'a'], 'data1': range(3)}) \ndf2 = pd.DataFrame({'key': ['a', 'b'], 'data2': range(2)})\ndf1\ndf2\npd.merge(df1, df2)\n\n\n\nAppend: append one dataframe to another\n\ndf = pd.DataFrame(np.random.randn(2, 4))\ndf\ns = pd.DataFrame(np.random.randn(1,4))\ns\ndf = pd.concat([df,s], ignore_index=True)\n\n\n\nRemove duplicates\n\ndf = pd.DataFrame({'k1': ['one'] * 3, 'k2': [1, 1, 2]})\ndf\ndf.drop_duplicates()\n\n\n\nReplace values\n\ndf = pd.DataFrame({'k1': [1, -1], 'k2': [-1, 2]}) # Say that -1 is a sentinel for missing data\ndf\ndf.replace(-1, np.nan)\n\n\n\nDiscretization and binning\n\nages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\nbins = [18, 25, 35, 60, 100]\ncats = pd.cut(ages, bins)\ncats.categories\npd.value_counts(cats)\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#data-wrangling-some-examples","position":39},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Further reading","lvl2":"pandas"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#further-reading-1","position":40},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Further reading","lvl2":"pandas"},"content":"Pandas docs: \n\nhttp://​pandas​.pydata​.org​/pandas​-docs​/stable/\n\nhttps://​bitbucket​.org​/hrojas​/learn​-pandas\n\nPython for Data Analysis (O’Reilly) by Wes McKinney (the author of pandas)\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#further-reading-1","position":41},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"matplotlib"},"type":"lvl2","url":"/notebooks/tutorial-2-python-for-data-analysis#matplotlib","position":42},{"hierarchy":{"lvl1":"Python for scientific computing","lvl2":"matplotlib"},"content":"matplotlib is the primary scientific plotting library in Python. It provides:\n\nPublication-quality \n\nvisualizations such as line charts, histograms, and scatter plots.\n\nIntegration in pandas to make plotting much easier.\n\nInteractive plotting in Jupyter notebooks for quick visualizations.\n\nRequires some setup. See preamble and \n\n%matplotlib.\n\nMany GUI backends, export to PDF, SVG, JPG, PNG, BMP, GIF, etc.\n\nEcosystem of libraries for more advanced plotting, e.g. \n\nSeaborn\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#matplotlib","position":43},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Low-level usage","lvl2":"matplotlib"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#low-level-usage","position":44},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Low-level usage","lvl2":"matplotlib"},"content":"plot() is the \n\nmain function to generate a plot (but many more exist):plot(x, y)        Plot x vs y, default settings\nplot(x, y, 'bo')  Plot x vs y, blue circle markers\nplot(y, 'r+')     Plot y (x = array 0..N-1), red plusses\n\nEvery plotting function is completely customizable through a large set of options.\n\nx = np.linspace(-10, 10, 100) # Sequence for X-axis \ny = np.sin(x) # sine values \np = plt.plot(x, y, marker=\"x\") # Line plot with marker x\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#low-level-usage","position":45},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"pandas + matplotlib","lvl2":"matplotlib"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#pandas-matplotlib","position":46},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"pandas + matplotlib","lvl2":"matplotlib"},"content":"pandas DataFrames offer an easier, higher-level interface for matplotlib functions\n\ndf = pd.DataFrame(np.random.randn(500, 4), \n                  columns=['a', 'b', 'c', 'd']) # random 4D data\np = df.cumsum() # Plot cumulative sum of all series\np.plot();\n\n\n\np = df[:10].plot(kind='bar') # First 10 arrays as bar plots  \n\n\n\np = df.boxplot() # Boxplot for each of the 4 series\n\n\n\n# Scatter plot using the 4 series for x, y, color, scale \ndf.plot(kind='scatter', x='a', y='b', c='c', s=df['d']*72, cmap='plasma');\n\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#pandas-matplotlib","position":47},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Advanced plotting libraries","lvl2":"matplotlib"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#advanced-plotting-libraries","position":48},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Advanced plotting libraries","lvl2":"matplotlib"},"content":"Several libraries, such as \n\nSeaborn offer more advanced plots and easier interfaces.\n\n\n","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#advanced-plotting-libraries","position":49},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Further reading links","lvl2":"matplotlib"},"type":"lvl3","url":"/notebooks/tutorial-2-python-for-data-analysis#further-reading-links","position":50},{"hierarchy":{"lvl1":"Python for scientific computing","lvl3":"Further reading links","lvl2":"matplotlib"},"content":"Matplotlib examples\n\nPlotting with pandas\n\nSeaborn examples","type":"content","url":"/notebooks/tutorial-2-python-for-data-analysis#further-reading-links","position":51},{"hierarchy":{"lvl1":"Machine Learning in Python"},"type":"lvl1","url":"/notebooks/tutorial-3-machine-learning-in-python","position":0},{"hierarchy":{"lvl1":"Machine Learning in Python"},"content":"# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python","position":1},{"hierarchy":{"lvl1":"Machine Learning in Python"},"type":"lvl1","url":"/notebooks/tutorial-3-machine-learning-in-python#machine-learning-in-python","position":2},{"hierarchy":{"lvl1":"Machine Learning in Python"},"content":"Joaquin Vanschoren, Eindhoven University of Technology\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#machine-learning-in-python","position":3},{"hierarchy":{"lvl1":"Machine Learning in Python","lvl2":"Why Python?"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#why-python","position":4},{"hierarchy":{"lvl1":"Machine Learning in Python","lvl2":"Why Python?"},"content":"Many data-heavy applications are now developed in Python\n\nHighly readable, less complexity, fast prototyping\n\nEasy to offload number crunching to underlying C/Fortran/...\n\nEasy to install and import many rich libraries\n\nnumpy: efficient data structures\n\nscipy: fast numerical recipes\n\nmatplotlib: high-quality graphs\n\nscikit-learn: machine learning algorithms\n\ntensorflow: neural networks\n\n...\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#why-python","position":5},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib"},"type":"lvl1","url":"/notebooks/tutorial-3-machine-learning-in-python#numpy-scipy-matplotlib","position":6},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib"},"content":"We’ll illustrate these with a practical example\n\nMany good tutorials online\n\nJake VanderPlas’ book and notebooks\n\nJ.R. Johansson’s notebooks\n\nDataCamp\n\n...\n\nfrom scipy.stats import gamma\nnp.random.seed(3)  # to reproduce the data\n\ndef gen_web_traffic_data():\n    '''\n    This function generates some fake data that first shows a weekly pattern \n    for a couple weeks before it grows exponentially.\n    '''\n    # 31 days, 24 hours\n    x = np.arange(1, 31*24)\n    \n    # Sine wave with weekly rhythm + noise + exponential increase\n    y = np.array(200*(np.sin(2*np.pi*x/(7*24))), dtype=np.float32)\n    y += gamma.rvs(15, loc=0, scale=100, size=len(x))\n    y += 2 * np.exp(x/100.0)\n    y = np.ma.array(y, mask=[y<0])\n\n    return x, y\n\ndef plot_web_traffic(x, y, models=None, mx=None, ymax=None):\n    '''\n    Plot the web traffic (y) over time (x). \n    \n    If models is given, it is expected to be a list fitted models,\n    which will be plotted as well (used later).\n    '''\n    plt.figure(figsize=(12,6), dpi=300) # width and height of the plot in inches\n    plt.scatter(x, y, s=10)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Hits/hour\")\n    plt.xticks([w*7*24 for w in range(20)], \n               ['week %i' %w for w in range(20)])\n    \n    if models: \n        colors = ['g', 'r', 'm', 'b', 'k']\n        linestyles = ['-', '-.', '--', ':', '-']\n\n        if mx is None:\n            mx = np.linspace(0, x[-1], 1000)\n        for model, style, color in zip(models, linestyles, colors):\n            plt.plot(mx, model(mx), linestyle=style, linewidth=2, c=color)\n\n        plt.legend([\"d=%i\" % m.order for m in models], loc=\"upper left\")\n        \n    plt.autoscale()\n    if ymax:\n        plt.ylim(ymax=ymax)\n\n    plt.grid()\n    plt.ylim(ymin=0)\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#numpy-scipy-matplotlib","position":7},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib","lvl2":"Example: Modelling web traffic"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#example-modelling-web-traffic","position":8},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib","lvl2":"Example: Modelling web traffic"},"content":"We generate some artificial data to mimic web traffic data\n\nE.g. website visits, tweets with certain hashtag,...\n\nWeekly rhythm + noise + exponential increase\n\nx, y = gen_web_traffic_data()\nplot_web_traffic(x, y)\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#example-modelling-web-traffic","position":9},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib","lvl3":"Use numpy to fit some polynomial lines","lvl2":"Example: Modelling web traffic"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#use-numpy-to-fit-some-polynomial-lines","position":10},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib","lvl3":"Use numpy to fit some polynomial lines","lvl2":"Example: Modelling web traffic"},"content":"polyfit fits a polynomial of degree d\n\npoly1d evaluates the function using the learned coefficients\n\nPlot with matplotlibf2 = np.poly1d(np.polyfit(x, y, 2))\nf10 = np.poly1d(np.polyfit(x, y, 10))\nf50 = np.poly1d(np.polyfit(x, y, 50))\n\nmx = np.linspace(0, x[-1], 1000)\nplt.plot(mx, f2(mx))\n\nf2 = np.poly1d(np.polyfit(x, y, 2))\nf10 = np.poly1d(np.polyfit(x, y, 10))\nf50 = np.poly1d(np.polyfit(x, y, 50))\nplot_web_traffic(x, y, [f2,f10,f50])\n\n\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#use-numpy-to-fit-some-polynomial-lines","position":11},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib","lvl3":"Evaluate","lvl2":"Example: Modelling web traffic"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#evaluate","position":12},{"hierarchy":{"lvl1":"Numpy, Scipy, Matplotlib","lvl3":"Evaluate","lvl2":"Example: Modelling web traffic"},"content":"Using root mean squared error: \\sqrt{\\sum_i (f(x_i) - y_i)^2}\n\nThe degree of the polynomial needs to be tuned to the data\n\nPredictions don’t look great. We need more sophisticated methods.\n\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\nmx=np.linspace(0, 6 * 7 * 24, 100)\n\ndef error(f, x, y):\n    return np.sqrt(np.sum((f(x)-y)**2))\n\n@interact\ndef play_with_degree(degree=(1,30,2)):\n    f = np.poly1d(np.polyfit(x, y, degree))\n    plot_web_traffic(x, y, [f], mx=mx, ymax=10000)\n    print(\"Training error for d=%i: %f\" % (f.order, error(f, x, y)))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#evaluate","position":13},{"hierarchy":{"lvl1":"scikit-learn"},"type":"lvl1","url":"/notebooks/tutorial-3-machine-learning-in-python#scikit-learn","position":14},{"hierarchy":{"lvl1":"scikit-learn"},"content":"One of the most prominent Python libraries for machine learning:\n\nContains many state-of-the-art machine learning algorithms\n\nBuilds on numpy (fast), implements advanced techniques\n\nWide range of evaluation measures and techniques\n\nOffers \n\ncomprehensive documentation about each algorithm\n\nWidely used, and a wealth of \n\ntutorials and code snippets are available\n\nWorks well with numpy, scipy, pandas, matplotlib,...\n\nNote: We’ll repeat most of the material below in the lectures and labs on model selection and data preprocessing, but it’s still very useful to study it beforehand.\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#scikit-learn","position":15},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Algorithms"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#algorithms","position":16},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Algorithms"},"content":"See the \n\nReference\n\nSupervised learning:\n\nLinear models (Ridge, Lasso, Elastic Net, ...)\n\nSupport Vector Machines\n\nTree-based methods (Classification/Regression Trees, Random Forests,...)\n\nNearest neighbors\n\nNeural networks\n\nGaussian Processes\n\nFeature selection\n\nUnsupervised learning:\n\nClustering (KMeans, ...)\n\nMatrix Decomposition (PCA, ...)\n\nManifold Learning (Embeddings)\n\nDensity estimation\n\nOutlier detection\n\nModel selection and evaluation:\n\nCross-validation\n\nGrid-search\n\nLots of metrics\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#algorithms","position":17},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Data import"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#data-import","position":18},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Data import"},"content":"Multiple options:\n\nA few toy datasets are included in sklearn.datasets\n\nImport 1000s of datasets via sklearn.datasets.fetch_openml\n\nYou can import data files (CSV) with pandas or numpyfrom sklearn.datasets import load_iris, fetch_openml\niris_data = load_iris()\ndating_data = fetch_openml(name=\"SpeedDating\")\n\nfrom sklearn.datasets import load_iris, fetch_openml\niris_data = load_iris()\ndating_data = fetch_openml(\"SpeedDating\", version=1)\n\n\n\nThese will return a Bunch object (similar to a dict)print(\"Keys of iris_dataset: {}\".format(iris_dataset.keys()))\nprint(iris_dataset['DESCR'][:193] + \"\\n...\")\n\nprint(\"Keys of iris_dataset: {}\".format(iris_data.keys()))\nprint(iris_data['DESCR'][:193] + \"\\n...\")\n\n\n\nTargets (classes) and features are lists of strings\n\nData and target values are always numeric (ndarrays)print(\"Targets: {}\".format(iris_data['target_names']))\nprint(\"Features: {}\".format(iris_data['feature_names']))\nprint(\"Shape of data: {}\".format(iris_data['data'].shape))\nprint(\"First 5 rows:\\n{}\".format(iris_data['data'][:5]))\nprint(\"Targets:\\n{}\".format(iris_data['target']))\n\nprint(\"Targets: {}\".format(iris_data['target_names']))\nprint(\"Features: {}\".format(iris_data['feature_names']))\nprint(\"Shape of data: {}\".format(iris_data['data'].shape))\nprint(\"First 5 rows:\\n{}\".format(iris_data['data'][:5]))\nprint(\"Targets:\\n{}\".format(iris_data['target']))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#data-import","position":19},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Building models"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#building-models","position":20},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Building models"},"content":"All scikitlearn estimators follow the same interface\n\nclass SupervisedEstimator(...):\n    def __init__(self, hyperparam, ...):\n\n    def fit(self, X, y):   # Fit/model the training data\n        ...                # given data X and targets y\n        return self\n     \n    def predict(self, X):  # Make predictions\n        ...                # on unseen data X  \n        return y_pred\n    \n    def score(self, X, y): # Predict and compare to true\n        ...                # labels y                \n        return score\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#building-models","position":21},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Training and testing data","lvl2":"Building models"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#training-and-testing-data","position":22},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Training and testing data","lvl2":"Building models"},"content":"To evaluate our classifier, we need to test it on unseen data.train_test_split: splits data randomly in 75% training and 25% test data.X_train, X_test, y_train, y_test = train_test_split(\n    iris_data['data'], iris_data['target'], random_state=0)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_data['data'], iris_data['target'], \n    random_state=0)\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#training-and-testing-data","position":23},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Fitting a model","lvl2":"Building models"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#fitting-a-model","position":24},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Fitting a model","lvl2":"Building models"},"content":"\n\nThe first model we’ll build is a k-Nearest Neighbor classifier.kNN is included in sklearn.neighbors, so let’s build our first modelknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#fitting-a-model","position":25},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Making predictions","lvl2":"Building models"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#making-predictions","position":26},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Making predictions","lvl2":"Building models"},"content":"Let’s create a new example and ask the kNN model to classify itX_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nclass_name = iris_data['target_names'][prediction]\n\nX_new = np.array([[5, 2.9, 1, 0.2]])\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n       iris_data['target_names'][prediction]))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#making-predictions","position":27},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Evaluating the model","lvl2":"Building models"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#evaluating-the-model","position":28},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Evaluating the model","lvl2":"Building models"},"content":"Feeding all test examples to the model yields all predictionsy_pred = knn.predict(X_test)\n\ny_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\n\n\n\nThe score function computes the percentage of correct predictionsknn.score(X_test, y_test)\n\nprint(\"Score: {:.2f}\".format(knn.score(X_test, y_test) ))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#evaluating-the-model","position":29},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Cross-validation"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#cross-validation","position":30},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Cross-validation"},"content":"More stable, thorough way to estimate generalization performance\n\nk-fold cross-validation (CV): split (randomized) data into k equal-sized parts, called folds\n\nFirst, fold 1 is the test set, and folds 2-5 comprise the training set\n\nThen, fold 2 is the test set, folds 1,3,4,5 comprise the training set\n\nCompute k evaluation scores, aggregate afterwards (e.g. take the mean)\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#cross-validation","position":31},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Cross-validation in scikit-learn","lvl2":"Cross-validation"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#cross-validation-in-scikit-learn","position":32},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Cross-validation in scikit-learn","lvl2":"Cross-validation"},"content":"cross_val_score function with learner, training data, labels\n\nReturns list of all scores\n\nDoes 3-fold CV by default, can be changed via cv hyperparameter\n\nDefault scoring measures are accuracy (classification) or R^2 (regression)\n\nEven though models are built internally, they are not returnedknn = KNeighborsClassifier(n_neighbors=1)\nscores = cross_val_score(knn, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nprint(\"Variance in cross-validation score: {:.4f}\".format(np.var(scores)))\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nknn = KNeighborsClassifier(n_neighbors=1)\n\nscores = cross_val_score(knn, iris.data, iris.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nprint(\"Variance in cross-validation score: {:.4f}\".format(np.var(scores)))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#cross-validation-in-scikit-learn","position":33},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"More variants","lvl2":"Cross-validation"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#more-variants","position":34},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"More variants","lvl2":"Cross-validation"},"content":"Stratified cross-validation: for inbalanced datasets\n\nLeave-one-out cross-validation: for very small datasets\n\nShuffle-Split cross-validation: whenever you need to shuffle the data first\n\nRepeated cross-validation: more trustworthy, but more expensive\n\nCross-validation with groups: Whenever your data contains non-independent datapoints, e.g. data points from the same patient\n\nBootstrapping: sampling with replacement, for extracting statistical properties\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#more-variants","position":35},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Avoid data leakage","lvl2":"Cross-validation"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#avoid-data-leakage","position":36},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Avoid data leakage","lvl2":"Cross-validation"},"content":"Simply taking the best performing model based on cross-validation performance yields optimistic results\n\nWe’ve already used the test data to evaluate each model!\n\nHence, we don’t have an independent test set to evaluate these hyperparameter settings\n\nInformation ‘leaks’ from test set into the final model\n\nSolution: Set aside part of the training data to evaluate the hyperparameter settings\n\nSelect best model on validation set\n\nRebuild the model on the training+validation set\n\nEvaluate optimal model on the test set\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#avoid-data-leakage","position":37},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Pipelines"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#pipelines","position":38},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Pipelines"},"content":"Many learning algorithms are greatly affected by how you represent the training data\n\nExamples: Scaling, numeric/categorical values, missing values, feature selection/construction\n\nWe typically need chain together different algorithms\n\nMany preprocessing steps\n\nPossibly many models\n\nThis is called a pipeline (or workflow)\n\nThe best way to represent data depends not only on the semantics of the data, but also on the kind of model you are using.\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#pipelines","position":39},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Example: Speed dating data","lvl2":"Pipelines"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#example-speed-dating-data","position":40},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Example: Speed dating data","lvl2":"Pipelines"},"content":"Data collected from speed dating events\n\nSee \n\nhttps://​www​.openml​.org​/d​/40536\n\nCould also be collected from dating website or app\n\nReal-world data:\n\nDifferent numeric scales\n\nMissing values\n\nLikely irrelevant features\n\nDifferent types: Numeric, categorical,...\n\nInput errors (e.g. ‘lawyer’ vs ‘Lawyer’)dating_data = fetch_openml(\"SpeedDating\")\n\ndating_data = fetch_openml(\"SpeedDating\", version=1)\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#example-speed-dating-data","position":41},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Scaling","lvl2":"Pipelines"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#scaling","position":42},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Scaling","lvl2":"Pipelines"},"content":"When the features have different scales (their values range between very different minimum and maximum values), one feature will overpower the others. Several scaling techniques are available to solve this:\n\nStandardScaler rescales all features to mean=0 and variance=1\n\nDoes not ensure and min/max value\n\nRobustScaler uses the median and quartiles\n\nMedian m: half of the values < m, half > m\n\nLower Quartile lq: 1/4 of values < lq\n\nUpper Quartile uq: 1/4 of values > uq\n\nIgnores outliers, brings all features to same scale\n\nMinMaxScaler brings all feature values between 0 and 1\n\nNormalizer scales data such that the feature vector has Euclidean length 1\n\nProjects data to the unit circle\n\nUsed when only the direction/angle of the data matters\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#scaling","position":43},{"hierarchy":{"lvl1":"scikit-learn","lvl4":"Applying scaling transformations","lvl3":"Scaling","lvl2":"Pipelines"},"type":"lvl4","url":"/notebooks/tutorial-3-machine-learning-in-python#applying-scaling-transformations","position":44},{"hierarchy":{"lvl1":"scikit-learn","lvl4":"Applying scaling transformations","lvl3":"Scaling","lvl2":"Pipelines"},"content":"Lets apply a scaling transformation manually, then use it to train a learning algorithm\n\nFirst, split the data in training and test set\n\nNext, we fit the preprocessor on the training data\n\nThis computes the necessary transformation parameters\n\nFor MinMaxScaler, these are the min/max values for every feature\n\nAfter fitting, we can transform the training and test datascaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#applying-scaling-transformations","position":45},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Missing value imputation","lvl2":"Pipelines"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#missing-value-imputation","position":46},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Missing value imputation","lvl2":"Pipelines"},"content":"Many sci-kit learn algorithms cannot handle missing value\n\nImputer replaces specific values\n\nmissing_values (default ‘NaN’) placeholder for the missing value\n\nstrategy:\n\nmean, replace using the mean along the axis\n\nmedian, replace using the median along the axis\n\nmost_frequent, replace using the most frequent value\n\nMany more advanced techniques exist, but not yet in scikit-learn\n\ne.g. low rank approximations (uses matrix factorization)\n\nimp = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimp.fit_transform(X1_train)\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#missing-value-imputation","position":47},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Feature encoding","lvl2":"Pipelines"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#feature-encoding","position":48},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Feature encoding","lvl2":"Pipelines"},"content":"scikit-learn classifiers only handle numeric data. If your features are categorical, you need to encode them first\n\nLabelEncoder simply replaces each value with an integer value\n\nOneHotEncoder converts a feature of n values to n binary features\n\nProvide categories as array or set to ‘auto’X_enc = OneHotEncoder(categories='auto').fit_transform(X)\n\n\nColumnTransformer can apply different transformers to different features\n\nTransformers can be pipelines doing multiple thingsnumeric_features = ['age', 'pref_o_attractive']\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_features = ['gender', 'd_d_age', 'field']\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#feature-encoding","position":49},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Building Pipelines"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#building-pipelines","position":50},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Building Pipelines"},"content":"In scikit-learn, a pipeline combines multiple processing steps in a single estimator\n\nAll but the last step should be transformer (have a transform method)\n\nThe last step can be a transformer too (e.g. Scaler+PCA)\n\nIt has a fit, predict, and score method, just like any other learning algorithm\n\nPipelines are built as a list of steps, which are (name, algorithm) tuples\n\nThe name can be anything you want, but can’t contain '__'\n\nWe use '__' to refer to the hyperparameters, e.g. svm__C\n\nLet’s build, train, and score a MinMaxScaler + LinearSVC pipeline:\n\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\npipe.fit(X_train, y_train).score(X_test, y_test)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", LinearSVC())])\n\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\npipe.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\n\n\n\nNow with cross-validation:scores = cross_val_score(pipe, cancer.data, cancer.target)\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(pipe, cancer.data, cancer.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\n\n\n\nWe can retrieve the trained SVM by querying the right step indicespipe.steps[1][1]\n\npipe.fit(X_train, y_train)\nprint(\"SVM component: {}\".format(pipe.steps[1][1]))\n\n\n\nOr we can use the named_steps dictionarypipe.named_steps['svm']\n\nprint(\"SVM component: {}\".format(pipe.named_steps['svm']))\n\n\n\nWhen you don’t need specific names for specific steps, you can use make_pipeline\n\nAssigns names to steps automaticallypipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\nprint(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n\nfrom sklearn.pipeline import make_pipeline\n# abbreviated syntax\npipe_short = make_pipeline(MinMaxScaler(), LinearSVC(C=100))\nprint(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#building-pipelines","position":51},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Model selection and Hyperparameter tuning"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#model-selection-and-hyperparameter-tuning","position":52},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Model selection and Hyperparameter tuning"},"content":"There are many algorithms to choose from\n\nMost algorithms have parameters (hyperparameters) that control model complexity\n\nNow that we know how to evaluate models, we can improve them selecting by tuning algorithms for your data\n\nWe can basically use any optimization technique to optimize hyperparameters:\n\nGrid search\n\nRandom search\n\nMore advanced techniques:\n\nLocal search\n\nRacing algorithms\n\nBayesian optimization\n\nMulti-armed bandits\n\nGenetic algorithms\n\nGrid vs Random Search\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#model-selection-and-hyperparameter-tuning","position":53},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Grid Search","lvl2":"Model selection and Hyperparameter tuning"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#grid-search","position":54},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Grid Search","lvl2":"Model selection and Hyperparameter tuning"},"content":"For each hyperparameter, create a list of interesting/possible values\n\nE.g. For kNN: k in [1,3,5,7,9,11,33,55,77,99]\n\nE.g. For SVM: C and gamma in [\n\n10-10..\n\n1010]\n\nEvaluate all possible combinations of hyperparameter values\n\nE.g. using cross-validation\n\nSplit the training data into a training and validation set\n\nSelect the hyperparameter values yielding the best results on the validation set\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#grid-search","position":55},{"hierarchy":{"lvl1":"scikit-learn","lvl4":"Grid search in scikit-learn","lvl3":"Grid Search","lvl2":"Model selection and Hyperparameter tuning"},"type":"lvl4","url":"/notebooks/tutorial-3-machine-learning-in-python#grid-search-in-scikit-learn","position":56},{"hierarchy":{"lvl1":"scikit-learn","lvl4":"Grid search in scikit-learn","lvl3":"Grid Search","lvl2":"Model selection and Hyperparameter tuning"},"content":"Create a parameter grid as a dictionary\n\nKeys are parameter names\n\nValues are lists of hyperparameter valuesparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nprint(\"Parameter grid:\\n{}\".format(param_grid))\n\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nprint(\"Parameter grid:\\n{}\".format(param_grid))\n\n\n\nGridSearchCV: like a classifier that uses CV to automatically optimize its hyperparameters internally\n\nInput: (untrained) model, parameter grid, CV procedure\n\nOutput: optimized model on given training data\n\nShould only have access to training datagrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.svm import SVC\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\nX_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, random_state=0)\ngrid_search.fit(X_train, y_train)\n\n\n\nThe optimized test score and hyperparameters can easily be retrieved:grid_search.score(X_test, y_test)\ngrid_search.best_params_\ngrid_search.best_score_\ngrid_search.best_estimator_\n\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nprint(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#grid-search-in-scikit-learn","position":57},{"hierarchy":{"lvl1":"scikit-learn","lvl4":"Nested cross-validation","lvl3":"Grid Search","lvl2":"Model selection and Hyperparameter tuning"},"type":"lvl4","url":"/notebooks/tutorial-3-machine-learning-in-python#nested-cross-validation","position":58},{"hierarchy":{"lvl1":"scikit-learn","lvl4":"Nested cross-validation","lvl3":"Grid Search","lvl2":"Model selection and Hyperparameter tuning"},"content":"Note that we are still using a single split to create the outer test set\n\nWe can also use cross-validation here\n\nNested cross-validation:\n\nOuter loop: split data in training and test sets\n\nInner loop: run grid search, splitting the training data into train and validation sets\n\nResult is a just a list of scores\n\nThere will be multiple optimized models and hyperparameter settings (not returned)\n\nTo apply on future data, we need to train GridSearchCV on all data again\n\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\n\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#nested-cross-validation","position":59},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Random Search","lvl2":"Model selection and Hyperparameter tuning"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#random-search","position":60},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Random Search","lvl2":"Model selection and Hyperparameter tuning"},"content":"Grid Search has a few downsides:\n\nOptimizing many hyperparameters creates a combinatorial explosion\n\nYou have to predefine a grid, hence you may jump over optimal values\n\nRandom Search:\n\nPicks n_iter random parameter values\n\nScales better, you control the number of iterations\n\nOften works better in practice, too\n\nnot all hyperparameters interact strongly\n\nyou don’t need to explore all combinations\n\nExecuting random search in scikit-learn:\n\nRandomizedSearchCV works like GridSearchCV\n\nHas n_iter parameter for the number of iterations\n\nSearch grid can use distributions instead of fixed listsparam_grid = {'C': expon(scale=100), \n              'gamma': expon(scale=.1)}\nrandom_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,\n                                   n_iter=20)\nrandom_search.fit(X_train, y_train)\nrandom_search.best_estimator_\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import expon\n\nparam_grid = {'C': expon(scale=100), \n              'gamma': expon(scale=.1)}\nrandom_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,\n                                   n_iter=20)\nX_train, X_test, y_train, y_test = train_test_split(\n        iris.data, iris.target, random_state=0)\nrandom_search.fit(X_train, y_train)\nrandom_search.best_estimator_\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#random-search","position":61},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Using Pipelines in Grid-searches","lvl2":"Model selection and Hyperparameter tuning"},"type":"lvl3","url":"/notebooks/tutorial-3-machine-learning-in-python#using-pipelines-in-grid-searches","position":62},{"hierarchy":{"lvl1":"scikit-learn","lvl3":"Using Pipelines in Grid-searches","lvl2":"Model selection and Hyperparameter tuning"},"content":"We can use the pipeline as a single estimator in cross_val_score or GridSearchCV\n\nTo define a grid, refer to the hyperparameters of the steps\n\nStep svm, parameter C becomes svm__Cparam_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\npipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\n\nparam_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n\n\nfrom sklearn import pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n\npipe = pipeline.Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\n\n\n\n","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#using-pipelines-in-grid-searches","position":63},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Automated Machine Learning"},"type":"lvl2","url":"/notebooks/tutorial-3-machine-learning-in-python#automated-machine-learning","position":64},{"hierarchy":{"lvl1":"scikit-learn","lvl2":"Automated Machine Learning"},"content":"Optimizes both the pipeline and all hyperparameters\n\nE.g. auto-sklearn\n\nDrop-in sklearn classifier\n\nAlso optimizes pipelines (e.g. feature selection)\n\nUses OpenML to find good models on similar datasets\n\nLacks Windows supportautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=60, # sec., for entire process\n    per_run_time_limit=15, # sec., for each model\n    ml_memory_limit=1024, # MB, memory limit\n)\nautoml.fit(X_train, y_train)","type":"content","url":"/notebooks/tutorial-3-machine-learning-in-python#automated-machine-learning","position":65}]}