{"version":3,"kind":"Notebook","sha256":"c634c926631ee9daea86547f3a0e979d983976c5872d55ba0b3aab7324dae0b0","slug":"labs.lab-6-tutorial","location":"/labs/Lab 6 - Tutorial.ipynb","dependencies":[],"frontmatter":{"title":"Lab 6: Transformers Tutorial","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"mlcourse","language":"python"},"github":"https://github.com/ml-course/master","copyright":"2025. CC0 Licensed - Use as you like. Appropriate credit is very welcome","numbering":{"title":{"offset":1}},"source_url":"https://github.com/ml-course/master/blob/master/labs/Lab 6 - Tutorial.ipynb","edit_url":"https://github.com/ml-course/master/edit/master/labs/Lab 6 - Tutorial.ipynb","thumbnail":"/scaled_dot_product_a-ec97a6969b77b7bd12db5955a71707ea.svg","exports":[{"format":"ipynb","filename":"Lab 6 - Tutorial.ipynb","url":"/Lab 6 - Tutorial-4470c675759f750afd740071d8931c42.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"In this tutorial, we’ll reproduce, step by step, the model from the paper that first introduced the transformer architecture, ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"jcZl33A8rc"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Attention Is All You Need","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"fMBDawGhvA"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"lIQlHwm8jr"},{"type":"text","value":"), albeit only the encoder part.\nAfter that, we’ll do a number of small experiments to visualize and better understand the inner workings.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"XKFLITQez9"}],"key":"YQ3XXDSoAH"}],"key":"MelU5PAlwl"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np \nimport math\nfrom functools import partial\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n\n## PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torchvision\nfrom torchvision.datasets import CIFAR100\nfrom torchvision import transforms\n\n# PyTorch Lightning\ntry:\n    import pytorch_lightning as pl\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n    !pip install --quiet pytorch-lightning>=1.4\n    import pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nDATASET_PATH = \"data\"\nCHECKPOINT_PATH = \"saved_models\"\n\n# Setting the seed\npl.seed_everything(42)\n# Ensure that all operations are deterministic on GPU (if used) for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = \"cpu\"\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nprint(\"Device:\", device)","key":"clqezrVU9C"},{"type":"outputs","id":"x8jA2HVeKsqq6Y9nFGOBD","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"Seed set to 42\n"},"children":[],"key":"Io4Kd4P2rm"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Device: mps\n"},"children":[],"key":"h25dZTjDg4"}],"key":"d5B5PncJrI"}],"key":"J853UYiJOx"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Attention mechanism","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KiY9bHs78e"}],"identifier":"attention-mechanism","label":"Attention mechanism","html_id":"attention-mechanism","implicit":true,"key":"UZwiyUmshU"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys. In other words, we want to dynamically decide on which inputs we want to “attend” more than others. To implement the attention mechanism, there are four parts we need to specify:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YdWDYXulhq"}],"key":"jtwWJUGvem"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Query","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"w2uVFagmai"}],"key":"xnQQlHykg2"},{"type":"text","value":": The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"RM2QtYjEsD"}],"key":"lEpp4HSBPF"}],"key":"aVePNFDZkG"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Keys","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"eOp8yq4sjS"}],"key":"g8yLM7NQIN"},{"type":"text","value":": For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"teMz5ygCjr"}],"key":"a48vGgsCF4"}],"key":"IwnyG3FsTW"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"strong","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Values","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"JSxX7Dr4xh"}],"key":"MW2qQ5e5OZ"},{"type":"text","value":": For each input element, we also have a value vector. This feature vector is the one we want to average over.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Vw1wG5Ahg9"}],"key":"CiU7fcCSTe"}],"key":"xlRIeNogNg"}],"key":"d72J46wHUL"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"The weights of the average are calculated by a softmax over all score function outputs.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"rR7veUW1qL"}],"key":"wrU47qmf0b"}],"key":"wyfSIYswKI"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The dot product attention takes as input a set of queries ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"dqRsdzceoD"},{"type":"inlineMath","value":"Q\\in\\mathbb{R}^{T\\times d_k}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>T</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">Q\\in\\mathbb{R}^{T\\times d_k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"UOHCsmXfSf"},{"type":"text","value":", keys ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Usal85Vgl4"},{"type":"inlineMath","value":"K\\in\\mathbb{R}^{T\\times d_k}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>T</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">K\\in\\mathbb{R}^{T\\times d_k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"DqdfNfoW3T"},{"type":"text","value":" and values ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DgXCkT1CPx"},{"type":"inlineMath","value":"V\\in\\mathbb{R}^{T\\times d_v}","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>T</mi><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">V\\in\\mathbb{R}^{T\\times d_v}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1645em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"v1h6qrvbhB"},{"type":"text","value":" where ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"epyygdbsex"},{"type":"inlineMath","value":"T","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span></span></span></span>","key":"kGQ8Zsskw8"},{"type":"text","value":" is the sequence length, and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EqOEtfmLMM"},{"type":"inlineMath","value":"d_k","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_k</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"Pvvxolr6La"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FrugnMN1sJ"},{"type":"inlineMath","value":"d_v","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1514em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"vSsqmvA4dp"},{"type":"text","value":" are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YEJQ2OUrFn"},{"type":"inlineMath","value":"i","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span>","key":"TJQQ4X6tUc"},{"type":"text","value":" to ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"IZ54MwwwId"},{"type":"inlineMath","value":"j","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j</span></span></span></span>","key":"PlJfALJkcH"},{"type":"text","value":" is based on its similarity of the query ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XzMgZFleiH"},{"type":"inlineMath","value":"Q_i","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>Q</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">Q_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">Q</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"VzNFpqIKgf"},{"type":"text","value":" and key ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"YKdmJjbI4g"},{"type":"inlineMath","value":"K_j","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub></mrow><annotation encoding=\"application/x-tex\">K_j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.9694em;vertical-align:-0.2861em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3117em;\"><span style=\"top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2861em;\"><span></span></span></span></span></span></span></span></span></span>","key":"S5djuZf0AL"},{"type":"text","value":", using the dot product as the similarity metric.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DH00F8dswM"}],"key":"DszgcOLnem"},{"type":"math","value":"\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Attention</mtext><mo stretchy=\"false\">(</mo><mi>Q</mi><mo separator=\"true\">,</mo><mi>K</mi><mo separator=\"true\">,</mo><mi>V</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence=\"true\">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence=\"true\">)</mo></mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">Attention</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">Q</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.4684em;vertical-align:-0.95em;\"></span><span class=\"mord text\"><span class=\"mord\">softmax</span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.5183em;\"><span style=\"top:-2.2528em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8572em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.8172em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1828em;\"><span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">Q</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.93em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span>","enumerator":"1","key":"e2drsU2C6d"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The matrix multiplication ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AtTizcrk6Z"},{"type":"inlineMath","value":"QK^T","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">QK^T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0358em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span></span></span></span>","key":"HQTYMX40uE"},{"type":"text","value":" performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"vxJcI54ivg"},{"type":"inlineMath","value":"T\\times T","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi><mo>×</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T\\times T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span></span></span></span>","key":"cbfjrRJolo"},{"type":"text","value":". Each row represents the attention logits for a specific element ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"oL30T7M694"},{"type":"inlineMath","value":"i","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span>","key":"KAOCwiL7Aa"},{"type":"text","value":" to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ybmdPwt0mt"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Vaswani et al., 2017","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"HkDW6jKUKz"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"N8sM93swzD"},{"type":"text","value":").","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TkabL1QniS"}],"key":"LCKHaWrpV6"},{"type":"paragraph","children":[{"type":"image","url":"/scaled_dot_product_a-ec97a6969b77b7bd12db5955a71707ea.svg","width":"210px","key":"Wsie0kGvOO","urlSource":"../notebooks/images/scaled_dot_product_attn.svg"}],"key":"tnJC2y4ZuU"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"The scaling factor of ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"f8ZBixcrRZ"},{"type":"inlineMath","value":"1/\\sqrt{d_k}","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn><mi mathvariant=\"normal\">/</mi><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding=\"application/x-tex\">1/\\sqrt{d_k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1072em;vertical-align:-0.25em;\"></span><span class=\"mord\">1/</span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8572em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.8172em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1828em;\"><span></span></span></span></span></span></span></span></span>","key":"yjEx9HhyBT"},{"type":"text","value":" is crucial to maintain an appropriate variance of attention values after initialization. The block ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"f3JJP0FNnb"},{"type":"inlineCode","value":"Mask (opt.)","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"YyPUkJFfpe"},{"type":"text","value":" in the diagram above represents the optional masking of specific entries in the attention matrix. This is usually done by setting the respective attention logits to a very low value.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"EVGGyjbmB3"}],"key":"J7k5Z3rKJy"},{"type":"paragraph","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"We can write a function below which computes the output features given the triple of queries, keys, and values:","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"X7QSFCZU0I"}],"key":"OZDqCbuaBg"}],"key":"x8N9xWZwjB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def scaled_dot_product(q, k, v, mask=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention","key":"H1BElPSXol"},{"type":"outputs","id":"nL6TbLW-th10Uz7FILFrN","children":[],"key":"yJss5B0Z1S"}],"key":"xryKoiituy"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Let’s generate a few random queries, keys, and value vectors, and calculate the attention outputs:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FAxeGsklu1"}],"key":"udy6nA2bOZ"}],"key":"eaG20jLweH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"seq_len, d_k = 3, 2\npl.seed_everything(42)\nq = torch.randn(seq_len, d_k)\nk = torch.randn(seq_len, d_k)\nv = torch.randn(seq_len, d_k)\nvalues, attention = scaled_dot_product(q, k, v)\nprint(\"Q\\n\", q)\nprint(\"K\\n\", k)\nprint(\"V\\n\", v)\nprint(\"Values\\n\", values)\nprint(\"Attention\\n\", attention)","key":"CemqKrOiHS"},{"type":"outputs","id":"wgk0DF0OHFkaChtKwxR16","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"Seed set to 42\n"},"children":[],"key":"Q6ZPYF7mrN"},{"type":"output","jupyter_data":{"name":"stdout","output_type":"stream","text":"Q\n tensor([[ 0.3367,  0.1288],\n        [ 0.2345,  0.2303],\n        [-1.1229, -0.1863]])\nK\n tensor([[ 2.2082, -0.6380],\n        [ 0.4617,  0.2674],\n        [ 0.5349,  0.8094]])\nV\n tensor([[ 1.1103, -1.6898],\n        [-0.9890,  0.9580],\n        [ 1.3221,  0.8172]])\nValues\n tensor([[ 0.5698, -0.1520],\n        [ 0.5379, -0.0265],\n        [ 0.2246,  0.5556]])\nAttention\n tensor([[0.4028, 0.2886, 0.3086],\n        [0.3538, 0.3069, 0.3393],\n        [0.1303, 0.4630, 0.4067]])\n"},"children":[],"key":"z9kE1pxn1V"}],"key":"lr2MCzzaZc"}],"key":"nvfmTXDZNM"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Multi-Head Attention","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"i7gq9JqMdR"}],"identifier":"multi-head-attention","label":"Multi-Head Attention","html_id":"multi-head-attention","implicit":true,"key":"ajHkyBTFgD"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. We refer to this as Multi-Head Attention layer with the learnable parameters ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lbYjAHP4OL"},{"type":"inlineMath","value":"W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>1...</mn><mi>h</mi></mrow><mi>Q</mi></msubsup><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>D</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2605em;vertical-align:-0.3013em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9592em;\"><span style=\"top:-2.3987em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1...</span><span class=\"mord mathnormal mtight\">h</span></span></span></span><span style=\"top:-3.1809em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">Q</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3013em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"brt02r9Ws7"},{"type":"text","value":", ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Fgi6Gu7TDW"},{"type":"inlineMath","value":"W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>1...</mn><mi>h</mi></mrow><mi>K</mi></msubsup><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>D</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1244em;vertical-align:-0.2831em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-2.4169em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1...</span><span class=\"mord mathnormal mtight\">h</span></span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2831em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"uTu2vZ0OrT"},{"type":"text","value":", ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"o62dFUH5Ax"},{"type":"inlineMath","value":"W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msubsup><mi>W</mi><mrow><mn>1...</mn><mi>h</mi></mrow><mi>V</mi></msubsup><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>D</mi><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1244em;vertical-align:-0.2831em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-2.4169em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1...</span><span class=\"mord mathnormal mtight\">h</span></span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2831em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1645em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"taCQewLX6g"},{"type":"text","value":", and ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d8se1IqLYY"},{"type":"inlineMath","value":"W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mi>O</mi></msup><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>h</mi><mo>⋅</mo><msub><mi>d</mi><mi>v</mi></msub><mo>×</mo><msub><mi>d</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">W^{O}\\in\\mathbb{R}^{h\\cdot d_v\\times d_{out}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8804em;vertical-align:-0.0391em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">O</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">h</span><span class=\"mbin mtight\">⋅</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1645em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2963em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">o</span><span class=\"mord mathnormal mtight\">u</span><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"EyHqOcJULm"},{"type":"text","value":" (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"VvlIjEkC35"},{"type":"inlineMath","value":"D","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">D</span></span></span></span>","key":"UvKtaBdhl0"},{"type":"text","value":" being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"NZq5RSPDyw"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Vaswani et al., 2017","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"WXJRkFuAim"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"AClS3HAAKy"},{"type":"text","value":").","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"hdoo1g0oE3"}],"key":"MjfxtBr6ny"},{"type":"paragraph","children":[{"type":"image","url":"/multihead_attention-d198b707c71afa1684f390babb21bca5.svg","width":"230px","key":"X1eseYH6gs","urlSource":"../notebooks/images/multihead_attention.svg"}],"key":"cCG41lUYN0"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"How are we applying a Multi-Head Attention layer in a neural network, where we don’t have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"l9gxkKhc6n"},{"type":"inlineMath","value":"X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mi>B</mi><mo>×</mo><mi>T</mi><mo>×</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathbb\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.05017em;\">B</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">T</span><span class=\"mbin mtight\">×</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3488em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">model</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1512em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>","key":"amWXJLFGCp"},{"type":"text","value":", as ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"utzy1U67St"},{"type":"inlineMath","value":"Q","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span></span></span></span>","key":"QcISoluR43"},{"type":"text","value":", ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Ytj8xHKKel"},{"type":"inlineMath","value":"K","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span>","key":"LOrwbqECoj"},{"type":"text","value":" and ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"IrUHeUD9F7"},{"type":"inlineMath","value":"V","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span>","key":"IDyWkkAyeY"},{"type":"text","value":" (","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Jf1XIbnsol"},{"type":"inlineMath","value":"B","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span>","key":"GRD23jwrB1"},{"type":"text","value":" being the batch size, ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"jwPVrejOdq"},{"type":"inlineMath","value":"T","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">T</span></span></span></span>","key":"IOEx90pWsD"},{"type":"text","value":" the sequence length, ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"cnJvSJvecY"},{"type":"inlineMath","value":"d_{\\text{model}}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding=\"application/x-tex\">d_{\\text{model}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">model</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"cixkXJqQgr"},{"type":"text","value":" the hidden dimensionality of ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"WOI1AUqwAn"},{"type":"inlineMath","value":"X","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>","key":"F4KfqQcQdS"},{"type":"text","value":"). The consecutive weight matrices ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"aczgTqsUmN"},{"type":"inlineMath","value":"W^{Q}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mi>Q</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^{Q}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">Q</span></span></span></span></span></span></span></span></span></span></span></span>","key":"WE4vblAGoO"},{"type":"text","value":", ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"JvpxYbblF5"},{"type":"inlineMath","value":"W^{K}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mi>K</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^{K}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span></span></span></span></span></span></span></span></span>","key":"RdvA5fKQPT"},{"type":"text","value":", and ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"HYRPpSsgOO"},{"type":"inlineMath","value":"W^{V}","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>W</mi><mi>V</mi></msup></mrow><annotation encoding=\"application/x-tex\">W^{V}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.22222em;\">V</span></span></span></span></span></span></span></span></span></span></span></span>","key":"hmhJj3awLU"},{"type":"text","value":" can transform ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"fAHgWWOrEG"},{"type":"inlineMath","value":"X","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>","key":"NLPB9GOgHv"},{"type":"text","value":" to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"nQd35Z9ap2"}],"key":"pySbWb2N3w"}],"key":"HlTCES2bez"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Helper function to support different mask shapes.\n# Output shape supports (batch_size, number of heads, seq length, seq length)\n# If 2D: broadcasted over batch size and number of heads\n# If 3D: broadcasted over number of heads\n# If 4D: leave as is\ndef expand_mask(mask):\n    assert mask.ndim >= 2, \"Mask must be at least 2-dimensional with seq_length x seq_length\"\n    if mask.ndim == 3:\n        mask = mask.unsqueeze(1)\n    while mask.ndim < 4:\n        mask = mask.unsqueeze(0)\n    return mask","key":"Z5e2OKXT4C"},{"type":"outputs","id":"cdDTzexPUePe9NK9X8XSg","children":[],"key":"XyhxBND0J5"}],"key":"cJM0DeO9vU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class MultiheadAttention(nn.Module):\n    \n    def __init__(self, input_dim, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n        \n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Stack all weight matrices 1...h together for efficiency\n        # Note that in many implementations you see \"bias=False\" which is optional\n        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n        self.o_proj = nn.Linear(embed_dim, input_dim)\n        \n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        # Original Transformer initialization, see PyTorch documentation\n        nn.init.xavier_uniform_(self.qkv_proj.weight)\n        self.qkv_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.o_proj.weight)\n        self.o_proj.bias.data.fill_(0)\n\n    def forward(self, x, mask=None, return_attention=False):\n        batch_size, seq_length, _ = x.size()\n        if mask is not None:\n            mask = expand_mask(mask)\n        qkv = self.qkv_proj(x)\n        \n        # Separate Q, K, V from linear output\n        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Determine value outputs\n        values, attention = scaled_dot_product(q, k, v, mask=mask)\n        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        values = values.reshape(batch_size, seq_length, self.embed_dim)\n        o = self.o_proj(values)\n        \n        if return_attention:\n            return o, attention\n        else:\n            return o","key":"gTYhrzKOtM"},{"type":"outputs","id":"DnIvy0OACC02PhyMcoiFV","children":[],"key":"QabxUdPBIk"}],"key":"Erv7u9O5tq"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Transformer Encoder","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Pr45ZIQm5o"}],"identifier":"transformer-encoder","label":"Transformer Encoder","html_id":"transformer-encoder","implicit":true,"key":"F3xeI3RDdt"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Next, we will look at how to apply the multi-head attention block inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner. We will focus here on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"LSDDFPX5k5"},{"type":"link","url":"https://arxiv.org/abs/1706.03762","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Vaswani et al., 2017","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SZjRGXmnoL"}],"urlSource":"https://arxiv.org/abs/1706.03762","key":"jLWdjUCciY"},{"type":"text","value":").:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"np4fkIkBF4"}],"key":"lZiRblZ5Lk"},{"type":"paragraph","children":[{"type":"image","url":"/transformer_architec-511dacf16462b4796f7c4fee3a575a0b.svg","width":"400px","key":"VuZ92pHoug","urlSource":"../notebooks/images/transformer_architecture.svg"}],"key":"dqOTI2Mu3I"},{"type":"paragraph","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The encoder consists of ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KxNwJHv4jk"},{"type":"inlineMath","value":"N","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span>","key":"zSL28BnxKa"},{"type":"text","value":" identical blocks that are applied in sequence. Taking as input ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"j5eNIis0TB"},{"type":"inlineMath","value":"x","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>","key":"jNZ69Ajhct"},{"type":"text","value":", it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"GJDZllHpqV"},{"type":"inlineMath","value":"\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>LayerNorm</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo>+</mo><mtext>Multihead</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">LayerNorm</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord\">Multihead</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">))</span></span></span></span>","key":"GrfBlOaAE9"},{"type":"text","value":" (","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"CpYwnbeVPt"},{"type":"inlineMath","value":"x","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>","key":"zeRIheFAvr"},{"type":"text","value":" being ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"QvvnkxhS9C"},{"type":"inlineMath","value":"Q","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8778em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">Q</span></span></span></span>","key":"pyvGvL1Kib"},{"type":"text","value":", ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Nqo94VUAgN"},{"type":"inlineMath","value":"K","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi></mrow><annotation encoding=\"application/x-tex\">K</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07153em;\">K</span></span></span></span>","key":"VXYH5IzRng"},{"type":"text","value":" and ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"uJRSGKf25k"},{"type":"inlineMath","value":"V","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>V</mi></mrow><annotation encoding=\"application/x-tex\">V</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span></span></span></span>","key":"iKPbDfFFoX"},{"type":"text","value":" input to the attention layer). The residual connection is crucial for enabling a smooth gradient flow through the model, and to make sure that the information about the original sequence isn’t lost. The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. Finally, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. The full transformation including the residual connection can be expressed as:","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"YzU3iA2QrV"}],"key":"jIscPbJwd2"},{"type":"math","value":"\\begin{split}\n    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n\\end{split}","position":{"start":{"line":9,"column":1},"end":{"line":14,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mtable rowspacing=\"0.25em\" columnalign=\"right left\" columnspacing=\"0em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mtext>FFN</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy=\"false\">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"true\"><mrow><mrow></mrow><mo>=</mo><mtext>LayerNorm</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo>+</mo><mtext>FFN</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding=\"application/x-tex\">\\begin{split}\n    \\text{FFN}(x) &amp; = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n    x &amp; = \\text{LayerNorm}(x + \\text{FFN}(x))\n\\end{split}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:3em;vertical-align:-1.25em;\"></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-r\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.75em;\"><span style=\"top:-3.91em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">FFN</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span><span style=\"top:-2.41em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.25em;\"><span></span></span></span></span></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.75em;\"><span style=\"top:-3.91em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mop\">max</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.41em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord text\"><span class=\"mord\">LayerNorm</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mord text\"><span class=\"mord\">FFN</span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">))</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.25em;\"><span></span></span></span></span></span></span></span></span></span></span></span>","enumerator":"2","key":"Rl12WJ5I88"},{"type":"paragraph","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Finally, we can start implementing the architecture below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization.","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"JN4TqXSEvS"}],"key":"HyeXUzBMHb"}],"key":"UWeLIaYV4u"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class EncoderBlock(nn.Module):\n    \n    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Dimensionality of the input\n            num_heads - Number of heads to use in the attention block\n            dim_feedforward - Dimensionality of the hidden layer in the MLP\n            dropout - Dropout probability to use in the dropout layers\n        \"\"\"\n        super().__init__()\n        \n        # Attention layer\n        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n        \n        # Two-layer MLP\n        self.linear_net = nn.Sequential(\n            nn.Linear(input_dim, dim_feedforward),\n            nn.Dropout(dropout),\n            nn.ReLU(inplace=True),\n            nn.Linear(dim_feedforward, input_dim)\n        )\n        \n        # Layers to apply in between the main layers\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask=None):\n        # Attention part\n        attn_out = self.self_attn(x, mask=mask)\n        x = x + self.dropout(attn_out)\n        x = self.norm1(x)\n        \n        # MLP part\n        linear_out = self.linear_net(x)\n        x = x + self.dropout(linear_out)\n        x = self.norm2(x)\n        \n        return x","key":"ypIGSrF5lo"},{"type":"outputs","id":"B8QApn7g9AlDCg5gbjWjh","children":[],"key":"WkyrURzVwh"}],"key":"SVv9cBw573"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"txFEA6QGgk"},{"type":"inlineCode","value":"get_attention_maps","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sxYrpfZWIV"},{"type":"text","value":". The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding the model later. However, the attention probabilities should be interpreted with a grain of salt as it ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ijGlnwF8Ck"},{"type":"link","url":"https://arxiv.org/abs/1902.10186","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"does not necessarily reflect the true interpretation of the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fgt1hT64zh"}],"urlSource":"https://arxiv.org/abs/1902.10186","key":"peDsVZtyXM"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uasb3jgl4K"}],"key":"Vh3gviOHKe"}],"key":"KAsR7VbbV0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class TransformerEncoder(nn.Module):\n    \n    def __init__(self, num_layers, **block_args):\n        super().__init__()\n        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n\n    def forward(self, x, mask=None):\n        for l in self.layers:\n            x = l(x, mask=mask)\n        return x\n\n    def get_attention_maps(self, x, mask=None):\n        attention_maps = []\n        for l in self.layers:\n            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n            attention_maps.append(attn_map)\n            x = l(x)\n        return attention_maps","key":"Ih4PvBG39f"},{"type":"outputs","id":"ggVM4j45fJm6L2w5Ut75C","children":[],"key":"sQ5jKLTzkp"}],"key":"eYvRIdz57u"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Positional encoding","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aoTBqxvZR2"}],"identifier":"positional-encoding","label":"Positional encoding","html_id":"positional-encoding","implicit":true,"key":"RHqvCktlcC"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. However, we can use patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ppeo4whmG8"}],"key":"MgXe01qQil"},{"type":"math","value":"PE_{(pos,i)} = \\begin{cases}\n    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n\\end{cases}","position":{"start":{"line":5,"column":1},"end":{"line":10,"column":1}},"html":"<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.36em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>sin</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1000</mn><msup><mn>0</mn><mrow><mi>i</mi><mi mathvariant=\"normal\">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if</mtext><mspace width=\"0.8536em\"/><mi>i</mi><mtext> mod </mtext><mn>2</mn><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>cos</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><mrow><mn>1000</mn><msup><mn>0</mn><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">PE_{(pos,i)} = \\begin{cases}\n    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) &amp; \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) &amp; \\text{otherwise}\\\\\n\\end{cases}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0385em;vertical-align:-0.3552em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">os</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.6em;vertical-align:-1.55em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-2.5em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎩</span></span></span><span style=\"top:-2.492em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span style=\"height:0.016em;width:0.8889em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='0.8889em' height='0.016em' style='width:0.8889em' viewBox='0 0 888.89 16' preserveAspectRatio='xMinYMin'><path d='M384 0 H504 V16 H384z M384 0 H504 V16 H384z'/></svg></span></span><span style=\"top:-3.15em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎨</span></span></span><span style=\"top:-4.292em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span style=\"height:0.016em;width:0.8889em;\"><svg xmlns=\"http://www.w3.org/2000/svg\" width='0.8889em' height='0.016em' style='width:0.8889em' viewBox='0 0 888.89 16' preserveAspectRatio='xMinYMin'><path d='M384 0 H504 V16 H384z M384 0 H504 V16 H384z'/></svg></span></span><span style=\"top:-4.3em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎧</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.05em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"mord\"><span class=\"mop\">sin</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7475em;\"><span style=\"top:-2.5648em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1000</span><span class=\"mord mtight\"><span class=\"mord mtight\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8932em;\"><span style=\"top:-2.8932em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5357em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">i</span><span class=\"mord mtight\">/</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3448em;margin-left:0em;margin-right:0.1em;\"><span class=\"pstrut\" style=\"height:2.6944em;\"></span><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">model</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3496em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.4461em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">os</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4352em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">)</span></span></span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"mord\"><span class=\"mop\">cos</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7475em;\"><span style=\"top:-2.5648em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1000</span><span class=\"mord mtight\"><span class=\"mord mtight\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8932em;\"><span style=\"top:-2.8932em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5357em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span><span class=\"mord mtight\">/</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3448em;margin-left:0em;margin-right:0.1em;\"><span class=\"pstrut\" style=\"height:2.6944em;\"></span><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">model</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3496em;\"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.4461em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">os</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4352em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">)</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:1em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.05em;\"><span style=\"top:-4.05em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">if</span></span><span class=\"mspace\" style=\"margin-right:0.8536em;\"></span><span class=\"mord mathnormal\">i</span><span class=\"mord text\"><span class=\"mord\"> mod </span></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mord\">0</span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.15em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">otherwise</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.55em;\"><span></span></span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>","enumerator":"3","key":"l4ERDvhfAK"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"children":[{"type":"inlineMath","value":"PE_{(pos,i)}","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">PE_{(pos,i)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0385em;vertical-align:-0.3552em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">os</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span></span></span></span>","key":"MPOQyUZTT3"},{"type":"text","value":" represents the position encoding at position ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"RSGxXJI7Is"},{"type":"inlineMath","value":"pos","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">pos</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mord mathnormal\">os</span></span></span></span>","key":"Z6UsYvl2sJ"},{"type":"text","value":" in the sequence, and hidden dimensionality ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"np3svZLLEy"},{"type":"inlineMath","value":"i","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6595em;\"></span><span class=\"mord mathnormal\">i</span></span></span></span>","key":"evdUhW5Vk0"},{"type":"text","value":". The intuition behind this encoding is that you can represent ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"BNTuN8B0uV"},{"type":"inlineMath","value":"PE_{(pos+k,:)}","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi><mo separator=\"true\">,</mo><mo>:</mo><mo stretchy=\"false\">)</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">PE_{(pos+k,:)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0385em;vertical-align:-0.3552em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">os</span><span class=\"mbin mtight\">+</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mpunct mtight\">,</span><span class=\"mrel mtight\">:</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span></span></span></span>","key":"eDzzOsEaLo"},{"type":"text","value":" as a linear function of ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"WDHcRuxgym"},{"type":"inlineMath","value":"PE_{(pos,:)}","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy=\"false\">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator=\"true\">,</mo><mo>:</mo><mo stretchy=\"false\">)</mo></mrow></msub></mrow><annotation encoding=\"application/x-tex\">PE_{(pos,:)}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0385em;vertical-align:-0.3552em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05764em;\">E</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">p</span><span class=\"mord mathnormal mtight\">os</span><span class=\"mpunct mtight\">,</span><span class=\"mrel mtight\">:</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span></span></span></span>","key":"o9jE7HLTQ3"},{"type":"text","value":", which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"QnnuwODCCS"},{"type":"inlineMath","value":"2\\pi","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">2\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span></span></span></span>","key":"YlnLwFtABB"},{"type":"text","value":" to ","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"iAsdouCiRY"},{"type":"inlineMath","value":"10000\\cdot 2\\pi","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>10000</mn><mo>⋅</mo><mn>2</mn><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">10000\\cdot 2\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">10000</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span></span></span></span>","key":"ghw65IbJra"},{"type":"text","value":". The positional encoding is implemented below.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"qUCN8ZWSoB"}],"key":"RdzsyAlbnC"}],"key":"KHFJnBx98N"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, max_len=5000):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        \n        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x","key":"htj1dnaUtd"},{"type":"outputs","id":"tNdSE8y_yM73f8_yCsAxB","children":[],"key":"QCQM37nAUt"}],"key":"YW58qLUViE"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let’s do it below.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EfZhe3cpxR"}],"key":"JUcpRvZp7u"}],"key":"Hx0kaO56TT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"encod_block = PositionalEncoding(d_model=48, max_len=96)\npe = encod_block.pe.squeeze().T.cpu().numpy()\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\npos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\nfig.colorbar(pos, ax=ax)\nax.set_xlabel(\"Position in sequence\")\nax.set_ylabel(\"Hidden dimension\")\nax.set_title(\"Positional encoding over hidden dimensions\")\nax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\nax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\nplt.show()","key":"NKb836O976"},{"type":"outputs","id":"KpI9EXAD7SI_NShHNGvHl","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"dfc8ce7cbea39ffc1fbd7703e739e409","path":"/dfc8ce7cbea39ffc1fbd7703e739e409.png"},"text/plain":{"content":"<Figure size 800x300 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"jtAK5mqReQ"}],"key":"jsHzx1wORH"}],"key":"HCtgjpK6Yo"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ps2mgwHfjY"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DYC273JYsc"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iFtCUYk1Ef"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"psxglKs9e2"},{"type":"text","value":", ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zmdvZvhFLx"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x7jfN3zcVJ"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w9FXoU1vtz"},{"type":"text","value":"4","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mhaAEANaz3"},{"type":"text","value":".","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"W403TnQ8Yk"}],"key":"zsRembk6pz"}],"key":"u3QYanB9bV"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"sns.set_theme()\nfig, ax = plt.subplots(2, 2, figsize=(12,4))\nax = [a for a_list in ax for a in a_list]\nfor i in range(len(ax)):\n    ax[i].plot(np.arange(1,17), pe[i,:16], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n    ax[i].set_title(f\"Encoding in hidden dimension {i+1}\")\n    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n    ax[i].set_xticks(np.arange(1,17))\n    ax[i].tick_params(axis='both', which='major', labelsize=10)\n    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n    ax[i].set_ylim(-1.2, 1.2)\nfig.subplots_adjust(hspace=0.8)\nsns.reset_orig()\nplt.show()","key":"ttbZYkdVZq"},{"type":"outputs","id":"Rr84q0KN89z8teiGh_PDY","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"75271b2b2af8d30005ca68232df581a9","path":"/75271b2b2af8d30005ca68232df581a9.png"},"text/plain":{"content":"<Figure size 1200x400 with 4 Axes>","content_type":"text/plain"}}},"children":[],"key":"XnwmJb8Rzb"}],"key":"g1sELqnpp9"}],"key":"xIWpnr854Q"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"As we can see, the patterns between the hidden dimension ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"VczgZK6aB7"},{"type":"text","value":"1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mXacHjvZkr"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rYGSUMA4l8"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rVb4z78s8S"},{"type":"text","value":" only differ in the starting angle. The wavelength is ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qNE4kslxgY"},{"type":"inlineMath","value":"2\\pi","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>2</mn><mi>π</mi></mrow><annotation encoding=\"application/x-tex\">2\\pi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">π</span></span></span></span>","key":"qvE6xmaoZB"},{"type":"text","value":", hence the repetition after position ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WRXYA86yjM"},{"type":"text","value":"6","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZbBoXXCoPA"},{"type":"text","value":". The hidden dimensions ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mwITTQRhns"},{"type":"text","value":"2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"iFoc1bjTHd"},{"type":"text","value":" and ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OUafHAJ4ty"},{"type":"text","value":"3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"reVkm9Zzpm"},{"type":"text","value":" have about twice the wavelength.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"y1d0DOzAWz"}],"key":"HzDZ6iaXPS"}],"key":"vq85oWMata"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Learning rate warm-up","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SVkwCDczdi"}],"identifier":"learning-rate-warm-up","label":"Learning rate warm-up","html_id":"learning-rate-warm-up","implicit":true,"key":"REoyZ1jjZh"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OAjyMlpEXS"}],"key":"NLaAvlMrh5"}],"key":"Gjbn1ymKpo"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n    \n    def __init__(self, optimizer, warmup, max_iters):\n        self.warmup = warmup\n        self.max_num_iters = max_iters\n        super().__init__(optimizer)\n        \n    def get_lr(self):\n        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n        return [base_lr * lr_factor for base_lr in self.base_lrs]\n    \n    def get_lr_factor(self, epoch):\n        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n        if epoch <= self.warmup:\n            lr_factor *= epoch * 1.0 / self.warmup\n        return lr_factor","key":"MYUkpnGdhD"},{"type":"outputs","id":"HxLiwj1H4K5d1aeT68ido","children":[],"key":"dRGsJU9NOf"}],"key":"nBEszPiZBP"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Needed for initializing the lr scheduler\np = nn.Parameter(torch.empty(4,4))\noptimizer = optim.Adam([p], lr=1e-3)\nlr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n\n# Plotting\nepochs = list(range(2000))\nsns.set()\nplt.figure(figsize=(8,3))\nplt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\nplt.ylabel(\"Learning rate factor\")\nplt.xlabel(\"Iterations (in batches)\")\nplt.title(\"Cosine Warm-up Learning Rate Scheduler\")\nplt.show()\nsns.reset_orig()","key":"hHqhlFvngl"},{"type":"outputs","id":"3aeFGn5pW0T5G_bc5jWix","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"fa203f1bd620987abfa757e4977db2b0","path":"/fa203f1bd620987abfa757e4977db2b0.png"},"text/plain":{"content":"<Figure size 800x300 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"ZlETSlGqTb"}],"key":"pSAoXClQWe"}],"key":"STYr0WPX7W"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"PyTorch Lightning Module","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZJPV6QbXvZ"}],"identifier":"pytorch-lightning-module","label":"PyTorch Lightning Module","html_id":"pytorch-lightning-module","implicit":true,"key":"ID65Fzf4d0"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Finally, we can embed the Transformer architecture into a PyTorch lightning module. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"sJHBOerxBu"},{"type":"inlineCode","value":"[CLS]","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"dP58ryFTtm"},{"type":"text","value":" token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d7RfyoG6Fq"}],"key":"M63NmrMvE8"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Additionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). The training, validation, and test step is left empty for now and will be filled for our task-specific models.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"p5h9aLgjeM"}],"key":"bJr67jsAFU"}],"key":"oFCWNIgZBT"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class TransformerPredictor(pl.LightningModule):\n\n    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n        \"\"\"\n        Inputs:\n            input_dim - Hidden dimensionality of the input\n            model_dim - Hidden dimensionality to use inside the Transformer\n            num_classes - Number of classes to predict per sequence element\n            num_heads - Number of heads to use in the Multi-Head Attention blocks\n            num_layers - Number of encoder blocks to use.\n            lr - Learning rate in the optimizer\n            warmup - Number of warmup steps. Usually between 50 and 500\n            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n            dropout - Dropout to apply inside the model\n            input_dropout - Dropout to apply on the input features\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self._create_model()\n\n    def _create_model(self):\n        # Input dim -> Model dim\n        self.input_net = nn.Sequential(\n            nn.Dropout(self.hparams.input_dropout),\n            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n        )\n        # Positional encoding for sequences\n        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n        # Transformer\n        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n                                              input_dim=self.hparams.model_dim,\n                                              dim_feedforward=2*self.hparams.model_dim,\n                                              num_heads=self.hparams.num_heads,\n                                              dropout=self.hparams.dropout)\n        # Output classifier per sequence lement\n        self.output_net = nn.Sequential(\n            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n            nn.LayerNorm(self.hparams.model_dim),\n            nn.ReLU(inplace=True),\n            nn.Dropout(self.hparams.dropout),\n            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n        ) \n\n    def forward(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Inputs:\n            x - Input features of shape [Batch, SeqLen, input_dim]\n            mask - Mask to apply on the attention outputs (optional)\n            add_positional_encoding - If True, we add the positional encoding to the input.\n                                      Might not be desired for some tasks.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        x = self.transformer(x, mask=mask)\n        x = self.output_net(x)\n        return x\n\n    @torch.no_grad()\n    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n        \"\"\"\n        Function for extracting the attention matrices of the whole Transformer for a single batch.\n        Input arguments same as the forward pass.\n        \"\"\"\n        x = self.input_net(x)\n        if add_positional_encoding:\n            x = self.positional_encoding(x)\n        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n        return attention_maps\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n        \n        # Apply lr scheduler per step\n        lr_scheduler = CosineWarmupScheduler(optimizer, \n                                             warmup=self.hparams.warmup, \n                                             max_iters=self.hparams.max_iters)\n        return [optimizer], [{'scheduler': lr_scheduler, 'interval': 'step'}]\n\n    def training_step(self, batch, batch_idx):\n        raise NotImplementedError\n\n    def validation_step(self, batch, batch_idx):\n        raise NotImplementedError    \n\n    def test_step(self, batch, batch_idx):\n        raise NotImplementedError   ","key":"tKIR4U2PZJ"},{"type":"outputs","id":"9zE47xPo_ikLQISBNjiiz","children":[],"key":"uW5HL8qhyO"}],"key":"d964SeHgKA"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"That’s it for now. You are now ready to start the labs.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mvsqP9JAVv"}],"key":"VEdFfA4rJn"}],"key":"JSYU0NyIP8"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Further reading","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CP5qo2MQpH"}],"identifier":"further-reading","label":"Further reading","html_id":"further-reading","implicit":true,"key":"VG8cyidFbm"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QAe5HbMSIW"}],"urlSource":"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html","key":"YQskPWz0RC"},{"type":"text","value":" - The original Google blog post about the Transformer paper, focusing on the application in machine translation.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nRH3RvD5q7"}],"key":"vZ17LUkyvb"}],"key":"zRoYkFxIYJ"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"http://jalammar.github.io/illustrated-transformer/","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"The Illustrated Transformer (Jay Alammar, 2018)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"tSp1UHJIjM"}],"urlSource":"http://jalammar.github.io/illustrated-transformer/","key":"yfzgMSxiYu"},{"type":"text","value":" - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"SMvrRxv5jv"}],"key":"TR1qORyUOy"}],"key":"imu0BUVpjj"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Attention? Attention! (Lilian Weng, 2018)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"gL720ekhTH"}],"urlSource":"https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html","key":"hb6XlfIuhk"},{"type":"text","value":" - A nice blog post summarizing attention mechanisms in many domains including vision.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"soInYPwc4v"}],"key":"FCBYUBoQxF"}],"key":"GYwUK4RwJz"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Illustrated: Self-Attention (Raimi Karim, 2019)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"fUa3aJTWOP"}],"urlSource":"https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a","key":"K90ugqCRdl"},{"type":"text","value":" - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Lwnxo3vvhU"}],"key":"a10rfkNKQq"}],"key":"tAmiXHcIBj"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"link","url":"https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"The Transformer family (Lilian Weng, 2020)","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"KcYl2N0jxg"}],"urlSource":"https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html","key":"iaO9Pw9PX7"},{"type":"text","value":" - A very detailed blog post reviewing more variants of Transformers besides the original one.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"AjCSpEQGSx"}],"key":"mWlBvkxCWk"}],"key":"TtgC4SCuRk"}],"key":"YMEo75ZiUz"}],"key":"KXGVXbfgqc"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"This tutorial was greatly inspired by the ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zxTsyYji50"},{"type":"link","url":"https://uvadlc-notebooks.readthedocs.io/","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"UvA Deep Learning tutorials","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gt8CJjgvtN"}],"urlSource":"https://uvadlc-notebooks.readthedocs.io/","key":"FLwbR9L40z"}],"key":"KENcIumuXZ"}],"key":"oEcX3SJKwL"}],"key":"W1qgB552Pb"},"references":{"cite":{"order":[],"data":{}}}}