{"version":3,"kind":"Notebook","sha256":"a8a375f291ac15673f175012c25626be9e7f752ef23d378ee6b76813e492d72b","slug":"labs.lab-3b-pipelines","location":"/labs/Lab 3b - Pipelines.ipynb","dependencies":[],"frontmatter":{"title":"Lab 4:  Data preprocessing","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"github":"https://github.com/ml-course/master","copyright":"2025. CC0 Licensed - Use as you like. Appropriate credit is very welcome","numbering":{"title":{"offset":1}},"source_url":"https://github.com/ml-course/master/blob/master/labs/Lab 3b - Pipelines.ipynb","edit_url":"https://github.com/ml-course/master/edit/master/labs/Lab 3b - Pipelines.ipynb","exports":[{"format":"ipynb","filename":"Lab 3b - Pipelines.ipynb","url":"/Lab 3b - Pipelines-227ccc9108fec480fb1db3ca4feccab8.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"We explore the performance of several linear regression models on a real-world dataset, i.e. ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"nEoOxIn2ac"},{"type":"link","url":"https://www.openml.org/d/41021","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"MoneyBall","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"n7v8P9pQ9g"}],"urlSource":"https://www.openml.org/d/41021","key":"Wa8e1l0z6n"},{"type":"text","value":". See the description on OpenML for more information. In short, this dataset captures performance data from baseball players. The regression task is to accurately predict the number of ‘runs’ each player can score, and understanding which are the most important factors.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"ttjjV31N6M"}],"key":"QduYrz2uxG"}],"key":"iTt9LKQPZp"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nimport seaborn as sns","key":"jt1Rp9QAg1"},{"type":"outputs","id":"QN1LCObhQ97NI7L7hrj2c","children":[],"key":"DtSSbrI1qP"}],"key":"SZ3n3XM0wF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Download MoneyBall data from OpenML\nmoneyball = oml.datasets.get_dataset(41021)\n# Get the pandas dataframe (default)\nX, y, _, attribute_names = moneyball.get_data(target=moneyball.default_target_attribute)","key":"uNV099HbYa"},{"type":"outputs","id":"mAOTh8CemiI6BoKu92IBG","children":[],"key":"EL8qoSzcZ5"}],"key":"eKmwiyqjyH"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"subslide"}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exploratory analysis and visualization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mrTNx1nlcN"}],"identifier":"exploratory-analysis-and-visualization","label":"Exploratory analysis and visualization","html_id":"exploratory-analysis-and-visualization","implicit":true,"key":"pxDpPv8Zck"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"First, we visually explore the data by visualizing the value distribution and the interaction between every other feature in a scatter matrix. We use the target feature as the color variable to see which features are correlated with the target.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"izL1h0BgpN"}],"key":"BmmBOUrNP4"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"For the plotting to work, however, we need to remove the categorical features (the first 2) and fill in the missing values. Let’s find out which columns have missing values. This matches what we already saw on the OpenML page (","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"WJK14hOlrA"},{"type":"link","url":"https://www.openml.org/d/41021","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"https://​www​.openml​.org​/d​/41021","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"BlOoeuqQAr"}],"urlSource":"https://www.openml.org/d/41021","key":"mKk2cs0sNp"},{"type":"text","value":").","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"A5mpvO6A8m"}],"key":"sVvCou8nMJ"}],"key":"amCJg2lN2x"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"pd.isnull(X).any()","key":"TRZcssM3ru"},{"type":"outputs","id":"pGXhVtDSyEonHBJeVrky2","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"Team            False\nLeague          False\nYear            False\nRA              False\nW               False\nOBP             False\nSLG             False\nBA              False\nPlayoffs        False\nRankSeason       True\nRankPlayoffs     True\nG               False\nOOBP             True\nOSLG             True\ndtype: bool","content_type":"text/plain"}}},"children":[],"key":"tq0H2eS6ju"}],"key":"oX6TWfUdeJ"}],"key":"tLH2ODSb89"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"For this first quick visualization, we will simply impute the missing values using the median. Removing all instances with missing values is not really an option since some features have consistent missing values: we would have to remove a lot of data.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HIJpPIzRHy"}],"key":"IkHZAIoort"}],"key":"AwktxHi8nB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Impute missing values with sklearn and rebuild the dataframe\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nX_clean_array = imputer.fit_transform(X[attribute_names[2:]]) # skip the first 2 features\n# The imputer will return a numpy array. To plot it we make it a pandas dataframe again.\nX_clean = pd.DataFrame(X_clean_array, columns = attribute_names[2:]) #","key":"Ndb5kJFHBK"},{"type":"outputs","id":"kPDxnzOm87pbR9S9RM7j_","children":[],"key":"Sa8rp85wcb"}],"key":"lZmXq9RLEH"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Next, we build the scatter matrix. We include the target column to see which features strongly correlate with the target, and also use the target value as the color to see which combinations of features correlate with the target.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KyZXpgbRyT"}],"key":"XVs6tFgEv2"}],"key":"zAUMYaogWx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from pandas.plotting import scatter_matrix\n\n# Scatter matrix of dataframe including the target feature\ncopyframe = X_clean.copy() \ncopyframe['y'] = pd.Series(y, index=copyframe.index)\nscatter_matrix(copyframe, c=y, figsize=(25,25), \n               marker='o', s=20, alpha=.8, cmap='viridis');","key":"serCIGHJJh"},{"type":"outputs","id":"m3-uQg9_PMO6PYZZGy6mA","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"image/png":{"content_type":"image/png","hash":"d4956cc111e048a2f2072e426e4ccb8b","path":"/d4956cc111e048a2f2072e426e4ccb8b.png"},"text/plain":{"content":"<Figure size 1800x1800 with 169 Axes>","content_type":"text/plain"}}},"children":[],"key":"U5FwtI26wk"}],"key":"ulLurHqUGv"}],"key":"ymtuWVPb4s"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"subslide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Several things immediately stand out:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"A5ejpAIzqF"}],"key":"aYVjEvqDzE"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"OBP, SLG and BA strongly correlate with the target (near-diagonals in the final column), but also combinations of either of these and W or R seem useful.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"GYe9fak847"}],"key":"sxg0NwuTGc"}],"key":"n32DhIJdeF"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RA, W, OBP, SLG and BA seem normally distributed, most others do not.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"X8ixJbqsIG"}],"key":"jfs8rsJUaf"}],"key":"RVFybsqesR"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"OOBP and OSLG have a very peaked distribution.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"QCqxQWrXWk"}],"key":"zJrtwUqrjb"}],"key":"eSxTrCoq0u"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"‘Playoffs’ seems to be categorical and should probably be encoded as such.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"t23trIPs4y"}],"key":"jl6wnvQt5m"}],"key":"Uj9fRNSCvB"}],"key":"W7xwMg0mkR"}],"key":"XkzZ7z3IUT"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"subslide"}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 1: Build a pipeline","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RHrI5Cz1cK"}],"identifier":"exercise-1-build-a-pipeline","label":"Exercise 1: Build a pipeline","html_id":"exercise-1-build-a-pipeline","implicit":true,"key":"nJ0A0IYVxB"}],"key":"dk1BUHDASq"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Implement a function ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"op9dmczZl8"},{"type":"inlineCode","value":"build_pipeline","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"w80YAm5d66"},{"type":"text","value":" that does the following:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NzSbEAMJMD"}],"key":"LlWF9gr4gb"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Impute missing values by replacing NaN’s with the feature median for numerical features.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Yi3PKjsXAa"}],"key":"OoUaxSNjod"}],"key":"DXKlzIk2cV"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Encode the categorical features using OneHotEncoding.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"tv2XV3L0q7"}],"key":"hNvIv4iyST"}],"key":"IcfGY7LCFl"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"If the attribute ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"v9RPEhVWZx"},{"type":"inlineCode","value":"scaling=True","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"ynRVkVrrTg"},{"type":"text","value":", also scale the data using standard scaling.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"kH689wQxAb"}],"key":"DPZAO4rpbU"}],"key":"qPXlR1PDxZ"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Attach the given regression model to the end of the pipeline","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"WSIdSHeiEu"}],"key":"AglEn8iSFh"}],"key":"S1p8hDBoJZ"}],"key":"EujSdsGyee"}],"key":"uBWpB1tPTg"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def build_pipeline(regressor, numerical, categorical, scaling=False):\n    \"\"\" Build a robust pipeline with the given regression model\n    Keyword arguments:\n    regressor -- the regression model\n    categorical -- the list of categorical features\n    scaling -- whether or not to scale the data\n    \n    Returns: a pipeline\n    \"\"\"\n    pass","key":"dKvDNrQsGj"},{"type":"outputs","id":"cvQASKCp5JrfDJDKxvydb","children":[],"key":"sEkkhRjsBm"}],"key":"RcdHX7OefN"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 2: Test the pipeline","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kr7B3vNY4G"}],"identifier":"exercise-2-test-the-pipeline","label":"Exercise 2: Test the pipeline","html_id":"exercise-2-test-the-pipeline","implicit":true,"key":"UJVUVgFWOu"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Test the pipeline by evaluating linear regression (without scaling) on the dataset, using 5-fold cross-validation and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"mTBYzkRsud"},{"type":"inlineMath","value":"R^2","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">R^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>","key":"xqhxcPYQn7"},{"type":"text","value":". Make sure to run it on the original dataset (‘X’), not the manually cleaned version (‘X_clean’).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"rBXeADB5yT"}],"key":"FOXRSV6hcn"}],"key":"E0j3DlnW4Z"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 3: A first benchmark","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JLnIqigcmG"}],"identifier":"exercise-3-a-first-benchmark","label":"Exercise 3: A first benchmark","html_id":"exercise-3-a-first-benchmark","implicit":true,"key":"uBqopA9eIR"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Evaluate the following algorithms in their default settings, both with and without scaling, and interpret the results:","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"De4X6tMfm7"}],"key":"jVh5R2pQzA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Linear regression","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"AZSvI3CVCM"}],"key":"GXGSBGghfF"}],"key":"svVvKpMHm5"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Ridge","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"GXtsp9zsno"}],"key":"CQi4Ser2Ig"}],"key":"luhzeBc0Ib"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Lasso","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"CrdnZxC3sE"}],"key":"DTwN8RymuY"}],"key":"okwfrkoOal"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"SVM (RBF)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"RAVJNgqWfF"}],"key":"nKyVHEYchq"}],"key":"JE7n5WCSVz"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"RandomForests","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"fUoVA8p4vJ"}],"key":"uoRSTJXA3B"}],"key":"OpKIHZV3As"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"GradientBoosting","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"C9IGIFEAn7"}],"key":"BUGWh8CbL0"}],"key":"pelDozY0c0"}],"key":"bTeSD7xeqL"}],"key":"HEG1CqLvui"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"subslide"}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 4: Tuning linear models","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"R6rpEYuGWh"}],"identifier":"exercise-4-tuning-linear-models","label":"Exercise 4: Tuning linear models","html_id":"exercise-4-tuning-linear-models","implicit":true,"key":"Yk6mNLCzqY"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Next, visualize the effect of the alpha regularizer for Ridge and Lasso. Vary alpha from 1e-4 to 1e6 and plot the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"aH0BrknSS7"},{"type":"inlineMath","value":"R^2","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">R^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>","key":"AmHSZvuk4i"},{"type":"text","value":" score as a line plot (one line for each algorithm). Always use scaling. Interpret the results.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jySyI2w92T"}],"key":"nFzvpfVktW"}],"key":"jd1zesFnh7"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 5: Tuning SVMs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BJNnwAr7TN"}],"identifier":"exercise-5-tuning-svms","label":"Exercise 5: Tuning SVMs","html_id":"exercise-5-tuning-svms","implicit":true,"key":"nTteZtI7Cm"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Next, tune the SVM’s C and gamma. You can stay within the 1e-6 to 1e6 range. Plot the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"cRbkt0ObFl"},{"type":"inlineMath","value":"R^2","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">R^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span>","key":"HWGZUJbKYA"},{"type":"text","value":" score as a heatmap.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"C4ezzbSCCL"}],"key":"TlFvzuTGNi"}],"key":"j4hNo3XYJW"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n    if ax is None:\n        ax = plt.gca()\n    # plot the mean cross-validation scores\n    img = ax.pcolor(values, cmap=cmap, vmin=None, vmax=None)\n    img.update_scalarmappable()\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n    ax.set_xticklabels(xticklabels)\n    ax.set_yticklabels(yticklabels)\n    ax.set_aspect(1)\n\n    for p, color, value in zip(img.get_paths(), img.get_facecolors(), img.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.mean(color[:3]) > 0.5:\n            c = 'k'\n        else:\n            c = 'w'\n        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n    return img","key":"cxorxDRwWG"},{"type":"outputs","id":"zzKM93n188yu_-q650hc5","children":[],"key":"jK7ADAvlOm"}],"key":"lzqRqJIH7r"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 5b: Tuning SVMs (2)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"x4sRSDiO80"}],"identifier":"exercise-5b-tuning-svms-2","label":"Exercise 5b: Tuning SVMs (2)","html_id":"exercise-5b-tuning-svms-2","implicit":true,"key":"x0jPQgGQW7"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Redraw the heatmap, but now use scaling. What do you observe?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"WUwvnhG7At"}],"key":"iwlU1SROPK"}],"key":"PauujHyACe"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"subslide"}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 6: Feature importance","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Ii0cPU8R9S"}],"identifier":"exercise-6-feature-importance","label":"Exercise 6: Feature importance","html_id":"exercise-6-feature-importance","implicit":true,"key":"O9R401kbJ2"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Retrieve the coefficients from the optimized Lasso, Ridge, and the feature importances from the default RandomForest and GradientBoosting models.\nCompare the results. Do the different models agree on which features are important? You will need to map the encoded feature names to the correct coefficients and feature importances. If you can, plot the importances as a bar chart.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"xtYUijpTct"}],"key":"CQHLgGKCho"}],"key":"NnQnvx64vH"}],"key":"oUr8vI1AfZ"},"references":{"cite":{"order":[],"data":{}}}}