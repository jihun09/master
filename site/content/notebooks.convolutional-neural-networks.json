{"version":3,"kind":"Notebook","sha256":"4d3f6d4eeb8e3d039ed17828582c8623d6dec482bc0456328521039f4aed65ba","slug":"notebooks.convolutional-neural-networks","location":"/notebooks/07 - Convolutional Neural Networks.ipynb","dependencies":[],"frontmatter":{"title":"Lecture 7: Convolutional Neural Networks","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"github":"https://github.com/ml-course/master","copyright":"2025. CC0 Licensed - Use as you like. Appropriate credit is very welcome","numbering":{"title":{"offset":1}},"source_url":"https://github.com/ml-course/master/blob/master/notebooks/07 - Convolutional Neural Networks.ipynb","edit_url":"https://github.com/ml-course/master/edit/master/notebooks/07 - Convolutional Neural Networks.ipynb","thumbnail":"/9897aafac834d0843165710057517d01.png","thumbnailOptimized":"/9897aafac834d0843165710057517d01.webp","exports":[{"format":"ipynb","filename":"07 - Convolutional Neural Networks.ipynb","url":"/07 - Convolutional N-1748807789103e6fb3cec1f785ba6e53.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","data":{"id":"VU3bkm1YYidb","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"strong","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Handling image data","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"dP2YMj2UKU"}],"key":"QFp47lObHO"}],"key":"WgutNsk9KF"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Joaquin Vanschoren, Eindhoven University of Technology","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"bnDxapgyej"}],"key":"acesBkamLZ"}],"identifier":"vu3bkm1yyidb","label":"VU3bkm1YYidb","html_id":"vu3bkm1yyidb","visibility":"show","key":"Zuu5vtM0qg"},{"type":"block","kind":"notebook-content","data":{"id":"MyAIooipYidn","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Overview","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OJWVMaTaQC"}],"identifier":"overview","label":"Overview","html_id":"overview","implicit":true,"key":"mAKmgRFTju"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Image convolution","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Ww5xpzv3j0"}],"key":"UX2StDp56E"}],"key":"aOxEiqGKbs"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Convolutional neural networks","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"qVddCDl7Pu"}],"key":"qwGpt1sXLA"}],"key":"QAciElQUv4"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Data augmentation","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AfKiSkXpyb"}],"key":"SYjQ4KH5Eo"}],"key":"fHOf7PUlgi"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Real-world CNNs","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"YDZtopKvcZ"}],"key":"x6T1PNPmKl"}],"key":"DtZWPreJJY"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Model interpretation","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"pkK6qAMedW"}],"key":"q54KHyG7yP"}],"key":"UC4jKUJwgu"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Using pre-trained networks (transfer learning)","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"bfmPunoF5F"}],"key":"Q3pRkBvg8C"}],"key":"SfMYRiZijb"}],"key":"iBiR9TJ861"}],"identifier":"myaiooipyidn","label":"MyAIooipYidn","html_id":"myaiooipyidn","visibility":"show","key":"huyTj10BZl"},{"type":"block","kind":"notebook-code","data":{"hide_input":true,"id":"czkUG-r4Yidn","slideshow":{"slide_type":"skip"},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# Auto-setup when running on Google Colab\nimport os\nif 'google.colab' in str(get_ipython()) and not os.path.exists('/content/master'):\n    !git clone -q https://github.com/ML-course/master.git /content/master\n    !pip --quiet install -r /content/master/requirements_colab.txt\n    %cd master/notebooks\n\n# Global imports and settings\n%matplotlib inline\nfrom preamble import *\ninteractive = True # Set to True for interactive plots \nif interactive:\n    fig_scale = 0.5\n    plt.rcParams.update(print_config)\nelse: # For printing\n    fig_scale = 0.4\n    plt.rcParams.update(print_config)\n    \nHTML('''<style>.rise-enabled .reveal pre {font-size=75%} </style>''')","identifier":"czkug-r4yidn-code","visibility":"hide","enumerator":"1","html_id":"czkug-r4yidn-code","key":"MxnDDEwrxi"},{"type":"outputs","id":"uvBZb18cn96wwUWhkIkOs","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":1,"metadata":{},"data":{"text/html":{"content":"<style>.rise-enabled .reveal pre {font-size=75%} </style>","content_type":"text/html"},"text/plain":{"content":"<IPython.core.display.HTML object>","content_type":"text/plain"}}},"children":[],"identifier":"czkug-r4yidn-outputs-0","html_id":"czkug-r4yidn-outputs-0","key":"Lb6B20W15S"}],"identifier":"czkug-r4yidn-outputs","visibility":"show","html_id":"czkug-r4yidn-outputs","key":"v5mrHthpB1"}],"identifier":"czkug-r4yidn","label":"czkUG-r4Yidn","html_id":"czkug-r4yidn","visibility":"show","key":"MP1B6uUbfn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}","key":"oVTZWR62KM"},{"type":"outputs","id":"Mg0Z_0cRFeoU6x3m_j9Vf","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/javascript":{"content":"IPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n","content_type":"application/javascript"},"text/plain":{"content":"<IPython.core.display.Javascript object>","content_type":"text/plain"}}},"children":[],"key":"kRIbnLbtzU"}],"key":"Od5UBowyhb"}],"key":"pAceBx2ymh"},{"type":"block","kind":"notebook-code","data":{"id":"kvofryp9Yido","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import pickle \ndata_dir = '../data/cats-vs-dogs_small'\nmodel_dir = '../data/models'\nif not os.path.exists(data_dir):\n    os.makedirs(data_dir)\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n    \nwith open(\"../data/histories.pkl\", \"rb\") as f:\n    histories = pickle.load(f)","identifier":"kvofryp9yido-code","visibility":"hide","enumerator":"2","html_id":"kvofryp9yido-code","key":"MvEAfytrTH"},{"type":"outputs","id":"8e0umkQgIHjvcQ5ipASfd","children":[],"identifier":"kvofryp9yido-outputs","visibility":"show","html_id":"kvofryp9yido-outputs","key":"lMA9zzaAbv"}],"identifier":"kvofryp9yido","label":"kvofryp9Yido","html_id":"kvofryp9yido","visibility":"show","key":"LdWo6TxAfo"},{"type":"block","kind":"notebook-content","data":{"id":"y5gSmtvqYidp","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Convolutions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OiM08Ssvss"}],"identifier":"convolutions","label":"Convolutions","html_id":"convolutions","implicit":true,"key":"y8s2sik7RA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Operation that transforms an image by sliding a smaller image (called a ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bd58dAPDbn"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"filter","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jUiA69YoZK"}],"key":"LlwECtM7LX"},{"type":"text","value":" or ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qly6SO2SrJ"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"kernel","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"HQKOStyda1"}],"key":"Ts0ds88Rlm"},{"type":"text","value":" ) over the image and multiplying the pixel values","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"RvyIgu6OQ7"}],"key":"e9nIbXkXFw"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Slide an ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pHkhPaESqf"},{"type":"inlineMath","value":"n","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"LIbfhUJEtF"},{"type":"text","value":" x ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GUY50NTQyj"},{"type":"inlineMath","value":"n","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"ROod3oCjby"},{"type":"text","value":" filter over ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GFgQSMULom"},{"type":"inlineMath","value":"n","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"WMcgEeuqWq"},{"type":"text","value":" x ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"qt9KdyR0Xn"},{"type":"inlineMath","value":"n","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"tE0rMmqrHz"},{"type":"text","value":" ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Tl5nFvbLGz"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"patches","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"O2q1OeZ5s7"}],"key":"e6qMZqN085"},{"type":"text","value":" of the original image","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Ioa0zhgStB"}],"key":"Gt9HnWYE8W"}],"key":"G9YgqkQtt5"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Every pixel is replaced by the ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"GCfE44ok3V"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"sum","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"NrX3HOexit"}],"key":"Hj0b6yJkHf"},{"type":"text","value":" of the ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"USB4GD3fJ3"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"element-wise products","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"gyg4mLBCIR"}],"key":"jk5P84cupj"},{"type":"text","value":" of the values of the image patch around that pixel and the kernel","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"rqaDJ0J6Xp"}],"key":"p3W1Gtbhw3"}],"key":"HQ4BXeJ4wc"}],"key":"jdc9qAOgV9"}],"key":"tBVS9pASBJ"}],"key":"tPH9pSxRYg"},{"type":"code","lang":"python","value":"# kernel and image_patch are n x n matrices\npixel_out = np.sum(kernel * image_patch)","position":{"start":{"line":6,"column":1},"end":{"line":9,"column":1}},"identifier":"y5gsmtvqyidp-code","enumerator":"3","html_id":"y5gsmtvqyidp-code","key":"KB9MTxo1PK"},{"type":"image","style":{"width":"500px","marginLeft":"auto","marginRight":"auto"},"url":"/9897aafac834d0843165710057517d01.png","alt":"ml","key":"ZPygs9TZyq","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_conv_filtering.png?raw=1","urlOptimized":"/9897aafac834d0843165710057517d01.webp"}],"identifier":"y5gsmtvqyidp","label":"y5gSmtvqYidp","html_id":"y5gsmtvqyidp","visibility":"show","key":"q0VR3SwU0q"},{"type":"block","kind":"notebook-code","data":{"id":"jNw-a9gjYidp","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"from __future__ import print_function\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual, Dropdown\nfrom skimage import color\n\n\n# Visualize convolution. See https://tonysyu.github.io/\ndef iter_pixels(image):\n    \"\"\" Yield pixel position (row, column) and pixel intensity. \"\"\"\n    height, width = image.shape[:2]\n    for i in range(height):\n        for j in range(width):\n            yield (i, j), image[i, j]\n            \n# Visualize result\ndef imshow_pair(image_pair, titles=('', ''), figsize=(8, 4), **kwargs):\n    fig, axes = plt.subplots(ncols=2, figsize=figsize)\n    for ax, img, label in zip(axes.ravel(), image_pair, titles):\n        ax.imshow(img, **kwargs)\n        ax.set_title(label, fontdict={'fontsize':32*fig_scale})\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n# Visualize result\ndef imshow_triple(axes, image_pair, titles=('', '', ''), figsize=(8, 4), **kwargs):\n    for ax, img, label in zip(axes, image_pair, titles):\n        ax.imshow(img, **kwargs)\n        ax.set_title(label, fontdict={'fontsize':10*fig_scale})\n        ax.set_xticks([])\n        ax.set_yticks([])\n        \n# Zero-padding\ndef padding_for_kernel(kernel):\n    \"\"\" Return the amount of padding needed for each side of an image.\n\n    For example, if the returned result is [1, 2], then this means an\n    image should be padded with 1 extra row on top and bottom, and 2\n    extra columns on the left and right.\n    \"\"\"\n    # Slice to ignore RGB channels if they exist.\n    image_shape = kernel.shape[:2]\n    # We only handle kernels with odd dimensions so make sure that's true.\n    # (The \"center\" pixel of an even number of pixels is arbitrary.)\n    assert all((size % 2) == 1 for size in image_shape)\n    return [(size - 1) // 2 for size in image_shape]\ndef add_padding(image, kernel):\n    h_pad, w_pad = padding_for_kernel(kernel)\n    return np.pad(image, ((h_pad, h_pad), (w_pad, w_pad)),\n                  mode='constant', constant_values=0)\ndef remove_padding(image, kernel):\n    inner_region = []  # A 2D slice for grabbing the inner image region\n    for pad in padding_for_kernel(kernel):\n        slice_i = np.s_[:] if pad == 0 else np.s_[pad: -pad]\n        inner_region.append(slice_i)\n    return image # [inner_region] # Broken in numpy 1.24, doesn't seem necessary\n\n# Slice windows\ndef window_slice(center, kernel):\n    r, c = center\n    r_pad, c_pad = padding_for_kernel(kernel)\n    # Slicing is (inclusive, exclusive) so add 1 to the stop value\n    return np.s_[r-r_pad:r+r_pad+1, c-c_pad:c+c_pad+1]\n        \n\n# Apply convolution kernel to image patch\ndef apply_kernel(center, kernel, original_image):\n    image_patch = original_image[window_slice(center, kernel)]\n    # An element-wise multiplication followed by the sum\n    return np.sum(kernel * image_patch)\n\n# Move kernel over the image\ndef iter_kernel_labels(image, kernel):\n    original_image = image\n    image = add_padding(original_image, kernel)\n    i_pad, j_pad = padding_for_kernel(kernel)\n\n    for (i, j), pixel in iter_pixels(original_image):\n        # Shift the center of the kernel to ignore padded border.\n        i += i_pad\n        j += j_pad\n        mask = np.zeros(image.shape, dtype=int)  # Background = 0\n        mask[window_slice((i, j), kernel)] = kernel   # Kernel = 1\n        #mask[i, j] = 2                           # Kernel-center = 2\n        yield (i, j), mask\n\n# Visualize kernel as it moves over the image\ndef visualize_kernel(kernel_labels, image):\n    return kernel_labels + image #color.label2rgb(kernel_labels, image, bg_label=0)\n\ndef convolution_demo(image, kernels, **kwargs):\n    # Dropdown for selecting kernels\n    kernel_names = list(kernels.keys())\n    kernel_selector = Dropdown(options=kernel_names, description='Kernel:')\n    \n    def update_convolution(kernel_name):\n        kernel = kernels[kernel_name]  # Get the selected kernel\n        gen_kernel_labels = iter_kernel_labels(image, kernel)\n        \n        image_cache = []\n        image_padded = add_padding(image, kernel)\n        \n        def convolution_step(i_step=0):\n            while i_step >= len(image_cache):\n                filtered_prev = image_padded if i_step == 0 else image_cache[-1][1]\n                filtered = filtered_prev.copy()\n                \n                center, kernel_labels = next(gen_kernel_labels)\n                filtered[center] = apply_kernel(center, kernel, image_padded)\n                kernel_overlay = visualize_kernel(kernel_labels, image_padded)\n                \n                image_cache.append((kernel_overlay, filtered))\n                \n            image_pair = [remove_padding(each, kernel) for each in image_cache[i_step]]\n            imshow_pair(image_pair, **kwargs)\n            plt.show()\n        \n        interact(convolution_step, i_step=(0, image.size - 1, 1))\n    \n    interact(update_convolution, kernel_name=kernel_selector);\n\n# Full process\ndef convolution_full(ax, image, kernel, **kwargs):\n    # Initialize generator since we're only ever going to iterate over\n    # a pixel once. The cached result is used, if we step back.\n    gen_kernel_labels = iter_kernel_labels(image, kernel)\n\n    image_cache = []\n    image_padded = add_padding(image, kernel)\n    # Plot original image and kernel-overlay next to filtered image.\n\n    for i_step in range(image.size-1):\n\n        # For the first step (`i_step == 0`), the original image is the\n        # filtered image; after that we look in the cache, which stores\n        # (`kernel_overlay`, `filtered`).\n        filtered_prev = image_padded if i_step == 0 else image_cache[-1][1]\n        # We don't want to overwrite the previously filtered image:\n        filtered = filtered_prev.copy()\n\n        # Get the labels used to visualize the kernel\n        center, kernel_labels = next(gen_kernel_labels)\n        # Modify the pixel value at the kernel center\n        filtered[center] = apply_kernel(center, kernel, image_padded)\n        # Take the original image and overlay our kernel visualization\n        kernel_overlay = visualize_kernel(kernel_labels, image_padded)\n        # Save images for reuse.\n        image_cache.append((kernel_overlay, filtered))\n\n    # Remove padding we added to deal with boundary conditions\n    # (Loop since each step has 2 images)\n    image_triple = [remove_padding(each, kernel)\n                  for each in image_cache[i_step]]\n    image_triple.insert(1,kernel)\n    imshow_triple(ax, image_triple, **kwargs)","identifier":"jnw-a9gjyidp-code","visibility":"hide","enumerator":"4","html_id":"jnw-a9gjyidp-code","key":"v5T8rhuMd1"},{"type":"outputs","id":"Cu7K9CYXuZqTX6kKt2-0J","children":[],"identifier":"jnw-a9gjyidp-outputs","visibility":"show","html_id":"jnw-a9gjyidp-outputs","key":"zFxXXtZ4S1"}],"identifier":"jnw-a9gjyidp","label":"jNw-a9gjYidp","html_id":"jnw-a9gjyidp","visibility":"show","key":"b7eZRI2gQc"},{"type":"block","kind":"notebook-content","data":{"id":"qFN4VgubYids","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Different kernels can detect different types of patterns in the image","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"g80o4lptbM"}],"key":"Xjmp4MMzKz"}],"key":"U73IHEkuvp"}],"key":"UfNsue6KeS"}],"identifier":"qfn4vgubyids","label":"qFN4VgubYids","html_id":"qfn4vgubyids","visibility":"show","key":"wSHl1dj68U"},{"type":"block","kind":"notebook-code","data":{"hide_input":true,"id":"UVcQRmuXYids","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"horizontal_edge_kernel = np.array([[ 1,  2,  1],\n                                   [ 0,  0,  0],\n                                   [-1, -2, -1]])\ndiagonal_edge_kernel = np.array([[1, 0, 0],\n                                 [0, 1, 0],\n                                 [0, 0, 1]])\nedge_detect_kernel = np.array([[-1, -1, -1],\n                               [-1,  8, -1],\n                               [-1, -1, -1]])\nall_kernels = {\"horizontal\": horizontal_edge_kernel,\n               \"diagonal\": diagonal_edge_kernel,\n               \"edge_detect\":edge_detect_kernel}","identifier":"uvcqrmuxyids-code","visibility":"hide","enumerator":"5","html_id":"uvcqrmuxyids-code","key":"D57hyvcoqk"},{"type":"outputs","id":"MQ-AgAc9J4yteEV36ErG2","children":[],"identifier":"uvcqrmuxyids-outputs","visibility":"show","html_id":"uvcqrmuxyids-outputs","key":"SIJzRksxA0"}],"identifier":"uvcqrmuxyids","label":"UVcQRmuXYids","html_id":"uvcqrmuxyids","visibility":"show","key":"laERaggbzV"},{"type":"block","kind":"notebook-code","data":{"id":"25ElHFEQYidt","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"mnist_data = oml.datasets.get_dataset(554) # Download MNIST data\n# Get the predictors X and the labels y\nX_mnist, y_mnist, c, a = mnist_data.get_data(dataset_format='array', target=mnist_data.default_target_attribute); \nimage = X_mnist[1].reshape((28, 28))\nimage = (image - np.min(image))/np.ptp(image) # Normalize\n\nif interactive:\n    titles = ('Image and kernel', 'Filtered image')\n    convolution_demo(image, all_kernels, vmin=-4, vmax=4, titles=titles, cmap='gray_r');","identifier":"25elhfeqyidt-code","visibility":"hide","enumerator":"6","html_id":"id-25elhfeqyidt-code","key":"hwHkCW6krN"},{"type":"outputs","id":"nS5DwXHDW_cmeNIhOKC8t","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"9e24497b939648cc9bea8519f8cf0044\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(Dropdown(description='Kernel:', options=('horizontal', 'diagonal', 'edge_detect'), valueâ€¦","content_type":"text/plain"}}},"children":[],"identifier":"25elhfeqyidt-outputs-0","html_id":"id-25elhfeqyidt-outputs-0","key":"pHMS9XM2tN"}],"identifier":"25elhfeqyidt-outputs","visibility":"show","html_id":"id-25elhfeqyidt-outputs","key":"ORVBSdY8KV"}],"identifier":"25elhfeqyidt","label":"25ElHFEQYidt","html_id":"id-25elhfeqyidt","visibility":"show","key":"lJafsSN6A5"},{"type":"block","kind":"notebook-code","data":{"id":"2Kp1dGzJYidt","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    fig, axs = plt.subplots(3,  3, figsize=(5*fig_scale, 5*fig_scale))\n    titles = ('Image and kernel', 'Hor. edge filter', 'Filtered image')\n    convolution_full(axs[0,:], image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Edge detect filter', 'Filtered image')\n    convolution_full(axs[1,:], image, edge_detect_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Diag. edge filter', 'Filtered image')\n    convolution_full(axs[2,:], image, diagonal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    plt.tight_layout()","identifier":"2kp1dgzjyidt-code","visibility":"hide","enumerator":"7","html_id":"id-2kp1dgzjyidt-code","key":"op5exzoI10"},{"type":"outputs","id":"3oUkzqWiaPCsecoVmNsTg","children":[],"identifier":"2kp1dgzjyidt-outputs","visibility":"show","html_id":"id-2kp1dgzjyidt-outputs","key":"c0lsutxk3p"}],"identifier":"2kp1dgzjyidt","label":"2Kp1dGzJYidt","html_id":"id-2kp1dgzjyidt","visibility":"show","key":"wfjrkdQ944"},{"type":"block","kind":"notebook-content","data":{"id":"greXxJk-Yidt","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Demonstration on Fashion-MNIST","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LZuPzn3qrG"}],"identifier":"demonstration-on-fashion-mnist","label":"Demonstration on Fashion-MNIST","html_id":"demonstration-on-fashion-mnist","implicit":true,"key":"qIuhI1ooab"}],"identifier":"grexxjk-yidt","label":"greXxJk-Yidt","html_id":"grexxjk-yidt","visibility":"show","key":"Z6e7agi5tA"},{"type":"block","kind":"notebook-code","data":{"id":"n_O2UWkyYidt","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"fmnist_data = oml.datasets.get_dataset(40996) # Download FMNIST data\n# Get the predictors X and the labels y\nX_fm, y_fm, _, _ = fmnist_data.get_data(dataset_format='array', target=fmnist_data.default_target_attribute)\nfm_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n              6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}","identifier":"n_o2uwkyyidt-code","visibility":"hide","enumerator":"8","html_id":"n-o2uwkyyidt-code","key":"WO3QSjWzmX"},{"type":"outputs","id":"OBXLoZQJnxMl-sQ79F8O5","children":[],"identifier":"n_o2uwkyyidt-outputs","visibility":"show","html_id":"n-o2uwkyyidt-outputs","key":"VtBukFH6YR"}],"identifier":"n_o2uwkyyidt","label":"n_O2UWkyYidt","html_id":"n-o2uwkyyidt","visibility":"show","key":"NDudbKlNeZ"},{"type":"block","kind":"notebook-code","data":{"id":"6LCiUiY2Yidu","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# build a list of figures for plotting\ndef buildFigureList(fig, subfiglist, titles, length):\n    for i in range(0,length):\n        pixels = np.array(subfiglist[i], dtype='float')\n        pixels = pixels.reshape((28, 28))\n        a=fig.add_subplot(1,length,i+1)\n        imgplot =plt.imshow(pixels, cmap='gray_r')\n        a.set_title(fm_classes[titles[i]], fontsize=6)\n        a.axes.get_xaxis().set_visible(False)\n        a.axes.get_yaxis().set_visible(False)\n    return\n\nsubfiglist = []\ntitles=[]\n\nfor i in range(0,7):\n    subfiglist.append(X_fm[i])\n    titles.append(y_fm[i])\n\nbuildFigureList(plt.figure(1),subfiglist, titles, 7)\nplt.show()","identifier":"6lciuiy2yidu-code","visibility":"hide","enumerator":"9","html_id":"id-6lciuiy2yidu-code","key":"X2aoR2qki3"},{"type":"outputs","id":"mBbgIEWmoTb0kcpyIzLY9","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content":"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","content_type":"application/pdf"},"image/png":{"content_type":"image/png","hash":"8dca3dbe5ca01d7f6eb5067d30ac1751","path":"/8dca3dbe5ca01d7f6eb5067d30ac1751.png"},"text/plain":{"content":"<Figure size 1500x900 with 7 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"6lciuiy2yidu-outputs-0","html_id":"id-6lciuiy2yidu-outputs-0","key":"qohQTd3uwc"}],"identifier":"6lciuiy2yidu-outputs","visibility":"show","html_id":"id-6lciuiy2yidu-outputs","key":"Xcd31Zr3RT"}],"identifier":"6lciuiy2yidu","label":"6LCiUiY2Yidu","html_id":"id-6lciuiy2yidu","visibility":"show","key":"IAUP2RF8wM"},{"type":"block","kind":"notebook-content","data":{"id":"CwPhhtmYYidu","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Demonstration of convolution with edge filters","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"S3JcwcVvvN"}],"key":"liVjdgwC09"}],"identifier":"cwphhtmyyidu","label":"CwPhhtmYYidu","html_id":"cwphhtmyyidu","visibility":"show","key":"ADsI46vcud"},{"type":"block","kind":"notebook-code","data":{"hide_input":true,"id":"2mx8X_PtYidu","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def normalize_image(X):\n    image = X.reshape((28, 28))\n    return (image - np.min(image))/np.ptp(image) # Normalize\n\nif interactive:\n    image = normalize_image(X_fm[3])\n    demo2 = convolution_demo(image, all_kernels,\n                             vmin=-4, vmax=4, cmap='gray_r');","identifier":"2mx8x_ptyidu-code","visibility":"hide","enumerator":"10","html_id":"id-2mx8x-ptyidu-code","key":"Osssq9zoWR"},{"type":"outputs","id":"6caAyTvVpS9CbjzDjD4hd","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"cb4b7e50debc4ddcb1f8a47d841850c0\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(Dropdown(description='Kernel:', options=('horizontal', 'diagonal', 'edge_detect'), valueâ€¦","content_type":"text/plain"}}},"children":[],"identifier":"2mx8x_ptyidu-outputs-0","html_id":"id-2mx8x-ptyidu-outputs-0","key":"Cb5D9SeaBZ"}],"identifier":"2mx8x_ptyidu-outputs","visibility":"show","html_id":"id-2mx8x-ptyidu-outputs","key":"r2pWrcvh4e"}],"identifier":"2mx8x_ptyidu","label":"2mx8X_PtYidu","html_id":"id-2mx8x-ptyidu","visibility":"show","key":"GnNkoCcmaX"},{"type":"block","kind":"notebook-code","data":{"id":"2hV7eNvGYidu","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    fig, axs = plt.subplots(3, 3, figsize=(5*fig_scale, 5*fig_scale))\n    titles = ('Image and kernel', 'Hor. edge filter', 'Filtered image')\n    convolution_full(axs[0,:], image, horizontal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Diag. edge filter', 'Filtered image')\n    convolution_full(axs[1,:], image, diagonal_edge_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    titles = ('Image and kernel', 'Edge detect filter', 'Filtered image')\n    convolution_full(axs[2,:], image, edge_detect_kernel, vmin=-4, vmax=4, titles=titles, cmap='gray_r')\n    plt.tight_layout()","identifier":"2hv7envgyidu-code","visibility":"hide","enumerator":"11","html_id":"id-2hv7envgyidu-code","key":"JQBeRiJARJ"},{"type":"outputs","id":"K52e9Dkgj676YVC0zgZsA","children":[],"identifier":"2hv7envgyidu-outputs","visibility":"show","html_id":"id-2hv7envgyidu-outputs","key":"KQzJXWRubc"}],"identifier":"2hv7envgyidu","label":"2hV7eNvGYidu","html_id":"id-2hv7envgyidu","visibility":"show","key":"vVqfbm3nUL"},{"type":"block","kind":"notebook-content","data":{"id":"TYOcaGH5Yidv","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Image convolution in practice","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Q6pzSv0dR7"}],"identifier":"image-convolution-in-practice","label":"Image convolution in practice","html_id":"image-convolution-in-practice","implicit":true,"key":"dVu5DLtXpA"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"How do we know which filters are best for a given image?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"hRL7Ac2TQZ"}],"key":"KcUHGv3wJ4"}],"key":"aphgTK9RHE"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Families","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xsr1goY93b"}],"key":"RmEqi3UTe7"},{"type":"text","value":" of kernels (or ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YnLygUTTQG"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"filter banks","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"dlZkk1hpxj"}],"key":"i7XtsCq7Oq"},{"type":"text","value":" ) can be run on every image","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"kF8KEu1sn9"}],"key":"mutllTIgMa"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Gabor, Sobel, Haar Wavelets,...","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Fa5IuPaySn"}],"key":"eEwxjQCcEB"}],"key":"gR7ezX82BF"}],"key":"mNn2CHHcWz"}],"key":"HexvyLy37p"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Gabor filters: Wave patterns generated by changing:","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"crRgbKdGI4"}],"key":"xsYQJjOWrV"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Frequency: narrow or wide ondulations","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"cdczqFybyF"}],"key":"c8sju49T8c"}],"key":"rG9FWBW8I0"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Theta: angle (direction) of the wave","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"CywnozcrMF"}],"key":"qskUSBXVF3"}],"key":"azOFBRj4DK"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sigma: resolution (size of the filter)","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"cJyShnrSve"}],"key":"NNSsUqaF8h"}],"key":"BAdN5alUNX"}],"key":"pRwlZJ2JiR"}],"key":"A2tXS7iTTv"}],"key":"lXJxUJRCAL"}],"identifier":"tyocagh5yidv","label":"TYOcaGH5Yidv","html_id":"tyocagh5yidv","visibility":"show","key":"uvS5rp9YEt"},{"type":"block","kind":"notebook-content","data":{"id":"ZczPiBggYidv","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Demonstration of Gabor filters","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"L5n7f4E7iB"}],"key":"wwhDijGimo"}],"identifier":"zczpibggyidv","label":"ZczPiBggYidv","html_id":"zczpibggyidv","visibility":"show","key":"uU4MVGC2Vr"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from scipy import ndimage as ndi\nfrom skimage import data\nfrom skimage.util import img_as_float\nfrom skimage.filters import gabor_kernel\n\n# Gabor Filters\n@interact\ndef demoGabor(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n    plt.gray()\n    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest', extent=[-1, 1, -1, 1])\n    plt.title(f'freq: {round(frequency,2)}, theta: {round(theta,2)}, sigma: {round(sigma,2)}', fontdict={'fontsize':14*fig_scale})\n    plt.xticks([])\n    plt.yticks([])\n    plt.show();","key":"hdrI2WaYPE"},{"type":"outputs","id":"skzeabR9Bzedm4Q1jsWaK","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"e3a9e309dcb24440bffff8f8605d2e6d\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(FloatSlider(value=0.46, description='frequency', max=1.0, min=0.01, step=0.05), FloatSliâ€¦","content_type":"text/plain"}}},"children":[],"key":"qZsLJPYhSl"}],"key":"KxM8RFaxzP"}],"key":"w5mWvt4Kem"},{"type":"block","kind":"notebook-code","data":{"id":"aLRU2LidYidv","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    plt.subplot(1, 3, 1)\n    demoGabor(frequency=0.16, theta=1.2, sigma=4.0)\n    plt.subplot(1, 3, 2)\n    demoGabor(frequency=0.31, theta=0, sigma=3.6)\n    plt.subplot(1, 3, 3)\n    demoGabor(frequency=0.36, theta=1.6, sigma=1.3)\n    plt.tight_layout()","identifier":"alru2lidyidv-code","visibility":"hide","enumerator":"12","html_id":"alru2lidyidv-code","key":"LFMzoniOSr"},{"type":"outputs","id":"DigxAObmgnbSIBv_h0k9G","children":[],"identifier":"alru2lidyidv-outputs","visibility":"show","html_id":"alru2lidyidv-outputs","key":"E4GZ4edMK1"}],"identifier":"alru2lidyidv","label":"aLRU2LidYidv","html_id":"alru2lidyidv","visibility":"show","key":"Tm7ARjVkLx"},{"type":"block","kind":"notebook-content","data":{"id":"EI_3btNjYidv","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Demonstration on the Fashion-MNIST data","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bZ8vj8Q35q"}],"key":"keaJSyivy1"}],"identifier":"ei_3btnjyidv","label":"EI_3btNjYidv","html_id":"ei-3btnjyidv","visibility":"show","key":"aKPKBwPKoi"},{"type":"block","kind":"notebook-code","data":{"id":"0CaWxAhjYidv","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# Calculate the magnitude of the Gabor filter response given a kernel and an imput image\ndef magnitude(image, kernel):\n    image = (image - image.mean()) / image.std() # Normalize images\n    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +\n                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)","identifier":"0cawxahjyidv-code","visibility":"hide","enumerator":"13","html_id":"id-0cawxahjyidv-code","key":"NTWJNILHqT"},{"type":"outputs","id":"tRmyK-0j4_rY-qkh3j8U2","children":[],"identifier":"0cawxahjyidv-outputs","visibility":"show","html_id":"id-0cawxahjyidv-outputs","key":"cdrCCoHdUd"}],"identifier":"0cawxahjyidv","label":"0CaWxAhjYidv","html_id":"id-0cawxahjyidv","visibility":"show","key":"sMDUIrKLFH"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@interact\ndef demoGabor2(frequency=(0.01,1,0.05), theta=(0,3.14,0.1), sigma=(0,5,0.1)):\n    plt.subplot(131)\n    plt.title('Original', fontdict={'fontsize':24*fig_scale})\n    plt.imshow(image)\n    plt.xticks([])\n    plt.yticks([])\n    plt.subplot(132)\n    plt.title('Gabor kernel', fontdict={'fontsize':24*fig_scale})\n    plt.imshow(np.real(gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma)), interpolation='nearest')\n    plt.xticks([])\n    plt.yticks([])\n    plt.subplot(133)\n    plt.title('Response magnitude', fontdict={'fontsize':24*fig_scale})\n    plt.imshow(np.real(magnitude(image, gabor_kernel(frequency=frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))), interpolation='nearest')\n    plt.tight_layout()\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()","key":"j2TAdgCXXj"},{"type":"outputs","id":"3L59ZwV-OONlbttdjSOpH","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"a7ebfa13680647bf8019ac4d0922e431\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(FloatSlider(value=0.46, description='frequency', max=1.0, min=0.01, step=0.05), FloatSliâ€¦","content_type":"text/plain"}}},"children":[],"key":"lQL8eL1sZQ"}],"key":"WiuwVzGHwD"}],"key":"fJR9lWTEX2"},{"type":"block","kind":"notebook-code","data":{"id":"-Z9XOB4SYidv","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    demoGabor2(frequency=0.16, theta=1.4, sigma=1.2)","identifier":"-z9xob4syidv-code","visibility":"hide","enumerator":"14","html_id":"id-z9xob4syidv-code","key":"NiEeSHZZ9z"},{"type":"outputs","id":"Rmoc_DrR98gvoEwioq-Dp","children":[],"identifier":"-z9xob4syidv-outputs","visibility":"show","html_id":"id-z9xob4syidv-outputs","key":"V25J6dVKv8"}],"identifier":"-z9xob4syidv","label":"-Z9XOB4SYidv","html_id":"id-z9xob4syidv","visibility":"show","key":"MmgsdMcha7"},{"type":"block","kind":"notebook-content","data":{"id":"kaO1xfOGYidv","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Filter banks","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PdWWo322qQ"}],"identifier":"filter-banks","label":"Filter banks","html_id":"filter-banks","implicit":true,"key":"pfFqA4aCOG"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Different filters detect different edges, shapes,...","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Vk4HktrfzO"}],"key":"lEghjlxYmb"}],"key":"pl2f1b8cYU"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Not all seem useful","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vZjdltt8JW"}],"key":"vYKFRlC7QU"}],"key":"VFI6Rck33v"}],"key":"LOe7YRo0e8"}],"identifier":"kao1xfogyidv","label":"kaO1xfOGYidv","html_id":"kao1xfogyidv","visibility":"show","key":"rtGWCrhMMy"},{"type":"block","kind":"notebook-code","data":{"id":"aB41kT1GYidv","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"# More images\n# Fetch some Fashion-MNIST images\nboot = X_fm[0].reshape(28, 28)\nshirt = X_fm[1].reshape(28, 28)\ndress = X_fm[2].reshape(28, 28)\nimage_names = ('boot', 'shirt', 'dress')\nimages = (boot, shirt, dress)\n\ndef plot_filter_bank(images):\n    # Create a set of kernels, apply them to each image, store the results\n    results = []\n    kernel_params = []\n    for theta in (0, 1):\n        theta = theta / 4. * np.pi\n        for frequency in (0.1, 0.2):\n            for sigma in (1, 3):\n                kernel = gabor_kernel(frequency, theta=theta,sigma_x=sigma,sigma_y=sigma)\n                params = 'theta=%.2f,\\nfrequency=%.2f\\nsigma=%.2f' % (theta, frequency, sigma)\n                kernel_params.append(params)\n                results.append((kernel, [magnitude(img, kernel) for img in images]))\n\n    # Plotting\n    fig, axes = plt.subplots(nrows=4, ncols=9, figsize=(14*fig_scale, 8*fig_scale))\n    plt.gray()\n    #fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)\n    axes[0][0].axis('off')\n\n    for label, img, ax in zip(image_names, images, axes[1:]):\n        axs = ax[0]\n        axs.imshow(img)\n        axs.set_ylabel(label, fontsize=12*fig_scale)\n        axs.set_xticks([]) # Remove axis ticks \n        axs.set_yticks([])\n        \n    # Plot Gabor kernel\n    col = 1\n    for label, (kernel, magnitudes), ax_col in zip(kernel_params, results, axes[0][1:]):\n        ax_col.imshow(np.real(kernel), interpolation='nearest') # Plot kernel\n        ax_col.set_title(label, fontsize=10*fig_scale)\n        ax_col.axis('off')\n        \n        # Plot Gabor responses with the contrast normalized for each filter\n        vmin = np.min(magnitudes)\n        vmax = np.max(magnitudes)\n        for patch, ax in zip(magnitudes, axes.T[col][1:]):\n            ax.imshow(patch, vmin=vmin, vmax=vmax) # Plot convolutions\n            ax.axis('off')\n        col += 1\n    \n    plt.show()\n\nplot_filter_bank(images)","identifier":"ab41kt1gyidv-code","visibility":"hide","enumerator":"15","html_id":"ab41kt1gyidv-code","key":"YZjwbs8Sja"},{"type":"outputs","id":"f3SPzbs0j9yjDXF5GN3GO","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"94c3c2de558c73ecafb897c89a02b29a","path":"/94c3c2de558c73ecafb897c89a02b29a.json"},"image/png":{"content_type":"image/png","hash":"b8b6855bf469b88db89ec471d88f5957","path":"/b8b6855bf469b88db89ec471d88f5957.png"},"text/plain":{"content":"<Figure size 2100x1200 with 36 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"ab41kt1gyidv-outputs-0","html_id":"ab41kt1gyidv-outputs-0","key":"vdF4ZxHyhK"}],"identifier":"ab41kt1gyidv-outputs","visibility":"show","html_id":"ab41kt1gyidv-outputs","key":"xBXWFYLrzC"}],"identifier":"ab41kt1gyidv","label":"aB41kT1GYidv","html_id":"ab41kt1gyidv","visibility":"show","key":"xohHMZR2ek"},{"type":"block","kind":"notebook-content","data":{"id":"qU2P04M8Yidw","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Convolutional neural nets","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"UPWSc73eNW"}],"identifier":"convolutional-neural-nets","label":"Convolutional neural nets","html_id":"convolutional-neural-nets","implicit":true,"key":"GWxR1U64hW"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Finding relationships between individual pixels and the correct class is hard","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"sErm8pyQlW"}],"key":"zcw8KwTZEg"}],"key":"nhagnpBU7W"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Simplify the problem by decomposing it into smaller problems","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"WARF2dLjck"}],"key":"WLRfxZqM2l"}],"key":"jFnTaQm3Iq"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"First, discover â€˜localâ€™ patterns (edges, lines, endpoints)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"k4lmGECqBV"}],"key":"fcJ9fYmFgl"}],"key":"Yj2RZNRgRy"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Representing such local patterns as features makes it easier to learn from them","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"MzdYB5YVEv"}],"key":"DLeQKNXKPj"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper layers will do that for us","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"GbtnPV7NZ3"}],"key":"UYq0ERfuq2"}],"key":"itZgqQGAcw"}],"key":"q6H9Qf0dba"}],"key":"CGmchzrjuy"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We could use convolutions, but how to choose the filters?","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"l0UeUAPTMX"}],"key":"GPuGVCW7wF"}],"key":"YqsluPkwUa"}],"key":"SC19Zrrizs"},{"type":"image","style":{"width":"300px","marginLeft":"auto","marginRight":"auto"},"url":"/c7d23bda8e8a2f88ba7bcdd0c7f8a35c.png","alt":"ml","key":"ECTfQz94Oj","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_patches.png?raw=1","urlOptimized":"/c7d23bda8e8a2f88ba7bcdd0c7f8a35c.webp"}],"identifier":"qu2p04m8yidw","label":"qU2P04M8Yidw","html_id":"qu2p04m8yidw","visibility":"show","key":"psOx4pjVKh"},{"type":"block","kind":"notebook-content","data":{"id":"9zsULjBQYidw","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Convolutional Neural Networks (ConvNets)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yXgacTdhnb"}],"identifier":"convolutional-neural-networks-convnets","label":"Convolutional Neural Networks (ConvNets)","html_id":"convolutional-neural-networks-convnets","implicit":true,"key":"oQAxB33cYn"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Instead of manually designing the filters, we can also ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"C7tndzjpA2"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"learn","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"xrn8Lebr8h"}],"key":"vqhxcJpCz1"},{"type":"text","value":" them based on data","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"A3DnQOLo2H"}],"key":"CK6sRo5dMf"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Choose filter sizes (manually), initialize with small random weights","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d4myj8h6eg"}],"key":"lFDybMh8BB"}],"key":"lvE2PtcHGS"}],"key":"Zd9YrUhE06"}],"key":"CNzjFY7mBp"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Forward pass: Convolutional layer slides the filter over the input, generates the output","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"iHvLK6Ef8i"}],"key":"pZatBQkyvj"}],"key":"iA2DLXk4qe"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Backward pass: Update the filter weights according to the loss gradients","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"oo3VNluuSs"}],"key":"FFQXCRg1Gp"}],"key":"mWH3X9QBrJ"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Illustration for 1 filter:","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"qHHXtYvYSS"}],"key":"vIiOddJNRZ"}],"key":"ErVGcsVffJ"}],"key":"hRxroLFSwq"},{"type":"image","style":{"width":"500px","marginLeft":"auto","marginRight":"auto"},"url":"/5886a09717516c68d1d1808d05a2402f.png","alt":"ml","key":"I9noCZ1w3r","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/cnn.png?raw=1"}],"identifier":"9zsuljbqyidw","label":"9zsULjBQYidw","html_id":"id-9zsuljbqyidw","visibility":"show","key":"BhqcXhaMKd"},{"type":"block","kind":"notebook-content","data":{"cell_style":"center","id":"vac7H3HNYidw","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Convolutional layers: Feature maps","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MrNZy7DJLC"}],"identifier":"convolutional-layers-feature-maps","label":"Convolutional layers: Feature maps","html_id":"convolutional-layers-feature-maps","implicit":true,"key":"oNEA8CGD0v"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"One filter is not sufficient to detect all relevant patterns in an image","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"SVfnwkEHaS"}],"key":"lLuwJzAcwE"}],"key":"sQ255hjPFf"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"A convolutional layer applies and learns ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"AUkXb2Cjn6"},{"type":"inlineMath","value":"d","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span></span></span></span>","key":"bqsTwHgIrN"},{"type":"text","value":" filters in parallel","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SsAlknkRyd"}],"key":"iOeYYFXjff"}],"key":"QesjRoeXGy"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Slide ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"LnyLRELkoz"},{"type":"inlineMath","value":"d","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span></span></span></span>","key":"EOMjwUfD38"},{"type":"text","value":" filters across the input image (in parallel) -> a (1x1xd) output per patch","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"g8GoNLyQyX"}],"key":"K3eU4blM9h"}],"key":"EReQxazYaN"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Reassemble into a ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"KgS48mgLxy"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"feature map","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"M2V6r4sQsD"}],"key":"j3Fq4sAo8Y"},{"type":"text","value":" with ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"MGbWksojlF"},{"type":"inlineMath","value":"d","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span></span></span></span>","key":"SDixTdpT4S"},{"type":"text","value":" â€˜channelsâ€™, a (width x height x d) tensor.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"g2KEu4vQBx"}],"key":"UTu8TcN8Ep"}],"key":"pNJJVEYgeQ"}],"key":"FY27SMhwM7"},{"type":"image","style":{"width":"600px","marginLeft":"auto","marginRight":"auto"},"url":"/9c9ac0cf06a7d2fb4da8e3d53efb9c41.png","alt":"ml","key":"MMhCu4FEIx","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_convolution.png?raw=1","urlOptimized":"/9c9ac0cf06a7d2fb4da8e3d53efb9c41.webp"}],"identifier":"vac7h3hnyidw","label":"vac7H3HNYidw","html_id":"vac7h3hnyidw","visibility":"show","key":"v7tPwFycbR"},{"type":"block","kind":"notebook-content","data":{"id":"Z_FWWcutYidw","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Border effects (zero padding)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Px2MhxYRaL"}],"identifier":"border-effects-zero-padding","label":"Border effects (zero padding)","html_id":"border-effects-zero-padding","implicit":true,"key":"wDJ2FinYAv"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Consider a 5x5 image and a 3x3 filter: there are only 9 possible locations, hence the output is a 3x3 feature map","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"AhXq99EFuk"}],"key":"FBftoEDwjW"}],"key":"WP2tw5ljST"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"If we want to maintain the image size, we use ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pcV6h2RH2Y"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"zero-padding","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"OQwiofIwxI"}],"key":"ieATHjt2vP"},{"type":"text","value":", adding 0â€™s all around the input tensor.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"u9CZxSAAjD"}],"key":"zcTTu5tC9h"}],"key":"mRa810t0hW"}],"key":"riB5BvEeIZ"},{"type":"image","style":{"float":"left","width":"45%"},"url":"/e42f34e0003f0d63123bba8c2ef59803.png","alt":"ml","key":"iveeikotWM","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_padding.png?raw=1","urlOptimized":"/e42f34e0003f0d63123bba8c2ef59803.webp"},{"type":"text","value":" ","key":"yZurmtMeMO"},{"type":"image","style":{"float":"left","width":"45%"},"url":"/bd943ce15326a49c665b8b5a7c690667.png","alt":"ml","key":"pd0XoJYo93","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_padding_2.png?raw=1","urlOptimized":"/bd943ce15326a49c665b8b5a7c690667.webp"}],"identifier":"z_fwwcutyidw","label":"Z_FWWcutYidw","html_id":"z-fwwcutyidw","visibility":"show","key":"d82B85eARJ"},{"type":"block","kind":"notebook-content","data":{"id":"aYvkxOPAYidw","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Undersampling (striding)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tYH5Hf6a3D"}],"identifier":"undersampling-striding","label":"Undersampling (striding)","html_id":"undersampling-striding","implicit":true,"key":"JqTMzIFfpd"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sometimes, we want to ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"CMZUoGCOew"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"downsample","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"SwzdgVQH3O"}],"key":"LceSzp5sV0"},{"type":"text","value":" a high-resolution image","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Rxj9Ii8fnk"}],"key":"GcY38fzkG7"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Faster processing, less noisy (hence less overfitting)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GU1uvgNnve"}],"key":"B44NnIE1NM"}],"key":"OiBCRjVh7O"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Forces the model to summarize information in (smaller) feature maps","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"f9zBz9lhJY"}],"key":"L4LgLhIWAw"}],"key":"GDDxmeaL29"}],"key":"mvMliYuroc"}],"key":"d35skyO6ec"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"One approach is to ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZFnu4waPQI"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"skip","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"n1DlFxlgM0"}],"key":"GlePmOXWLy"},{"type":"text","value":" values during the convolution","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"MLxlN4K4Ad"}],"key":"YjJa9sRdwy"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Distance between 2 windows: ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"YFkilTHE4v"},{"type":"emphasis","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"stride length","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"bsPUaNc3t7"}],"key":"g3k75FTLHy"}],"key":"jysUMpH9Ve"}],"key":"RCRWQyoCoi"}],"key":"x0vlL4AXsh"}],"key":"SaQKSxFTnV"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Example with stride length 2 (without padding):","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"xWTzW2wdma"}],"key":"lAf6qXNKet"}],"key":"HZVLAJoS36"}],"key":"Iap8Vv7BQQ"},{"type":"image","style":{"width":"700px","marginLeft":"auto","marginRight":"auto"},"url":"/6a59aee3321202d3fc5cbec4666e8024.png","alt":"ml","key":"Ab7RgwmYgR","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_strides.png?raw=1","urlOptimized":"/6a59aee3321202d3fc5cbec4666e8024.webp"}],"identifier":"ayvkxopayidw","label":"aYvkxOPAYidw","html_id":"ayvkxopayidw","visibility":"show","key":"naGTZORg86"},{"type":"block","kind":"notebook-content","data":{"id":"fMLBW1wLYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Max-pooling","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vQ8ZOpcZ1u"}],"identifier":"max-pooling","label":"Max-pooling","html_id":"max-pooling","implicit":true,"key":"yxewF8E9Jf"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Another approach to shrink the input tensors is ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Y5zC9WRlSa"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"max-pooling","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Eq0hXndEgF"}],"key":"SwjJX8G7Ke"},{"type":"text","value":" :","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qLl9nhReqx"}],"key":"Jce9AnRk9m"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Run a filter with a fixed stride length over the image","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Wwif9OZ59e"}],"key":"K0Sx860wyZ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Usually 2x2 filters and stride lenght 2","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"xSsHC2heQT"}],"key":"MqngHiUkuW"}],"key":"wLWpvoUevC"}],"key":"xgI9btuWxA"}],"key":"gDgcCH0j8r"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The filter simply returns the ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"nmysjyhxR2"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"max","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"t7spI8QiSo"}],"key":"ZY5O79wvGt"},{"type":"text","value":" (or ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"mTjChbxDrC"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"avg","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"kxUUwjOX6T"}],"key":"oHDoOoxczl"},{"type":"text","value":" ) of all values","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ygrp6d0vE7"}],"key":"mgnXLoOgxX"}],"key":"Fleko9Th5m"}],"key":"Uprz1cb8Fp"}],"key":"glvxQTzLEE"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Agressively reduces the number of weights (less overfitting)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"jHSzLI9Hcp"}],"key":"JIdVKwXNnD"}],"key":"oarP8M6wcs"}],"key":"gxgO6BJp1m"},{"type":"image","style":{"width":"800px","marginLeft":"auto","marginRight":"auto"},"url":"/c9c1aa08cfb2f72798463fd9fc92163b.png","alt":"ml","key":"a8jNKPm8sq","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/maxpool.png?raw=1","urlOptimized":"/c9c1aa08cfb2f72798463fd9fc92163b.webp"}],"identifier":"fmlbw1wlyidx","label":"fMLBW1wLYidx","html_id":"fmlbw1wlyidx","visibility":"show","key":"Jpr5xv1pOW"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Receptive field","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Gflze0qbGU"}],"identifier":"receptive-field","label":"Receptive field","html_id":"receptive-field","implicit":true,"key":"g1oWTOlNgO"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Receptive field","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"PCLJuVii30"}],"key":"YKeS5gWZed"},{"type":"text","value":": how much each output neuron â€˜seesâ€™ of the input image","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lwJC7yzkmW"}],"key":"pIJ15eRuXQ"}],"key":"vkIIrT8XUU"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Translation invariance","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"o3hEykOLE4"}],"key":"dDnF1ml3Gh"},{"type":"text","value":": shifting the input does not affect the output","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"WtkV01Z176"}],"key":"Enr3EUFGa0"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Large receptive field -> neurons can â€˜seeâ€™ patterns anywhere in the input","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"NX1dDb5NwE"}],"key":"QX0CQJ7O0Z"}],"key":"XzrnzPBGmo"}],"key":"pLpFDJkqz4"}],"key":"Qra0xw9Ttu"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"nxn","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mi>x</mi><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">nxn</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mord mathnormal\">x</span><span class=\"mord mathnormal\">n</span></span></span></span>","key":"flcE1Iv6Sr"},{"type":"text","value":" convolutions only increase the receptive field by ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"L709jfgAfS"},{"type":"inlineMath","value":"n+2","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi><mo>+</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">n+2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6667em;vertical-align:-0.0833em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6444em;\"></span><span class=\"mord\">2</span></span></span></span>","key":"TFgUJJ6ztC"},{"type":"text","value":" each layer","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"booDSQ8y9B"}],"key":"eULPEUkfsQ"}],"key":"ZHqXnWgaTz"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Maxpooling doubles the receptive field without deepening the network","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"DPvcfOjlZE"}],"key":"t8Ga41ciuj"}],"key":"CbMlzVXC99"}],"key":"DTbzPHTn1o"}],"key":"R5rsnfhehT"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"-"}},"children":[{"type":"code","lang":"python","executable":true,"value":"import matplotlib.patches as patches\n\ndef draw_grid(ax, size, offset):\n    \"\"\"Draws a grid without text labels\"\"\"\n    for i in range(size):\n        for j in range(size):\n            ax.add_patch(patches.Rectangle((j + offset[0], -i + offset[1]), 1, 1, \n                                           fill=False, edgecolor='gray', linewidth=1))\n\ndef highlight_region(ax, positions, offset, color, alpha=0.3):\n    \"\"\"Highlights a specific region in the grid\"\"\"\n    for x, y in positions:\n        ax.add_patch(patches.Rectangle((x + offset[0], -y + offset[1]), 1, 1, fill=True, color=color, alpha=alpha))\n\ndef draw_connection_hull(ax, points, color, alpha):\n    \"\"\"Draws a polygon representing the hull of connection lines\"\"\"\n    ax.add_patch(patches.Polygon(points, closed=True, facecolor=color, alpha=alpha, edgecolor=None))\n    \ndef add_titles(ax, option):\n    \"\"\"Adds titles above each matrix\"\"\"\n    titles = [\"Input\", option, \"Output_1\", \"Kernel_2\", \"Output_2\"]\n    positions = [(0, 1.5), (9, 1.5), (15, 1.5), (20, 1.5), (24, 1.5)]\n    \n    for title, (x, y) in zip(titles, positions):\n        ax.text(x, y, title, fontsize=12, fontweight='bold', ha='left')\n\nlayer_options = ['3x3 Kernel', '3x3 Kernel, Stride 2', '5x5 Kernel', 'MaxPool 2x2']\nlayer_options2 = ['3x3 Kernel', '3x3 Kernel, Dilation 2']\n\n@interact\ndef visualize_receptive_field(option=layer_options):\n    fig, ax = plt.subplots(figsize=(18, 6))\n    ax.set_xlim(-2, 26)\n    ax.set_ylim(-9, 2)\n    ax.axis('off')\n    add_titles(ax, option)\n    kernel_size = 0\n    \n    grids = [(8, (0, 0)), (4, (15, 0)), (3, (20, 0)), (2, (24, 0))]\n    \n    single_output_rf = [(0, 0)]\n    for size, offset in grids:\n        draw_grid(ax, size, offset)\n    \n    if option == 'MaxPool 2x2':\n        full_input_rf = [(x, y) for x in range(6) for y in range(6)]\n        highlight_region(ax, full_input_rf, (0, 0), 'green', alpha=0.3)\n    else:\n        kernel_size = 3 if option.startswith('3x3 Kernel') else 5\n        draw_grid(ax, kernel_size, (9, 0))\n        \n        input_highlight_size = kernel_size + 2\n        if option == '3x3 Kernel, Stride 2' or option == '3x3 Kernel, Dilation 2':\n            input_highlight_size = kernel_size + 4\n\n        full_input_rf = [(x, y) for x in range(input_highlight_size) for y in range(input_highlight_size)]\n        kernel_1 = [(x, y) for x in range(kernel_size) for y in range(kernel_size)]\n        kernel_rf = kernel_1\n        if option == '3x3 Kernel, Dilation 2':\n            kernel_rf = [(x*2, y*2) for x in range(kernel_size) for y in range(kernel_size)]\n\n        highlight_region(ax, full_input_rf, (0, 0), 'green')\n        highlight_region(ax, kernel_rf, (0, 0), 'blue')\n        highlight_region(ax, kernel_1, (9, 0), 'blue')\n        highlight_region(ax, single_output_rf, (15, 0), 'blue')\n    \n    kernel2_rf = [(x, y) for x in range(3) for y in range(3)]\n    \n    highlight_region(ax, kernel2_rf, (15, 0), 'green')\n    highlight_region(ax, kernel2_rf, (20, 0), 'green')\n    highlight_region(ax, single_output_rf, (24, 0), 'green')\n    \n    connection_hulls = [\n        ([(23, -2), (23, 1), (24, 1), (24, 0)], 'green', 0.1),\n        ([(18, -2), (18, 1), (20, 1), (20, -2)], 'green', 0.1)\n    ]\n    \n    kernel_fp = kernel_size * 2 - 1 if option == '3x3 Kernel, Dilation 2' else kernel_size\n\n    if option != 'MaxPool 2x2':\n        connection_hulls.extend([\n            ([(kernel_fp, 1-kernel_fp), (kernel_fp, 1), (9, 1), (9, 1-kernel_size)], 'blue', 0.1),\n            ([(9+kernel_size, 1-kernel_size), (9+kernel_size, 1), (15, 1), (15, 0)], 'blue', 0.1)\n        ])\n    else:\n        connection_hulls.extend([\n            ([(6, -5), (6, 1), (15, 1), (15, -2)], 'green', 0.1),\n        ])        \n    \n    for points, color, alpha in connection_hulls:\n        draw_connection_hull(ax, points, color, alpha)\n    \n    plt.show()","key":"eLf4gfkuzQ"},{"type":"outputs","id":"Yv0fa5ZAzZaVmYDzCyqcC","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"f63f0903aed44141b5331039c3d150a0\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(Dropdown(description='option', options=('3x3 Kernel', '3x3 Kernel, Stride 2', '5x5 Kerneâ€¦","content_type":"text/plain"}}},"children":[],"key":"amU3R1PJ1O"}],"key":"ETUdt6F0XI"}],"key":"Kd036JWakM"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"-"}},"children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    for option in layer_options[0::3]:\n        visualize_receptive_field(option=option)","key":"GSAMqNbqTn"},{"type":"outputs","id":"7JdxlazsxUbT1ew-BOH-X","children":[],"key":"Z5UUPtRLs8"}],"key":"nOVWGRIvqo"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Dilated convolutions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"b2j02Gr3V3"}],"identifier":"dilated-convolutions","label":"Dilated convolutions","html_id":"dilated-convolutions","implicit":true,"key":"sZzO8nvnQn"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Downsample by introducing â€˜gapsâ€™ between filter elements by spacing them out","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"RiaqkqMIFX"}],"key":"CYsYtsMYNW"}],"key":"K2Orb6FEgw"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Increases the receptive field exponentially","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"aD1hZMLGZA"}],"key":"bjsn6mhf7u"}],"key":"WuvFALW2th"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Doesnâ€™t need extra parameters or computation (unlike larger filters)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"gPQgN1I9sD"}],"key":"eKlYmlTyfq"}],"key":"E8nn8VoxSP"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Retains feature map size (unlike pooling)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"v4sOwS3JlT"}],"key":"CYD3IEEGwV"}],"key":"c7AZmSkwo1"}],"key":"jZcL7tIHii"}],"key":"PIItT5xTDs"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"@interact\ndef visualize_receptive_field2(option=layer_options2):\n    visualize_receptive_field(option)","key":"fRgudvIkS6"},{"type":"outputs","id":"5jSRf2tL1jcQfEf5JrsJs","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"ba7504fe72974165ac4d81250b4a6149\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(Dropdown(description='option', options=('3x3 Kernel', '3x3 Kernel, Dilation 2'), value='â€¦","content_type":"text/plain"}}},"children":[],"key":"KknrsCSAYm"}],"key":"g3EoqrMOCJ"}],"key":"cBJenpaVRC"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    visualize_receptive_field(option=layer_options2[1])","key":"feQoCcyhcJ"},{"type":"outputs","id":"33vso0oFCoFd8kX4z4Pva","children":[],"key":"pZls6NqfsM"}],"key":"ELnNC7tCx2"},{"type":"block","kind":"notebook-content","data":{"id":"MWkWdA1ZYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Convolutional nets in practice","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"JnA5v0tg3X"}],"identifier":"convolutional-nets-in-practice","label":"Convolutional nets in practice","html_id":"convolutional-nets-in-practice","implicit":true,"key":"jIaM5mtL9Q"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use multiple convolutional layers to learn patterns at different levels of abstraction","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"RvaXyPJ1FJ"}],"key":"pSH5SUwH3g"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Find local patterns first (e.g. edges), then patterns across those patterns","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QDNyI8Q30n"}],"key":"EHkvbF86xi"}],"key":"BvPNf8g59R"}],"key":"B3U3Aay25A"}],"key":"X5puuOPlEu"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use MaxPooling layers to reduce resolution, increase translation invariance","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"kj1vj4xVsr"}],"key":"c8lMOI7ABa"}],"key":"qDfgobilD0"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use sufficient filters in the first layer (otherwise information gets lost)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"YAvz5pcvxT"}],"key":"mo5h1l8H2Z"}],"key":"kwamHELAiP"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"In deeper layers, use increasingly more filters","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"E9jwIC73bA"}],"key":"EYnNkSbT2g"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Preserve information about the input as resolution descreases","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"yMvFuC5KOf"}],"key":"gZznz8PWaL"}],"key":"NqrNBvQ0o8"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Avoid decreasing the number of activations (resolution x nr of filters)","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"yH4A3wVouE"}],"key":"dTgzhzGpWP"}],"key":"EroskueaRJ"}],"key":"rHM4TAv1dI"}],"key":"F6odfDgycT"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"For very deep nets, add ","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"rd7FLJkyhp"},{"type":"emphasis","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"skip connections","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"Jf2AVOlidb"}],"key":"isTRdm5765"},{"type":"text","value":" to preserve information (and gradients)","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"OdKZ6kb5Z7"}],"key":"hbjq9nj7Wg"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Sums up outputs of earlier layers to those of later layers (with same dimensions)","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"AE2SQPpu40"}],"key":"maS7JmSz7v"}],"key":"UAyK9XuZiA"}],"key":"Ekh164OgI2"}],"key":"AyHbaDKnmw"}],"key":"CK5DcO8pFz"}],"identifier":"mwkwda1zyidx","label":"MWkWdA1ZYidx","html_id":"mwkwda1zyidx","visibility":"show","key":"bECSPDkH76"},{"type":"block","kind":"notebook-content","data":{"id":"dBJasfAZYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Example with PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"hqSoBH4ldJ"}],"identifier":"example-with-pytorch","label":"Example with PyTorch","html_id":"example-with-pytorch","implicit":true,"key":"JqGxZyQcVz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"Conv2d","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lGEtkgfFAD"},{"type":"text","value":" for 2D convolutional layers","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uySy75bF1s"}],"key":"pi80cgEmTD"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Grayscale image: 1 ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"qndiOgyZ2i"},{"type":"emphasis","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"in_channels","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"CC2h5bMqkz"}],"key":"cV8ZBLRBeF"}],"key":"jyIDJ3dPdX"}],"key":"w3QBhFbHhw"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"32 filters: 32 ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"YQEPO0GYC1"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"out_channels","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"TRAx7Z4OcT"}],"key":"LvTaCLfCAO"},{"type":"text","value":", 3x3 size","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"Bh170xz73i"}],"key":"lJTPFf7MoC"}],"key":"Lsn1YpAs8j"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper layers use 64 filters","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"m55Ihtn2S3"}],"key":"JyXKpeiSvA"}],"key":"WbThXpbayv"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"ReLU","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"LsKrTOAd67"},{"type":"text","value":" activation, no padding","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"eGdOOZEHpf"}],"key":"JZxKG7QByU"}],"key":"fz0Let4DHc"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"MaxPool2d","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"A1fMVuf6ia"},{"type":"text","value":" for max-pooling, 2x2","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"zDWkiSdMf4"}],"key":"qH1B3ZsYCU"}],"key":"aKBCHeU74h"}],"key":"QQCnBZrhpW"}],"key":"AXGNhAXU6s"}],"key":"cSiW7WEmKr"},{"type":"code","lang":"python","value":"model = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n    nn.ReLU()\n)","position":{"start":{"line":10,"column":1},"end":{"line":21,"column":1}},"identifier":"dbjasfazyidx-code","enumerator":"16","html_id":"dbjasfazyidx-code","key":"ZeazTvyPnY"}],"identifier":"dbjasfazyidx","label":"dBJasfAZYidx","html_id":"dbjasfazyidx","visibility":"show","key":"hwyrS4tJMZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import torch\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0),\n    nn.ReLU(), \n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU()\n)","key":"Rv6dyWcelo"},{"type":"outputs","id":"o6IJ2pLJCv4N_0L923bIX","children":[],"key":"sFZFl7SK4T"}],"key":"ScotKsPmtH"},{"type":"block","kind":"notebook-content","data":{"id":"Dsycp1XqYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Observe how the input image on 1x28x28 is transformed to a 64x3x3 feature map","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yo5HGM0yiu"}],"key":"c12QTJijo1"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"In pytorch, shapes are (batch_size, channels, height, width)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"VD4aqtrJRc"}],"key":"WN9cFqTWzH"}],"key":"mG6wfbh6uo"}],"key":"uz7TafO4cM"}],"key":"JXo5RVu5cA"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Conv2d parameters = (kernelÂ size^2 Ã— inputÂ channels + 1) Ã— outputÂ channels","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"olPDvoW4QM"}],"key":"XEkE5SoxWD"}],"key":"mIgjJzXVUx"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"No zero-padding: every output is 2 pixels less in every dimension","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"GPOwMydmKc"}],"key":"II3lzX4FSl"}],"key":"qm2kIQQv1s"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"After every MaxPooling, resolution halved in every dimension","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"XHqhekmtiZ"}],"key":"NcWS2FJOOH"}],"key":"iCjWAfCuCx"}],"key":"mKuGu1M6ya"}],"identifier":"dsycp1xqyidx","label":"Dsycp1XqYidx","html_id":"dsycp1xqyidx","visibility":"show","key":"rHBvhN6Edf"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from torchinfo import summary\nsummary(model, input_size=(1, 1, 28, 28))","key":"sEvO2WcGni"},{"type":"outputs","id":"bV6MMuAoBTKE4zvSpeKhC","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":23,"metadata":{},"data":{"text/plain":{"content":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [1, 64, 3, 3]             --\nâ”œâ”€Conv2d: 1-1                            [1, 32, 26, 26]           320\nâ”œâ”€ReLU: 1-2                              [1, 32, 26, 26]           --\nâ”œâ”€MaxPool2d: 1-3                         [1, 32, 13, 13]           --\nâ”œâ”€Conv2d: 1-4                            [1, 64, 11, 11]           18,496\nâ”œâ”€ReLU: 1-5                              [1, 64, 11, 11]           --\nâ”œâ”€MaxPool2d: 1-6                         [1, 64, 5, 5]             --\nâ”œâ”€Conv2d: 1-7                            [1, 64, 3, 3]             36,928\nâ”œâ”€ReLU: 1-8                              [1, 64, 3, 3]             --\n==========================================================================================\nTotal params: 55,744\nTrainable params: 55,744\nNon-trainable params: 0\nTotal mult-adds (M): 2.79\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.24\nParams size (MB): 0.22\nEstimated Total Size (MB): 0.47\n==========================================================================================","content_type":"text/plain"}}},"children":[],"key":"drGqVEohKg"}],"key":"aIigeG3OqL"}],"key":"arX8m2UjKt"},{"type":"block","kind":"notebook-content","data":{"id":"U33_pUgfYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"To classify the images, we still need a linear and output layer.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MuAcQWeCtO"}],"key":"Q52rPL0mlj"}],"key":"X4CFJHKyob"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We flatten the 3x3x64 feature map to a vector of size 576","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"yELjIRIR6M"}],"key":"sJjIgcK51L"}],"key":"LAFxvB32gL"}],"key":"YTj5SjqG6l"},{"type":"code","lang":"python","value":"model = nn.Sequential(\n    ...\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(64 * 3 * 3, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10)\n)","position":{"start":{"line":4,"column":1},"end":{"line":14,"column":1}},"identifier":"u33_pugfyidx-code","enumerator":"17","html_id":"u33-pugfyidx-code","key":"p0D76mhxIp"}],"identifier":"u33_pugfyidx","label":"U33_pUgfYidx","html_id":"u33-pugfyidx","visibility":"show","key":"ZsZb4cysH4"},{"type":"block","kind":"notebook-code","data":{"id":"fYKm2llNYidx","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"model = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(64 * 3 * 3, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10)\n)","identifier":"fykm2llnyidx-code","visibility":"hide","enumerator":"18","html_id":"fykm2llnyidx-code","key":"TD6R5mRd4O"},{"type":"outputs","id":"a1Sokxry-K7nJ7597OJWm","children":[],"identifier":"fykm2llnyidx-outputs","visibility":"show","html_id":"fykm2llnyidx-outputs","key":"F3TawriLE6"}],"identifier":"fykm2llnyidx","label":"fYKm2llNYidx","html_id":"fykm2llnyidx","visibility":"show","key":"cbbKOsBPUq"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Complete model. Flattening adds a lot of weights!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Dm06HgFajS"}],"key":"TpA7euT0Sd"}],"key":"K2OAyumteg"},{"type":"block","kind":"notebook-code","data":{"hide_input":true,"id":"vBwH9sGEYidx","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"summary(model, input_size=(1, 1, 28, 28))","identifier":"vbwh9sgeyidx-code","visibility":"hide","enumerator":"19","html_id":"vbwh9sgeyidx-code","key":"r98aMZYDeF"},{"type":"outputs","id":"QcM56kX9Ky2KbbTHsL5sc","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":25,"metadata":{},"data":{"text/plain":{"content":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [1, 10]                   --\nâ”œâ”€Conv2d: 1-1                            [1, 32, 26, 26]           320\nâ”œâ”€ReLU: 1-2                              [1, 32, 26, 26]           --\nâ”œâ”€MaxPool2d: 1-3                         [1, 32, 13, 13]           --\nâ”œâ”€Conv2d: 1-4                            [1, 64, 11, 11]           18,496\nâ”œâ”€ReLU: 1-5                              [1, 64, 11, 11]           --\nâ”œâ”€MaxPool2d: 1-6                         [1, 64, 5, 5]             --\nâ”œâ”€Conv2d: 1-7                            [1, 64, 3, 3]             36,928\nâ”œâ”€ReLU: 1-8                              [1, 64, 3, 3]             --\nâ”œâ”€Flatten: 1-9                           [1, 576]                  --\nâ”œâ”€Linear: 1-10                           [1, 64]                   36,928\nâ”œâ”€ReLU: 1-11                             [1, 64]                   --\nâ”œâ”€Linear: 1-12                           [1, 10]                   650\n==========================================================================================\nTotal params: 93,322\nTrainable params: 93,322\nNon-trainable params: 0\nTotal mult-adds (M): 2.82\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.24\nParams size (MB): 0.37\nEstimated Total Size (MB): 0.62\n==========================================================================================","content_type":"text/plain"}}},"children":[],"identifier":"vbwh9sgeyidx-outputs-0","html_id":"vbwh9sgeyidx-outputs-0","key":"d0VBUlj2HB"}],"identifier":"vbwh9sgeyidx-outputs","visibility":"show","html_id":"vbwh9sgeyidx-outputs","key":"PqaP9K4Kmf"}],"identifier":"vbwh9sgeyidx","label":"vBwH9sGEYidx","html_id":"vbwh9sgeyidx","visibility":"show","key":"zemIWQpjin"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Global Average Pooling (GAP)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"adexfzwurS"}],"identifier":"global-average-pooling-gap","label":"Global Average Pooling (GAP)","html_id":"global-average-pooling-gap","implicit":true,"key":"XYKbCxraWP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Instead of flattening, we do GAP: returns average of each activation map","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"alHvaAJ7HZ"}],"key":"RsfL6eYh1R"}],"key":"Qin0ce43LB"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We can drop the hidden dense layer: number of outputs > number of classes","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"XsR6fi9FXP"}],"key":"FVavWYIpXb"}],"key":"oRX4L3RC9H"}],"key":"txUZ1sC2yv"},{"type":"code","lang":"python","value":"model = nn.Sequential(...\n    nn.AdaptiveAvgPool2d(1), # Global Average Pooling\n    nn.Flatten(),            # Convert (batch, 64, 1, 1) -> (batch, 64)\n    nn.Linear(64, 10))       # Output layer for 10 classes","position":{"start":{"line":5,"column":1},"end":{"line":10,"column":1}},"key":"l4OduqSyLe"},{"type":"image","style":{"width":"70%","marginLeft":"auto","marginRight":"auto"},"url":"/f92734e78c69dbdfce8fc608ad207bdb.png","alt":"ml","key":"OTPEnrzJUW","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/09_GAP.png?raw=1","urlOptimized":"/f92734e78c69dbdfce8fc608ad207bdb.webp"}],"key":"aQwVZNxXru"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"With ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DUwZzwm5EV"},{"type":"inlineCode","value":"GlobalAveragePooling","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uLa1XL3ixQ"},{"type":"text","value":": much fewer weights to learn","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DTlpaoIy7M"}],"key":"f8dKbHuWAr"}],"key":"EfXFZHswUS"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use with caution: this destroys the location information learned by the CNN","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"pGQa3tqfEW"}],"key":"ftnkUhTwyt"}],"key":"DuQexEd8pF"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Not ideal for tasks such as object localization","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"WID41oX7Tp"}],"key":"NDBytzH8ZO"}],"key":"bLH5ZCy1wT"}],"key":"A7dYGLjhqR"}],"key":"pwLaDNg5WN"},{"type":"block","kind":"notebook-code","data":{"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"model = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0),\n    nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1),  # Global Average Pooling (GAP)\n    nn.Flatten(),  # Convert (batch, 64, 1, 1) -> (batch, 64)\n    nn.Linear(64, 10)  # Output layer for 10 classes\n)\nsummary(model, input_size=(1, 1, 28, 28))","visibility":"hide","key":"wueP8aBUF2"},{"type":"outputs","id":"gr6C9Kvd3maKgBaHzhn-V","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":26,"metadata":{},"data":{"text/plain":{"content":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [1, 10]                   --\nâ”œâ”€Conv2d: 1-1                            [1, 32, 26, 26]           320\nâ”œâ”€ReLU: 1-2                              [1, 32, 26, 26]           --\nâ”œâ”€MaxPool2d: 1-3                         [1, 32, 13, 13]           --\nâ”œâ”€Conv2d: 1-4                            [1, 64, 11, 11]           18,496\nâ”œâ”€ReLU: 1-5                              [1, 64, 11, 11]           --\nâ”œâ”€MaxPool2d: 1-6                         [1, 64, 5, 5]             --\nâ”œâ”€Conv2d: 1-7                            [1, 64, 3, 3]             36,928\nâ”œâ”€ReLU: 1-8                              [1, 64, 3, 3]             --\nâ”œâ”€AdaptiveAvgPool2d: 1-9                 [1, 64, 1, 1]             --\nâ”œâ”€Flatten: 1-10                          [1, 64]                   --\nâ”œâ”€Linear: 1-11                           [1, 10]                   650\n==========================================================================================\nTotal params: 56,394\nTrainable params: 56,394\nNon-trainable params: 0\nTotal mult-adds (M): 2.79\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.24\nParams size (MB): 0.23\nEstimated Total Size (MB): 0.47\n==========================================================================================","content_type":"text/plain"}}},"children":[],"key":"teUFAlmKCr"}],"visibility":"show","key":"keNV5Jdb8E"}],"visibility":"show","key":"lufr9SMHYP"},{"type":"block","kind":"notebook-content","data":{"id":"66ch90fUYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Run the model on MNIST dataset","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"K7xDkH0Ti3"}],"key":"lLj3Uo8s5g"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Train and test as usual: 99% accuracy","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"fJWwzajxon"}],"key":"UchZzYDOqd"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compared to 97,8% accuracy with the dense architecture","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SWhRNInpyR"}],"key":"Cdz5y37lYR"}],"key":"vMOryWwkDt"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"Flatten","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"JyS7rAcdVA"},{"type":"text","value":" and ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"xJuOTcJxNx"},{"type":"inlineCode","value":"GlobalAveragePooling","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"WXIRsvQm17"},{"type":"text","value":" yield similar performance","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Efy0e10hXl"}],"key":"quD4cLtCE5"}],"key":"slbmeVQJe8"}],"key":"rBL1gjCiEd"}],"key":"X7BfX6KTHY"}],"key":"Sd2uPxg4w3"}],"identifier":"66ch90fuyidx","label":"66ch90fUYidx","html_id":"id-66ch90fuyidx","visibility":"show","key":"UG6XnHDzey"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"import pytorch_lightning as pl\n\n# Keeps a history of scores to make plotting easier\nclass MetricTracker(pl.Callback):\n    def __init__(self):\n        super().__init__()\n        self.history = {\n            \"train_loss\": [],\n            \"train_acc\": [],\n            \"val_loss\": [],\n            \"val_acc\": []\n        }\n        self.first_validation = True  # Flag to ignore first validation step\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        \"\"\"Collects training metrics at the end of each epoch\"\"\"\n        train_loss = trainer.callback_metrics.get(\"train_loss\")\n        train_acc = trainer.callback_metrics.get(\"train_acc\")\n\n        if train_loss is not None:\n            self.history[\"train_loss\"].append(train_loss.cpu().item())\n        if train_acc is not None:\n            self.history[\"train_acc\"].append(train_acc.cpu().item())\n\n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Collects validation metrics at the end of each epoch\"\"\"\n        if self.first_validation:  \n            self.first_validation = False  # Skip first validation logging\n            return  \n\n        val_loss = trainer.callback_metrics.get(\"val_loss\")\n        val_acc = trainer.callback_metrics.get(\"val_acc\")\n\n        if val_loss is not None:\n            self.history[\"val_loss\"].append(val_loss.cpu().item())\n        if val_acc is not None:\n            self.history[\"val_acc\"].append(val_acc.cpu().item())\n            \ndef plot_training(history):\n    plt.figure(figsize=(12, 4))  # Increased figure size\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history[\"train_loss\"], label=\"Train Loss\", marker='o', lw=2)\n    plt.plot(history[\"val_loss\"], label=\"Validation Loss\", marker='o', lw=2)\n    plt.xlabel(\"Epochs\", fontsize=14)  # Larger font size\n    plt.ylabel(\"Loss\", fontsize=14)\n    plt.title(\"Loss vs. Epochs\", fontsize=16, fontweight=\"bold\")\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12)\n\n    # Plot Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history[\"train_acc\"], label=\"Train Accuracy\", marker='o', lw=2)\n    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\", marker='o', lw=2)\n    plt.xlabel(\"Epochs\", fontsize=14)\n    plt.ylabel(\"Accuracy\", fontsize=14)\n    plt.title(\"Accuracy vs. Epochs\", fontsize=16, fontweight=\"bold\")\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=12)\n\n    plt.tight_layout()  # Adjust layout for readability\n    plt.show()","key":"eva3R7cBPl"},{"type":"outputs","id":"-j762oUAFD7sCXVGHcFDx","children":[],"key":"HXwu3YPOJz"}],"key":"FT5kRMDT43"},{"type":"block","kind":"notebook-code","data":{"id":"esBEzxeBYidx","slideshow":{"slide_type":"skip"},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"import torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split\nfrom torchmetrics.functional import accuracy\n\n# Model in Pytorch Lightning\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, learning_rate=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.Linear(64, 10)\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    # Logging of loss and accuracy for later plotting\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = accuracy(logits, y, task=\"multiclass\", num_classes=10)\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = accuracy(logits, y, task=\"multiclass\", num_classes=10)\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n# Compute mean and std to normalize the data\n# Couldn't find a way to do this automatically in PyTorch :(\n# Normalization is not strictly needed, but speeds up convergence\ndataset = datasets.MNIST(root=\".\", train=True, transform=transforms.ToTensor(), download=True)\nloader = torch.utils.data.DataLoader(dataset, batch_size=1000, num_workers=4, shuffle=False)\nmean = torch.mean(torch.stack([batch[0].mean() for batch in loader]))\nstd = torch.mean(torch.stack([batch[0].std() for batch in loader]))\n\n# Loading the data. We'll discuss data loaders again soon.\nclass MNISTDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size=64):\n        super().__init__()\n        self.batch_size = batch_size\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((mean,), (std,))  # Normalize MNIST. Make more general?\n        ])\n\n    def prepare_data(self):\n        datasets.MNIST(root=\".\", train=True, download=True)  # Downloads dataset\n\n    def setup(self, stage=None):\n        full_train = datasets.MNIST(root=\".\", train=True, transform=self.transform)\n        self.train, self.val = random_split(full_train, [55000, 5000])\n        self.test = datasets.MNIST(root=\".\", train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val, batch_size=self.batch_size, num_workers=4)\n\n    def test_dataloader(self):\n        return DataLoader(self.test, batch_size=self.batch_size, num_workers=4)\n\n    \n# Initialize data & model\npl.seed_everything(42)  # Ensure reproducibility\ndata_module = MNISTDataModule(batch_size=64)\nmodel = MNISTModel(learning_rate=0.001)\n\n# Trainer with logging & checkpointing\naccelerator = \"cpu\"\nif torch.backends.mps.is_available():\n    accelerator = \"mps\"\nif torch.cuda.is_available():\n    accelerator = \"gpu\"\n\nmetric_tracker = MetricTracker()  # Callback to track per-epoch metrics\n\ntrainer = pl.Trainer(\n    max_epochs=10,  # Train for 10 epochs\n    accelerator=accelerator,\n    devices=\"auto\",\n    log_every_n_steps=10,\n    deterministic=True,\n    callbacks=[metric_tracker]  # Attach callback to trainer\n)\n\nif histories and histories[\"mnist\"]:\n    history = histories[\"mnist\"]\nelse:\n    trainer.fit(model, datamodule=data_module)\n    history = metric_tracker.history\n\n# Test after training (sanity check)\n# trainer.test(model, datamodule=data_module)","identifier":"esbezxebyidx-code","visibility":"hide","enumerator":"20","html_id":"esbezxebyidx-code","key":"BGCjKv1zJN"},{"type":"outputs","id":"76nBm57l7nkDTXA1ZqKuf","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"Seed set to 42\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n"},"children":[],"identifier":"esbezxebyidx-outputs-0","html_id":"esbezxebyidx-outputs-0","key":"s6gbzL2anY"}],"identifier":"esbezxebyidx-outputs","visibility":"show","html_id":"esbezxebyidx-outputs","key":"PlFf1A8yOc"}],"identifier":"esbezxebyidx","label":"esBEzxeBYidx","html_id":"esbezxebyidx","visibility":"show","key":"UIfwuapF7N"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_training(history)","key":"HiMslzsxCB"},{"type":"outputs","id":"O0LLoZqzk4WHXfVn2gN_x","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"13c3b6803fcb8d66dfe75eaeb9aec19e","path":"/13c3b6803fcb8d66dfe75eaeb9aec19e.json"},"image/png":{"content_type":"image/png","hash":"23a51f314eb4e9894d4a9e079f154b22","path":"/23a51f314eb4e9894d4a9e079f154b22.png"},"text/plain":{"content":"<Figure size 3600x1200 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"DwXCj24Pph"}],"key":"stg3ngWhSo"}],"key":"ES63XsT4lO"},{"type":"block","kind":"notebook-content","data":{"id":"LG9VevMHYidx","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Cats vs Dogs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RyIhF00g5X"}],"identifier":"cats-vs-dogs","label":"Cats vs Dogs","html_id":"cats-vs-dogs","implicit":true,"key":"WEXAuFkS2O"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"A more realistic dataset: ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kusUbbKb0I"},{"type":"link","url":"https://www.kaggle.com/c/dogs-vs-cats/data","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Cats vs Dogs","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"lkZEM13pz1"}],"urlSource":"https://www.kaggle.com/c/dogs-vs-cats/data","key":"W83HyBADHR"}],"key":"dHEg2Jpp8m"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Colored JPEG images, different sizes","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CgunpE6VLA"}],"key":"fZ1n6wky8k"}],"key":"DoHgyy4HYS"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Not nicely centered, translation invariance is important","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"Wx4PYhnN3Z"}],"key":"gN0JsFHBSz"}],"key":"pACKiNzErA"}],"key":"RVQFDaTHUx"}],"key":"IwJNw8Olcw"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Preprocessing","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AEffxVeu8Q"}],"key":"rT31CdGRBZ"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Decode JPEG images to floating-point tensors","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"vH1u0taAqt"}],"key":"otgJld5Cju"}],"key":"Pb6RAc7EEZ"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Rescale pixel values to [0,1]","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"cG59wfXB64"}],"key":"H1qpBPffR9"}],"key":"TlN7QHLLKB"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Resize images to 150x150 pixels","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"BWOcvFHsvd"}],"key":"eOHwaytp1o"}],"key":"DmhdsuDrUv"}],"key":"SxJKCI0JA6"}],"key":"AfWjmXT2h2"}],"key":"yJTO4qZlfI"}],"identifier":"lg9vevmhyidx","label":"LG9VevMHYidx","html_id":"lg9vevmhyidx","visibility":"show","key":"xvgQpWaa5B"},{"type":"block","kind":"notebook-content","data":{"id":"mmTTvevaYidy","slideshow":{"slide_type":"skip"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Uncomment to run from scratch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lkNV9cnPv7"}],"key":"MnNbSFqcTj"},{"type":"code","lang":"python","value":"# TODO: upload dataset to OpenML so we can avoid the manual steps.\n\nimport os, shutil \n# Download data from https://www.kaggle.com/c/dogs-vs-cats/data\n# Uncompress `train.zip` into the `original_dataset_dir`\noriginal_dataset_dir = '../data/cats-vs-dogs_original'\n\n# The directory where we will\n# store our smaller dataset\ntrain_dir = os.path.join(data_dir, 'train')\nvalidation_dir = os.path.join(data_dir, 'validation')\n\nif not os.path.exists(data_dir):\n    os.mkdir(data_dir)\n    os.mkdir(train_dir)\n    os.mkdir(validation_dir)\n    \ntrain_cats_dir = os.path.join(train_dir, 'cats')\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\n\nif not os.path.exists(train_cats_dir):\n    os.mkdir(train_cats_dir)\n    os.mkdir(train_dogs_dir)\n    os.mkdir(validation_cats_dir)\n    os.mkdir(validation_dogs_dir)\n\n# Copy first 2000 cat images to train_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_cats_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# Copy next 1000 cat images to validation_cats_dir\nfnames = ['cat.{}.jpg'.format(i) for i in range(2000, 3000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_cats_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# Copy first 2000 dog images to train_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(2000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_dogs_dir, fname)\n    shutil.copyfile(src, dst)\n    \n# Copy next 1000 dog images to validation_dogs_dir\nfnames = ['dog.{}.jpg'.format(i) for i in range(2000, 3000)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_dogs_dir, fname)\n    shutil.copyfile(src, dst)","position":{"start":{"line":3,"column":1},"end":{"line":59,"column":1}},"identifier":"mmttvevayidy-code","enumerator":"21","html_id":"mmttvevayidy-code","key":"VhS2ucSVLv"}],"identifier":"mmttvevayidy","label":"mmTTvevaYidy","html_id":"mmttvevayidy","visibility":"show","key":"oSfM1b7iEj"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"import random\n\n# Set random seed for reproducibility\ndef seed_everything(seed=42):\n    pl.seed_everything(seed)  # Sets seed for PyTorch Lightning\n    torch.manual_seed(seed)  # PyTorch\n    torch.cuda.manual_seed_all(seed)  # CUDA (if available)\n    np.random.seed(seed)  # NumPy\n    random.seed(seed)  # Python random module\n    torch.backends.cudnn.deterministic = True  # Ensures reproducibility in CNNs\n    torch.backends.cudnn.benchmark = False  # Ensures consistency\n\nseed_everything(42)  # Set global seed\n\nclass CatDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size=20, img_size=(150, 150)):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_size = img_size\n\n        # Define image transformations\n        self.transform = transforms.Compose([\n            transforms.Resize(self.img_size),  # Resize to 150x150\n            transforms.ToTensor(),  # Convert to tensor (also scales 0-1)\n        ])\n\n    def setup(self, stage=None):\n        \"\"\"Load datasets\"\"\"\n        train_dir = os.path.join(self.data_dir, \"train\")\n        val_dir = os.path.join(self.data_dir, \"validation\")\n\n        self.train_dataset = datasets.ImageFolder(root=train_dir, transform=self.transform)\n        self.val_dataset = datasets.ImageFolder(root=val_dir, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n\n# ----------------------------\n# Load dataset and visualize a batch\n# ----------------------------\ndata_module = CatDataModule(data_dir=data_dir)\ndata_module.setup()\ntrain_loader = data_module.train_dataloader()","key":"D9E3XfKqDU"},{"type":"outputs","id":"5fSMxhSrO1VJpjN9OfPjR","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"Seed set to 42\n"},"children":[],"key":"wGs5XmsWFA"}],"key":"vdVBUfuHie"}],"key":"biV8noYTAB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Get a batch of data\ndata_batch, labels_batch = next(iter(train_loader))\n\n# Visualize images\nplt.figure(figsize=(10, 5))\nfor i in range(7):\n    plt.subplot(1, 7, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(data_batch[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n    plt.title(\"Cat\" if labels_batch[i] == 0 else \"Dog\", fontsize=16)\nplt.tight_layout()\nplt.show()","key":"Hm6KvDJbwE"},{"type":"outputs","id":"nvwF6cQYUQdqQuUQW4Np2","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"7ac376351c4d5d1b40c06542fd631940","path":"/7ac376351c4d5d1b40c06542fd631940.json"},"image/png":{"content_type":"image/png","hash":"d58fb8011290a317fdba7ba61dbab6f2","path":"/d58fb8011290a317fdba7ba61dbab6f2.png"},"text/plain":{"content":"<Figure size 3000x1500 with 7 Axes>","content_type":"text/plain"}}},"children":[],"key":"UZelWP6DUX"}],"key":"HsKmNPIMtQ"}],"key":"ENGCWUNFvb"},{"type":"block","kind":"notebook-content","data":{"id":"AxG3qHwZYidy","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data loader","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bvqQCHxCXY"}],"identifier":"data-loader","label":"Data loader","html_id":"data-loader","implicit":true,"key":"rTS11GKVO0"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We create a Pytorch Lightning ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"m48endSdNC"},{"type":"inlineCode","value":"DataModule","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"cAomDxCo82"},{"type":"text","value":" to do preprocessing and data loading","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Ij9nuvS3Ts"}],"key":"yfn7xnRvmo"}],"key":"anIIE4aVVb"}],"key":"uGeZRyG6iP"},{"type":"code","lang":"python","value":"class ImageDataModule(pl.LightningDataModule):\n  def __init__(self, data_dir, batch_size=20, img_size=(150, 150)):\n    super().__init__()\n    self.transform = transforms.Compose([\n      transforms.Resize(self.img_size),  # Resize to 150x150\n      transforms.ToTensor()])  # Convert to tensor (also scales 0-1)\n  def setup(self, stage=None):\n    self.train_dataset = datasets.ImageFolder(root=train_dir, transform=self.transform)\n    self.val_dataset = datasets.ImageFolder(root=val_dir, transform=self.transform)","position":{"start":{"line":4,"column":1},"end":{"line":14,"column":1}},"identifier":"axg3qhwzyidy-code","enumerator":"22","html_id":"axg3qhwzyidy-code","key":"HBgKxyHlaV"},{"type":"code","lang":"python","value":"  def train_dataloader(self):\n    return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n  def val_dataloader(self):\n    return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)","position":{"start":{"line":15,"column":1},"end":{"line":20,"column":1}},"key":"d7CgSQSn8b"}],"identifier":"axg3qhwzyidy","label":"AxG3qHwZYidy","html_id":"axg3qhwzyidy","visibility":"show","key":"bvHseLqcIQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from torchmetrics.classification import Accuracy\n\n# Model in PyTorch Lightning\nclass CatImageClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Define convolutional layers\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(128, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.AdaptiveAvgPool2d(1)  # GAP replaces Flatten()\n        )\n\n        # Fully connected layers\n        self.fc_layers = nn.Sequential(\n            nn.Linear(128, 512),  # GAP outputs (batch, 128, 1, 1) â†’ Flatten to (batch, 128)\n            nn.ReLU(),\n            nn.Linear(512, 1)  # Binary classification (1 output neuron)\n        )\n\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = Accuracy(task=\"binary\")\n\n    def forward(self, x):\n        x = self.conv_layers(x)  # Convolutions + GAP\n        x = x.view(x.size(0), -1)  # Flatten from (batch, 128, 1, 1) â†’ (batch, 128)\n        x = self.fc_layers(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)  # Remove extra dimension\n        loss = self.loss_fn(logits, y.float())  # BCE loss requires float labels\n\n        preds = torch.sigmoid(logits)  # Convert logits to probabilities\n        acc = self.accuracy(preds, y)\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)\n        loss = self.loss_fn(logits, y.float())\n\n        preds = torch.sigmoid(logits)\n        acc = self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)","key":"KMU9OdAlot"},{"type":"outputs","id":"ZydiLj8minNu6-pPu_1Kq","children":[],"key":"rT7SwxROLL"}],"key":"ZI6ron7bqA"},{"type":"block","kind":"notebook-content","data":{"id":"vzNmmf-oYidy","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rJSEiEQKOU"}],"identifier":"model","label":"Model","html_id":"model","implicit":true,"key":"mwEUCO3oLl"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Since the images are more complex, we add another convolutional layer and increase the number of filters to 128.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ovnqxVxgzc"}],"key":"YNKepT2xLc"}],"identifier":"vznmmf-oyidy","label":"vzNmmf-oYidy","html_id":"vznmmf-oyidy","visibility":"show","key":"RwFtNfH3bm"},{"type":"block","kind":"notebook-code","data":{"hide_input":true,"id":"C2VPJ0GGYidy","slideshow":{"slide_type":"-"},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"model = CatImageClassifier()\nsummary(model, input_size=(1, 3, 150, 150))","identifier":"c2vpj0ggyidy-code","visibility":"hide","enumerator":"23","html_id":"c2vpj0ggyidy-code","key":"bhUmhprHN0"},{"type":"outputs","id":"3eEyYWMENdzlAkOoHvFjo","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":33,"metadata":{},"data":{"text/plain":{"content":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCatImageClassifier                       [1, 1]                    --\nâ”œâ”€Sequential: 1-1                        [1, 128, 1, 1]            --\nâ”‚    â””â”€Conv2d: 2-1                       [1, 32, 148, 148]         896\nâ”‚    â””â”€ReLU: 2-2                         [1, 32, 148, 148]         --\nâ”‚    â””â”€MaxPool2d: 2-3                    [1, 32, 74, 74]           --\nâ”‚    â””â”€Conv2d: 2-4                       [1, 64, 72, 72]           18,496\nâ”‚    â””â”€ReLU: 2-5                         [1, 64, 72, 72]           --\nâ”‚    â””â”€MaxPool2d: 2-6                    [1, 64, 36, 36]           --\nâ”‚    â””â”€Conv2d: 2-7                       [1, 128, 34, 34]          73,856\nâ”‚    â””â”€ReLU: 2-8                         [1, 128, 34, 34]          --\nâ”‚    â””â”€MaxPool2d: 2-9                    [1, 128, 17, 17]          --\nâ”‚    â””â”€Conv2d: 2-10                      [1, 128, 15, 15]          147,584\nâ”‚    â””â”€ReLU: 2-11                        [1, 128, 15, 15]          --\nâ”‚    â””â”€MaxPool2d: 2-12                   [1, 128, 7, 7]            --\nâ”‚    â””â”€AdaptiveAvgPool2d: 2-13           [1, 128, 1, 1]            --\nâ”œâ”€Sequential: 1-2                        [1, 1]                    --\nâ”‚    â””â”€Linear: 2-14                      [1, 512]                  66,048\nâ”‚    â””â”€ReLU: 2-15                        [1, 512]                  --\nâ”‚    â””â”€Linear: 2-16                      [1, 1]                    513\n==========================================================================================\nTotal params: 307,393\nTrainable params: 307,393\nNon-trainable params: 0\nTotal mult-adds (M): 234.16\n==========================================================================================\nInput size (MB): 0.27\nForward/backward pass size (MB): 9.68\nParams size (MB): 1.23\nEstimated Total Size (MB): 11.18\n==========================================================================================","content_type":"text/plain"}}},"children":[],"identifier":"c2vpj0ggyidy-outputs-0","html_id":"c2vpj0ggyidy-outputs-0","key":"YhOZNe9K0Z"}],"identifier":"c2vpj0ggyidy-outputs","visibility":"show","html_id":"c2vpj0ggyidy-outputs","key":"zWSDtejBVK"}],"identifier":"c2vpj0ggyidy","label":"C2VPJ0GGYidy","html_id":"c2vpj0ggyidy","visibility":"show","key":"w5fsopSCu3"},{"type":"block","kind":"notebook-content","data":{"id":"ztXKZpfXYidy","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Training","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XQku9rYuys"}],"identifier":"training","label":"Training","html_id":"training","implicit":true,"key":"aufJyPD6Hw"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We use a ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Dhw49SaXQd"},{"type":"inlineCode","value":"Trainer","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nF5NzU3YqP"},{"type":"text","value":" module (from PyTorch Lightning) to simplify training","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"o4CB7Mptzk"}],"key":"FTPDv7FKLn"}],"key":"YalgOynWRr"}],"key":"HH9tpUZ2fL"},{"type":"code","lang":"python","value":"trainer = pl.Trainer(\n    max_epochs=20,        # Train for 20 epochs\n    accelerator=\"gpu\",    # Move data and model to GPU\n    devices=\"auto\",       # Number of GPUs\n    deterministic=True,   # Set random seeds, for reproducibility\n    callbacks=[metric_tracker,      # Callback for logging loss and acc\n               checkpoint_callback] # Callback for logging weights\n)\ntrainer.fit(model, datamodule=data_module)","position":{"start":{"line":4,"column":1},"end":{"line":14,"column":1}},"identifier":"ztxkzpfxyidy-code","enumerator":"24","html_id":"ztxkzpfxyidy-code","key":"YSuS8gtW90"}],"identifier":"ztxkzpfxyidy","label":"ztXKZpfXYidy","html_id":"ztxkzpfxyidy","visibility":"show","key":"UvGV2OscAV"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Tip: to store the best model weights, you can add a ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BPkhjGGRkr"},{"type":"inlineCode","value":"ModelCheckpoint","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lNfir7pN0r"},{"type":"text","value":" callback","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"vAdY32OLxL"}],"key":"RiBAn92mVz"}],"key":"bS03SzYYac"}],"key":"Ml1ANjhsLr"},{"type":"code","lang":"python","value":"checkpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",   # Save model with lowest val. loss\n    mode=\"min\",           # \"min\" for loss, \"max\" for accuracy\n    save_top_k=1,         # Keep only the best model\n    dirpath=\"weights/\",   # Directory to save checkpoints\n    filename=\"cat_model\", # File name pattern\n)","position":{"start":{"line":3,"column":1},"end":{"line":11,"column":1}},"key":"vsl5IRRpwY"}],"key":"BKq9iJvVFD"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The model learns well for the first 20 epochs, but then starts overfitting a lot!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"LkCP48mffE"}],"key":"Byj5sT4W1T"}],"key":"LgqmIGDWcO"},{"type":"block","kind":"notebook-code","data":{"id":"n8mxwm43Yidy","slideshow":{"slide_type":"skip"},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"from pytorch_lightning.callbacks import ModelCheckpoint\n\n# Train Cat model\npl.seed_everything(42)  # Ensure reproducibility\ndata_module = CatDataModule(data_dir, batch_size=64)\nmodel = CatImageClassifier(learning_rate=0.001)\nmetric_tracker = MetricTracker()  # Callback to track per-epoch metrics\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n# Define checkpoint callback to save the best model\ncheckpoint_callback = ModelCheckpoint(\n    monitor=\"val_loss\",  # Saves model with lowest validation loss\n    mode=\"min\",  # \"min\" for loss, \"max\" for accuracy\n    save_top_k=1,  # Keep only the best model\n    dirpath=\"../data/checkpoints/\",  # Directory to save checkpoints\n    filename=\"cat_model\",  # File name pattern\n)\n\ntrainer = pl.Trainer(\n    max_epochs=50,  # Train for 20 epochs\n    accelerator=accelerator,\n    devices=\"auto\",\n    log_every_n_steps=10,\n    deterministic=True,\n    callbacks=[metric_tracker, checkpoint_callback]  # Attach callback to trainer\n)\n\nif histories and histories[\"cat\"]:\n    history_cat = histories[\"cat\"]\nelse:\n    trainer.fit(model, datamodule=data_module)\n    history_cat = metric_tracker.history","identifier":"n8mxwm43yidy-code","visibility":"hide","enumerator":"25","html_id":"n8mxwm43yidy-code","key":"VKf4xFf91d"},{"type":"outputs","id":"1ESmEtaf-tJk37yPL0Y8G","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"Seed set to 42\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n"},"children":[],"identifier":"n8mxwm43yidy-outputs-0","html_id":"n8mxwm43yidy-outputs-0","key":"mIvNctq4LH"}],"identifier":"n8mxwm43yidy-outputs","visibility":"show","html_id":"n8mxwm43yidy-outputs","key":"A8ZLnZue9k"}],"identifier":"n8mxwm43yidy","label":"n8mxwm43Yidy","html_id":"n8mxwm43yidy","visibility":"show","key":"z9mUGTnimQ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_training(history_cat)","key":"JcSsuhnjhz"},{"type":"outputs","id":"hi_by5OS3qYta-smnPd4c","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"bd2c7b3df4572a4f26d9112ea9eb235f","path":"/bd2c7b3df4572a4f26d9112ea9eb235f.json"},"image/png":{"content_type":"image/png","hash":"6df342355bf3c5d5ddd547701440d1cd","path":"/6df342355bf3c5d5ddd547701440d1cd.png"},"text/plain":{"content":"<Figure size 3600x1200 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"w8dYulbWAZ"}],"key":"izPxvfBcIe"}],"key":"WJag8BcRKs"},{"type":"block","kind":"notebook-content","data":{"id":"-jt2dfLfYidy","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Solving overfitting in CNNs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"HfirqfKpCc"}],"identifier":"solving-overfitting-in-cnns","label":"Solving overfitting in CNNs","html_id":"solving-overfitting-in-cnns","implicit":true,"key":"k26JKyR2pq"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"There are various ways to further improve the model:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GTYE9jHC18"}],"key":"toEi01Xl3D"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Generating more training data (data augmentation)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YsW0oeqP0C"}],"key":"O7UZG93I7e"}],"key":"H9rNfJJInV"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Regularization (e.g. Dropout, L1/L2, Batch Normalization,...)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"hljefKFww1"}],"key":"hcswQqEHhv"}],"key":"DtHaTb81DZ"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use pretrained rather than randomly initialized filters","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"FOQhrjT3I6"}],"key":"sGZhgGo8Ti"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"These are trained on a lot more data","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"NKkMTFXyIm"}],"key":"jBKUXyiPXP"}],"key":"yIQ6SfI7jK"}],"key":"v2Gy08Gews"}],"key":"ATV1uDWQbi"}],"key":"flaNZYJ2Yx"}],"key":"RmG9vZ17tH"}],"key":"xtvdZTBvns"}],"identifier":"-jt2dflfyidy","label":"-jt2dfLfYidy","html_id":"id-jt2dflfyidy","visibility":"show","key":"dD8lHV7T0x"},{"type":"block","kind":"notebook-content","data":{"id":"qaFft7RyYidy","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Data augmentation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PyRqNRvISO"}],"identifier":"data-augmentation","label":"Data augmentation","html_id":"data-augmentation","implicit":true,"key":"gv4zzodkbz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Generate new images via image transformations (only on training data!)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"NeTSAqk41a"}],"key":"BqCPNsldiX"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Images will be randomly transformed ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SV58MNXMkE"},{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"every epoch","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vPk3FYhL6F"}],"key":"Po7fLw5nDl"}],"key":"JTcGFWk2r8"}],"key":"Mf7Gqez6NF"}],"key":"BwJb0d5SSW"}],"key":"a1M8VpvOjb"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Update the transform in the data module","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"YTlCqiZRpJ"}],"key":"ZaZap9eTfs"}],"key":"MtweWaSMvM"}],"key":"WBmnOfxmp7"},{"type":"code","lang":"python","value":"self.train_transform = transforms.Compose([\n    transforms.Resize(self.img_size), # Resize to 150x150\n    transforms.RandomRotation(40),    # Rotations up to 40 degrees\n    transforms.RandomResizedCrop(self.img_size, \n                                 scale=(0.8, 1.2)), # Scale + crop, up to 20%\n    transforms.RandomHorizontalFlip(),              # Horizontal flip\n    transforms.RandomAffine(degrees=0, shear=20),   # Shear, up to 20%\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, \n                           saturation=0.2),         # Color jitter","position":{"start":{"line":6,"column":1},"end":{"line":16,"column":1}},"identifier":"qafft7ryyidy-code","enumerator":"26","html_id":"qafft7ryyidy-code","key":"DyW5Q0Yw9C"},{"type":"code","lang":"python","value":"    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n    ])","position":{"start":{"line":17,"column":1},"end":{"line":21,"column":1}},"key":"gCsHJoERdq"}],"identifier":"qafft7ryyidy","label":"qaFft7RyYidy","html_id":"qafft7ryyidy","visibility":"show","key":"ZUslMOqYd8"},{"type":"block","kind":"notebook-code","data":{"id":"pmYGo1pHYidy","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"class CatDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size=20, img_size=(150, 150)):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_size = img_size\n\n        # Training Data Augmentation \n        self.train_transform = transforms.Compose([\n            transforms.Resize(self.img_size),\n            transforms.RandomRotation(40),\n            transforms.RandomResizedCrop(self.img_size, scale=(0.8, 1.2)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomAffine(degrees=0, shear=20),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n        # Test Data Transforms (NO augmentation, just resize + normalize)\n        self.val_transform = transforms.Compose([\n            transforms.Resize(self.img_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n    def setup(self, stage=None):\n        \"\"\"Load datasets with correct transforms\"\"\"\n        train_dir = os.path.join(self.data_dir, \"train\")\n        val_dir = os.path.join(self.data_dir, \"validation\")\n\n        # Apply augmentation only to training data\n        self.train_dataset = datasets.ImageFolder(root=train_dir, transform=self.train_transform)\n        self.val_dataset = datasets.ImageFolder(root=val_dir, transform=self.val_transform)\n\n    def train_dataloader(self):\n        \"\"\"Applies augmentation via the pre-defined transform\"\"\"\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n\n    def val_dataloader(self):\n        \"\"\"Loads validation data WITHOUT augmentation\"\"\"\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)","identifier":"pmygo1phyidy-code","visibility":"hide","enumerator":"27","html_id":"pmygo1phyidy-code","key":"fWW8yBmIQ3"},{"type":"outputs","id":"KTONPYgMAVFNPyujvWCJc","children":[],"identifier":"pmygo1phyidy-outputs","visibility":"show","html_id":"pmygo1phyidy-outputs","key":"zyUfpnGVzR"}],"identifier":"pmygo1phyidy","label":"pmYGo1pHYidy","html_id":"pmygo1phyidy","visibility":"show","key":"wBAC46Bpr4"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Augmentation example","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tiRqx1k6Dc"}],"key":"tL3pwO6jur"}],"key":"QRWSLbpL3n"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def show_augmented_images(data_module, num_images=8):\n    \"\"\"Visualize the same image with different random augmentations.\"\"\"\n    \n    train_dataset = data_module.train_dataset  # Get training dataset with augmentation\n    \n    # Select a random image (without augmentation)\n    idx = np.random.randint(len(train_dataset))\n    original_img, label = train_dataset[idx]  # This is already augmented\n\n    # Convert original image back to NumPy format\n    original_img_np = original_img.permute(1, 2, 0).numpy()  # Convert (C, H, W) â†’ (H, W, C)\n    original_img_np = (original_img_np - original_img_np.min()) / (original_img_np.max() - original_img_np.min())  # Normalize\n\n    fig, axes = plt.subplots(2, 4, figsize=(10, 5))  # Create 4x2 grid\n    axes = axes.flatten()\n\n    for i in range(num_images):\n        # Apply new augmentation on the same image each time\n        img, _ = train_dataset[idx]  # Re-fetch the same image, but with a new random augmentation\n        \n        # Convert tensor image back to NumPy format\n        img = img.permute(1, 2, 0).numpy()  # Convert (C, H, W) â†’ (H, W, C)\n        img = (img - img.min()) / (img.max() - img.min())  # Normalize\n\n        # Plot the augmented image\n        axes[i].imshow(img)\n        axes[i].set_xticks([])\n        axes[i].set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n\n# Load dataset and visualize augmented images\ndata_module = CatDataModule(data_dir)  # Set correct dataset path\ndata_module.setup()\ncat_data_module = data_module\nshow_augmented_images(data_module)","key":"tka3Pdbcf2"},{"type":"outputs","id":"sOxVY5jZSdDypszDitxF9","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"6e8eb725c3b3ead6c95dda31adfc60df","path":"/6e8eb725c3b3ead6c95dda31adfc60df.json"},"image/png":{"content_type":"image/png","hash":"114d0bdbf6e64378d46c9186f4b6c10f","path":"/114d0bdbf6e64378d46c9186f4b6c10f.png"},"text/plain":{"content":"<Figure size 3000x1500 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"xnPd0qpcKC"}],"key":"DqXsQSPmvN"}],"key":"LafpXk5uW7"},{"type":"block","kind":"notebook-content","data":{"id":"lEciXicRYidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"We also add Dropout before the Dense layer, and L2 regularization (â€˜weight decayâ€™) in Adam","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZO0RbYstAD"}],"key":"JTxxRSRpy6"}],"identifier":"lecixicryidz","label":"lEciXicRYidz","html_id":"lecixicryidz","visibility":"show","key":"m0B8mtAHpn"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"class CatImageClassifier(pl.LightningModule):\n    def __init__(self, learning_rate=0.001):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Define convolutional layers (CNN)\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.Conv2d(128, 128, kernel_size=3, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n\n            nn.AdaptiveAvgPool2d(1)  # GAP instead of Flatten\n        )\n\n        # Fully connected layers (FC) with Dropout\n        self.fc_layers = nn.Sequential(\n            nn.Linear(128, 512),  # GAP outputs (batch, 128, 1, 1) â†’ Flatten to (batch, 128)\n            nn.ReLU(),\n            nn.Dropout(0.5),  # Dropout (same as Keras Dropout(0.5))\n            nn.Linear(512, 1)  # Binary classification (1 output neuron)\n        )\n\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = Accuracy(task=\"binary\")\n\n    def forward(self, x):\n        x = self.conv_layers(x)  # Convolutions + GAP\n        x = x.view(x.size(0), -1)  # Flatten from (batch, 128, 1, 1) â†’ (batch, 128)\n        x = self.fc_layers(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)  # Remove extra dimension\n        loss = self.loss_fn(logits, y.float())  # BCE loss requires float labels\n\n        preds = torch.sigmoid(logits)  # Convert logits to probabilities\n        acc = self.accuracy(preds, y)\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"train_acc\", acc, prog_bar=True, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x).squeeze(1)\n        loss = self.loss_fn(logits, y.float())\n\n        preds = torch.sigmoid(logits)\n        acc = self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-4)\n    \nmodel = CatImageClassifier()\nsummary(model, input_size=(1, 3, 150, 150))","key":"sdT92RLKyx"},{"type":"outputs","id":"VrZ7icuxyaracQc5uF3Xv","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":38,"metadata":{},"data":{"text/plain":{"content":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCatImageClassifier                       [1, 1]                    --\nâ”œâ”€Sequential: 1-1                        [1, 128, 1, 1]            --\nâ”‚    â””â”€Conv2d: 2-1                       [1, 32, 148, 148]         896\nâ”‚    â””â”€ReLU: 2-2                         [1, 32, 148, 148]         --\nâ”‚    â””â”€MaxPool2d: 2-3                    [1, 32, 74, 74]           --\nâ”‚    â””â”€Conv2d: 2-4                       [1, 64, 72, 72]           18,496\nâ”‚    â””â”€ReLU: 2-5                         [1, 64, 72, 72]           --\nâ”‚    â””â”€MaxPool2d: 2-6                    [1, 64, 36, 36]           --\nâ”‚    â””â”€Conv2d: 2-7                       [1, 128, 34, 34]          73,856\nâ”‚    â””â”€ReLU: 2-8                         [1, 128, 34, 34]          --\nâ”‚    â””â”€MaxPool2d: 2-9                    [1, 128, 17, 17]          --\nâ”‚    â””â”€Conv2d: 2-10                      [1, 128, 15, 15]          147,584\nâ”‚    â””â”€ReLU: 2-11                        [1, 128, 15, 15]          --\nâ”‚    â””â”€MaxPool2d: 2-12                   [1, 128, 7, 7]            --\nâ”‚    â””â”€AdaptiveAvgPool2d: 2-13           [1, 128, 1, 1]            --\nâ”œâ”€Sequential: 1-2                        [1, 1]                    --\nâ”‚    â””â”€Linear: 2-14                      [1, 512]                  66,048\nâ”‚    â””â”€ReLU: 2-15                        [1, 512]                  --\nâ”‚    â””â”€Dropout: 2-16                     [1, 512]                  --\nâ”‚    â””â”€Linear: 2-17                      [1, 1]                    513\n==========================================================================================\nTotal params: 307,393\nTrainable params: 307,393\nNon-trainable params: 0\nTotal mult-adds (M): 234.16\n==========================================================================================\nInput size (MB): 0.27\nForward/backward pass size (MB): 9.68\nParams size (MB): 1.23\nEstimated Total Size (MB): 11.18\n==========================================================================================","content_type":"text/plain"}}},"children":[],"key":"v8nwQCu69T"}],"key":"aGOSrG1K9T"}],"key":"Qs0nQHIbFq"},{"type":"block","kind":"notebook-content","data":{"id":"U7Vu-dtOYidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"No more overfitting!","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"faGC7Eixup"}],"key":"urFLZPUVOw"}],"identifier":"u7vu-dtoyidz","label":"U7Vu-dtOYidz","html_id":"u7vu-dtoyidz","visibility":"show","key":"jVNB5p87qu"},{"type":"block","kind":"notebook-code","data":{"id":"O_8mOXUUYidz","slideshow":{"slide_type":"skip"},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"pl.seed_everything(42)  # Ensure reproducibility\ndata_module = CatDataModule(data_dir, batch_size=64)\nmodel = CatImageClassifier(learning_rate=0.001)\nmetric_tracker = MetricTracker()  # Callback to track per-epoch metrics\n\ntrainer = pl.Trainer(\n    max_epochs=50,  # Train for 20 epochs\n    accelerator=accelerator,\n    devices=\"auto\",\n    log_every_n_steps=10,\n    deterministic=True,\n    callbacks=[metric_tracker, checkpoint_callback]  # Attach callback to trainer\n)\n\n# If previously trained, load history and weights\nif histories and histories[\"cat2\"]:\n    history_cat2 = histories[\"cat2\"]\n    model = CatImageClassifier.load_from_checkpoint(\"../data/checkpoints/cat_model.ckpt\")\nelse:\n    trainer.fit(model, datamodule=data_module)\n    history_cat2 = metric_tracker.history\n    \n# Set to evaluation mode so we don't update the weights\nmodel.eval()","identifier":"o_8moxuuyidz-code","visibility":"hide","enumerator":"28","html_id":"o-8moxuuyidz-code","key":"fcSatQbjjH"},{"type":"outputs","id":"Z4IdDJYRpNeDMAeiu_DzD","children":[{"type":"output","jupyter_data":{"name":"stderr","output_type":"stream","text":"Seed set to 42\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n"},"children":[],"identifier":"o_8moxuuyidz-outputs-0","html_id":"o-8moxuuyidz-outputs-0","key":"DEPHnJAj6Y"},{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":39,"metadata":{},"data":{"text/plain":{"content":"CatImageClassifier(\n  (conv_layers): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n    (10): ReLU()\n    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): AdaptiveAvgPool2d(output_size=1)\n  )\n  (fc_layers): Sequential(\n    (0): Linear(in_features=128, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=512, out_features=1, bias=True)\n  )\n  (loss_fn): BCEWithLogitsLoss()\n  (accuracy): BinaryAccuracy()\n)","content_type":"text/plain"}}},"children":[],"identifier":"o_8moxuuyidz-outputs-1","html_id":"o-8moxuuyidz-outputs-1","key":"dA8TyhsfB9"}],"identifier":"o_8moxuuyidz-outputs","visibility":"show","html_id":"o-8moxuuyidz-outputs","key":"NjKX8pi4J4"}],"identifier":"o_8moxuuyidz","label":"O_8mOXUUYidz","html_id":"o-8moxuuyidz","visibility":"show","key":"VjUrrTZFwC"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"history_cat2 = histories[\"cat2\"]\ncat_model = CatImageClassifier.load_from_checkpoint(\"../data/checkpoints/cat_model.ckpt\")","key":"lvpZLTuD3f"},{"type":"outputs","id":"2muO5PWMOpmMuuSV_IzuP","children":[],"key":"DDN0hHFdrS"}],"key":"WXUfHNadjw"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_training(history_cat2)","key":"AEKAMauTN9"},{"type":"outputs","id":"oKXLBp7TcN4he4vi4uEzb","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"ca1fb51f99192e38616b9e6eb462a748","path":"/ca1fb51f99192e38616b9e6eb462a748.json"},"image/png":{"content_type":"image/png","hash":"64db106974901fa9ccbe4ad62765bfe7","path":"/64db106974901fa9ccbe4ad62765bfe7.png"},"text/plain":{"content":"<Figure size 3600x1200 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"S9CnWdBqxP"}],"key":"oKhF2UTeRR"}],"key":"sRyptiH2jI"},{"type":"block","kind":"notebook-content","data":{"id":"V1r71cd1Yidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Real-world CNNs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DfrRscIaOd"}],"identifier":"real-world-cnns","label":"Real-world CNNs","html_id":"real-world-cnns","implicit":true,"key":"DIq8ZbczCF"},{"type":"heading","depth":3,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"VGG16","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"WFDhxm4RGh"}],"identifier":"vgg16","label":"VGG16","html_id":"vgg16","implicit":true,"key":"KycKqtZVT8"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper architecture (16 layers): allows it to learn more complex high-level features","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"VqX295ebyX"}],"key":"RpDpJqiZMP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Textures, patterns, shapes,...","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"tAm42K3Nsk"}],"key":"XmTJDpJWze"}],"key":"XA2NTmYqaj"}],"key":"fPqiXoINne"}],"key":"edTqBsaoeW"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Small filters (3x3) work better: capture spatial information while reducing number of parameters","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ZT5tiR51P1"}],"key":"O64sGB0fSL"}],"key":"rYh3Zj71sR"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Max-pooling (2x2): reduces spatial dimension, improves translation invariance","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"uUHwi6w1Gv"}],"key":"G36FvHlzNE"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Lower resolution forces model to learn robust features (less sensitive to small input changes)","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"UOfwWf45zy"}],"key":"nWgiQrWlhy"}],"key":"UxLNTRYeJz"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Only after every 2 layers, otherwise dimensions reduce too fast","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"dc2JxAy942"}],"key":"WAT5vCqPOC"}],"key":"bathURutxO"}],"key":"NqMRXgtWLQ"}],"key":"LesHhZtVcm"},{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Downside: too many parameters, expensive to train","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"j7Kw0rRG0M"}],"key":"HDcF8VYjQl"}],"key":"cFmrKWVfsv"}],"key":"CmcrKreLJf"},{"type":"image","style":{"width":"70%","marginLeft":"auto","marginRight":"auto"},"url":"/922ede31b9064082591035652deb4cdd.png","alt":"ml","key":"wZqKfEV23i","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/09_vgg16.png?raw=1","urlOptimized":"/922ede31b9064082591035652deb4cdd.webp"}],"identifier":"v1r71cd1yidz","label":"V1r71cd1Yidz","html_id":"v1r71cd1yidz","visibility":"show","key":"BszMD9immO"},{"type":"block","kind":"notebook-content","data":{"id":"V1r71cd1Yidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Inceptionv3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lM4B1l3Pyu"}],"identifier":"inceptionv3","label":"Inceptionv3","html_id":"inceptionv3","implicit":true,"key":"Hb7a0DsnOl"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Inception modules: parallel branches learn features of different sizes and scales (3x3, 5x5, 7x7,...)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"EhJ5WPwz9u"}],"key":"ZlN8Lyotii"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Add reduction blocks that reduce dimensionality via convolutions with stride 2","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QOpzUP3Lkt"}],"key":"VJ4re1p6RW"}],"key":"HPdsmbOqyN"}],"key":"z0KxWG2Jcy"}],"key":"qngP1DUYwo"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Factorized convolutions: a 3x3 conv. can be replaced by combining 1x3 and 3x1, and is 33% cheaper","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"JtrPNwnEph"}],"key":"wvEiWn21hh"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"A 5x5 can be replaced by combining 3x3 and 3x3, which can in turn be factorized as above","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pSYMYMFoOA"}],"key":"TLF9h3I1P2"}],"key":"vfOiRf5U54"}],"key":"VmxylM9w8a"}],"key":"sejK0ovyOT"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"1x1 convolutions, or Network-In-Network (NIN) layers help reduce the number of channels: cheaper","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"y0hq5ILdlc"}],"key":"RhNR7xT5Wr"}],"key":"hbYQYZYXzt"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"An auxiliary classifier adds an additional gradient signal deeper in the network","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"LVq46hZirE"}],"key":"qcL1fOVZZh"}],"key":"qQyKTFWr2Z"}],"key":"wrkB01aF00"},{"type":"image","style":{"width":"80%","marginLeft":"auto","marginRight":"auto"},"url":"/f77332ce5a9ca5439a1668b734b9f97d.png","alt":"ml","key":"AyGKZdGk4H","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/09_inception.png?raw=1","urlOptimized":"/f77332ce5a9ca5439a1668b734b9f97d.webp"}],"identifier":"v1r71cd1yidz","label":"V1r71cd1Yidz","html_id":"v1r71cd1yidz-1","visibility":"show","key":"yC2Jgimi2c"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Factorized convolutions","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"EAPp7Dio82"}],"identifier":"factorized-convolutions","label":"Factorized convolutions","html_id":"factorized-convolutions","implicit":true,"key":"XC83EmCtTO"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"A 3x3 conv. can be replaced by combining 1x3 and 3x1, and is 33% cheaper","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"XRJBBaVDUw"}],"key":"HPavhxNEU0"}],"key":"jr1SLmzw4x"}],"key":"ccI0zf34Lm"},{"type":"image","style":{"width":"80%","marginLeft":"auto","marginRight":"auto"},"url":"/8c6de2343b629e63e81a64ee7a3906eb.jpeg","alt":"ml","key":"JGkQ1TAuF2","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/factorized_convolutions.jpg?raw=1","urlOptimized":"/8c6de2343b629e63e81a64ee7a3906eb.webp"}],"key":"FCiiUnp1cX"},{"type":"block","kind":"notebook-content","data":{"id":"V1r71cd1Yidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"ResNet50","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZLFfd7KQ8n"}],"identifier":"resnet50","label":"ResNet50","html_id":"resnet50","implicit":true,"key":"p3DfTdlhL5"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Residual (skip) connections: add earlier feature map to a later one (dimensions must match)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ecpuGJJn3Q"}],"key":"XbZ0lf2MIX"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Information can bypass layers, reduces vanishing gradients, allows much deeper nets","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KwZpJz7bZw"}],"key":"GNdKtsRWH6"}],"key":"VGfRM0DJu6"}],"key":"jfTOwC75Kw"}],"key":"R4qVRfK5pV"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Residual blocks: skip small number or layers and repeat many times","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"XgAEg8QBZN"}],"key":"Azr7bEzMU1"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Match dimensions though padding and 1x1 convolutions","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"fPdd92TMqM"}],"key":"AjJbaLAcQ8"}],"key":"lq67fxcdvs"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"When resolution drops, add 1x1 convolutions with stride 2","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"lm55k9pKLd"}],"key":"k5iIWVRxPn"}],"key":"roRtHB3iNd"}],"key":"MRVjdHA1Tt"}],"key":"IJWcx1eNJ6"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Can be combined with Inception blocks","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Pqguw0tj1P"}],"key":"PYsAlhLPdf"}],"key":"lW8WC3m0FU"}],"key":"SaKCvQHFNx"},{"type":"image","style":{"width":"90%","marginLeft":"auto","marginRight":"auto"},"url":"/a0f1c2817a7746da0b7cf01c515c231c.png","alt":"ml","key":"ATaMQWsw5T","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/09_resnet50.png?raw=1","urlOptimized":"/a0f1c2817a7746da0b7cf01c515c231c.webp"}],"identifier":"v1r71cd1yidz","label":"V1r71cd1Yidz","html_id":"v1r71cd1yidz-2","visibility":"show","key":"vWbOWqsp8Y"},{"type":"block","kind":"notebook-content","data":{"id":"V1r71cd1Yidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Interpreting the model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"SXqOlhLu6l"}],"identifier":"interpreting-the-model","label":"Interpreting the model","html_id":"interpreting-the-model","implicit":true,"key":"QirmMZnxxh"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Letâ€™s see what the convnet is learning exactly by observing the intermediate feature maps","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Ru75wMnMHk"}],"key":"RfzR35emdO"}],"key":"i2GAeU6cmn"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We can do this easily by attaching a â€˜hookâ€™ to a layer so we can read itâ€™s output (activation)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"fw2LmLR86q"}],"key":"Ng25TJtxEv"}],"key":"PZpG9zpXxr"}],"key":"FOC83gUiTj"},{"type":"code","lang":"python","value":"# Create a hook to send outputs to a global variable (activation)\ndef hook_fn(module, input, output): \n    nonlocal activation\n    activation = output.detach()\n    \n# Add a hook to a specific layer\nhook = model.features[layer_id].register_forward_hook(hook_fn)","position":{"start":{"line":5,"column":1},"end":{"line":13,"column":1}},"identifier":"v1r71cd1yidz-code","enumerator":"29","html_id":"v1r71cd1yidz-code","key":"XLDcNDoqFH"},{"type":"code","lang":"python","value":"# Do a forward pass without gradient computation\nwith torch.no_grad(): \n    model(image_tensor) \n\n# Access the global variable\nreturn activation","position":{"start":{"line":14,"column":1},"end":{"line":21,"column":1}},"key":"wRHdZHVgUE"}],"identifier":"v1r71cd1yidz","label":"V1r71cd1Yidz","html_id":"v1r71cd1yidz-3","visibility":"show","key":"OLz8ailmEV"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Result for a specific filter (Layer 0, Filter 0)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mk619vzcjg"}],"key":"hVzd7v5Kqy"}],"key":"UukwNagpnu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from PIL import Image\nimport os\n\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\ndef load_image(img_path, img_size=(150, 150)):\n    \"\"\"Load and preprocess image as a PyTorch tensor.\"\"\"\n    transform = transforms.Compose([\n        transforms.Resize(img_size),\n        transforms.ToTensor(),  # Converts image to tensor with values in [0,1]\n    ])\n    \n    img = Image.open(img_path).convert(\"RGB\")  # Ensure RGB format\n    img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n    return img_tensor\n\ndef get_layer_activations(model, img_tensor, layer_idx=0, keep_gradients=False):\n    \"\"\"Extract activations from a specific layer.\"\"\"\n    activation = None\n    \n    def hook_fn(module, input, output):\n        nonlocal activation\n        if keep_gradients: # Only for gradient ascent (later)\n            activation = output\n        else:\n            activation = output.detach()\n\n    # Register hook to capture the activation\n    # Handles our custom model and more general models like VGG\n    layer = model.conv_layers[layer_idx] if hasattr(model, \"conv_layers\") else model[layer_idx]\n    hook = layer.register_forward_hook(hook_fn)    \n    \n    if keep_gradients:\n        model(img_tensor)  # Run the image through the model\n    else:\n        with torch.no_grad():\n            model(img_tensor)  # Idem but no grad\n    \n    hook.remove()  # Remove the hook after getting activations\n    return activation\n\ndef visualize_activations(model, img_tensor, layer_idx=0, filter_idx=0):\n    \"\"\"Visualize input image and activations of a selected filter.\"\"\"\n\n    # Get activations from the specified layer\n    activations = get_layer_activations(model, img_tensor, layer_idx)\n\n    # Convert activations to numpy for visualization\n    activation_np = activations.squeeze(0).cpu().numpy()  # Remove batch dim\n    \n    # Show input image\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(4, 2))\n    \n    # Convert input tensor to NumPy\n    img_np = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()  # (H, W, C)\n    img_np = np.clip(img_np, 0, 1)  # Ensure values are in range [0,1]\n    \n    ax1.imshow(img_np)\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n    ax1.set_xlabel(\"Input Image\", fontsize=8)\n    \n    # Visualize a specific filter's activation\n    ax2.imshow(activation_np[filter_idx], cmap=\"viridis\")\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    ax2.set_xlabel(f\"Activation of Filter {filter_idx}\", fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()","key":"LbG0jI1R1Y"},{"type":"outputs","id":"HzC1mZxDUyN1GbSh4O_7C","children":[],"key":"mp71H7Iqau"}],"key":"EFTIfjCZu5"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Load model and visualize activations\nimg_path = os.path.join(data_dir, \"train/cats/cat.1700.jpg\")  # Update path\nimg_tensor = load_image(img_path).to(accelerator)\n\nvisualize_activations(cat_model, img_tensor, layer_idx=0, filter_idx=0)","key":"FJs7ERHt19"},{"type":"outputs","id":"5-nTaRvgHgIVwYNI_8qCt","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"286aa5d0761c98cd4e53135119597a36","path":"/286aa5d0761c98cd4e53135119597a36.json"},"image/png":{"content_type":"image/png","hash":"e177828577040927cf24a4aec8c0e6c0","path":"/e177828577040927cf24a4aec8c0e6c0.png"},"text/plain":{"content":"<Figure size 1200x600 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"kUhGe9LuWv"}],"key":"Z0YPkBHJHS"}],"key":"IhgMctB4H5"},{"type":"block","kind":"notebook-content","data":{"id":"ZnfFIOe4Yidz","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"The same filter will highlight the same patterns in other inputs.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Jmfuz92FhZ"}],"key":"f4SfgyIyaY"}],"identifier":"znffioe4yidz","label":"ZnfFIOe4Yidz","html_id":"znffioe4yidz","visibility":"show","key":"QvT6MXpKhi"},{"type":"block","kind":"notebook-code","data":{"id":"-rge-b3xYidz","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"img_path_dog = os.path.join(data_dir, \"train/dogs/dog.1528.jpg\")\nimg_tensor_dog = load_image(img_path_dog).to(accelerator)\n\nvisualize_activations(cat_model, img_tensor_dog, layer_idx=0, filter_idx=0)","identifier":"-rge-b3xyidz-code","visibility":"hide","enumerator":"30","html_id":"id-rge-b3xyidz-code","key":"S5zFfEgtro"},{"type":"outputs","id":"4dfox3FvZUhBYziJxlRDp","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"0965306270062272ade3f337c9b2c852","path":"/0965306270062272ade3f337c9b2c852.json"},"image/png":{"content_type":"image/png","hash":"6dd430e76d3f624821b7ae6c7f63922b","path":"/6dd430e76d3f624821b7ae6c7f63922b.png"},"text/plain":{"content":"<Figure size 1200x600 with 2 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"-rge-b3xyidz-outputs-0","html_id":"id-rge-b3xyidz-outputs-0","key":"nzR22io9RZ"}],"identifier":"-rge-b3xyidz-outputs","visibility":"show","html_id":"id-rge-b3xyidz-outputs","key":"UfHL6FpmfV"}],"identifier":"-rge-b3xyidz","label":"-rge-b3xYidz","html_id":"id-rge-b3xyidz","visibility":"show","key":"mFr1aBqvsC"},{"type":"block","kind":"notebook-code","data":{"id":"bOrFvcNaYid0","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"def visualize_all_filters(model, img_tensor, layer_idx=0, max_per_row=16):\n    \"\"\"Visualize all filters of a given layer as a grid of feature maps.\"\"\"\n    activations = get_layer_activations(model, img_tensor, layer_idx)\n    activation_np = activations.squeeze(0).cpu().numpy()\n    \n    num_filters = activation_np.shape[0]\n    num_cols = min(num_filters, max_per_row)\n    num_rows = (num_filters + num_cols - 1) // num_cols  # Ceiling division\n    \n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols, num_rows))\n    axes = np.array(axes).reshape(num_rows, num_cols)  # Ensure it's a 2D array\n    \n    for i in range(num_rows * num_cols):\n        ax = axes[i // num_cols, i % num_cols]\n        \n        if i < num_filters:\n            ax.imshow(activation_np[i], cmap=\"viridis\")\n        \n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    plt.suptitle(f\"Activations of Layer {layer_idx}\", fontsize=16, y=1.0)\n    plt.tight_layout()\n    plt.show()","identifier":"borfvcnayid0-code","visibility":"hide","enumerator":"31","html_id":"borfvcnayid0-code","key":"PPSermRNWS"},{"type":"outputs","id":"5kJQDXaHz-S4hoAoNZZcS","children":[],"identifier":"borfvcnayid0-outputs","visibility":"show","html_id":"borfvcnayid0-outputs","key":"fQ9c4tSdet"}],"identifier":"borfvcnayid0","label":"bOrFvcNaYid0","html_id":"borfvcnayid0","visibility":"show","key":"Isn0hswke6"},{"type":"block","kind":"notebook-content","data":{"id":"n5l_5tKAYid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"All filters for the first 2 convolutional layers: edges, colors, simple shapes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"u2jWe1sGAJ"}],"key":"NNYzbYZ6bN"}],"key":"ZPiGKzLKrh"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Empty filter activations occur:","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Nmaw8OLw6w"}],"key":"PRM4TddJSj"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Filter is not interested in that input image (maybe itâ€™s dog-specific)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xP4YFN3Jfr"}],"key":"xlzYdLj3DT"}],"key":"ywDXZUgUhV"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Incomplete training, Dying ReLU,...","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"cb21fHTztQ"}],"key":"PkPfEWMxL6"}],"key":"nJWbcgPuIK"}],"key":"mCex4gDYX7"}],"key":"py7ce83p9R"}],"key":"tHQd5vh3sv"}],"identifier":"n5l_5tkayid0","label":"n5l_5tKAYid0","html_id":"n5l-5tkayid0","visibility":"show","key":"BLOFfrCUI4"},{"type":"block","kind":"notebook-code","data":{"hide_input":false,"id":"KRMhlJq9Yid0","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"visualize_all_filters(cat_model, img_tensor, layer_idx=0)\nvisualize_all_filters(cat_model, img_tensor, layer_idx=3)","identifier":"krmhljq9yid0-code","visibility":"hide","enumerator":"32","html_id":"krmhljq9yid0-code","key":"nNnjjVq2Rw"},{"type":"outputs","id":"a3F0W-6Fth6XhdabTUwYr","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"418b9aff7e1f4d3fe28bee06b36fc4ae","path":"/418b9aff7e1f4d3fe28bee06b36fc4ae.json"},"image/png":{"content_type":"image/png","hash":"14b3145f49c33cb56538379f782ce3dd","path":"/14b3145f49c33cb56538379f782ce3dd.png"},"text/plain":{"content":"<Figure size 4800x600 with 32 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"krmhljq9yid0-outputs-0","html_id":"krmhljq9yid0-outputs-0","key":"sbC1axyK4w"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"96ec7942f756857af677bedd6e354c5c","path":"/96ec7942f756857af677bedd6e354c5c.json"},"image/png":{"content_type":"image/png","hash":"3340e6dfb56928a9f6b72d85bd32b0a4","path":"/3340e6dfb56928a9f6b72d85bd32b0a4.png"},"text/plain":{"content":"<Figure size 4800x1200 with 64 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"krmhljq9yid0-outputs-1","html_id":"krmhljq9yid0-outputs-1","key":"ZyFeS9MVaA"}],"identifier":"krmhljq9yid0-outputs","visibility":"show","html_id":"krmhljq9yid0-outputs","key":"TOhMTu4p3s"}],"identifier":"krmhljq9yid0","label":"KRMhlJq9Yid0","html_id":"krmhljq9yid0","visibility":"show","key":"f06HkCpFQj"},{"type":"block","kind":"notebook-content","data":{"id":"TNcHZCb5Yid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"3rd convolutional layer: increasingly abstract: ears, nose, eyes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Hf7UF9DNwo"}],"key":"My5f67bNnd"}],"key":"AtaShvjm2g"}],"key":"rn9gx0wfzq"}],"identifier":"tnchzcb5yid0","label":"TNcHZCb5Yid0","html_id":"tnchzcb5yid0","visibility":"show","key":"nIxpzIleSM"},{"type":"block","kind":"notebook-code","data":{"hide_input":false,"id":"-TLo8KGCYid0","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"visualize_all_filters(cat_model, img_tensor, layer_idx=6)","identifier":"-tlo8kgcyid0-code","visibility":"hide","enumerator":"33","html_id":"id-tlo8kgcyid0-code","key":"mbyzKA56fF"},{"type":"outputs","id":"xS9pWyOBRsl-db9ZyU39o","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"73f4ad12d21b5fee31b0c06a531a84da","path":"/73f4ad12d21b5fee31b0c06a531a84da.json"},"image/png":{"content_type":"image/png","hash":"946b232ff2e7fbedd1ca20a2660bbb74","path":"/946b232ff2e7fbedd1ca20a2660bbb74.png"},"text/plain":{"content":"<Figure size 4800x2400 with 128 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"-tlo8kgcyid0-outputs-0","html_id":"id-tlo8kgcyid0-outputs-0","key":"j9Mg7sL6Hy"}],"identifier":"-tlo8kgcyid0-outputs","visibility":"show","html_id":"id-tlo8kgcyid0-outputs","key":"ncqoam1xg7"}],"identifier":"-tlo8kgcyid0","label":"-TLo8KGCYid0","html_id":"id-tlo8kgcyid0","visibility":"show","key":"bJzuI1rZCs"},{"type":"block","kind":"notebook-content","data":{"hide_input":false,"id":"nKTu3_vEYid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Last convolutional layer: more abstract patterns","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eD6mu3uDKb"}],"key":"AemOgsBTA5"}],"key":"eMdQimTinj"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Each filter combines information from all filters in previous layer","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"CAGAQTQdtk"}],"key":"o6qBWTllYc"}],"key":"Ogozffti5M"}],"key":"C7gj7OcNPB"}],"identifier":"nktu3_veyid0","label":"nKTu3_vEYid0","html_id":"nktu3-veyid0","visibility":"show","key":"U50kFqBguO"},{"type":"block","kind":"notebook-code","data":{"hide_input":false,"id":"mvywAcb-Yid0","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"visualize_all_filters(cat_model, img_tensor, layer_idx=9)","identifier":"mvywacb-yid0-code","visibility":"hide","enumerator":"34","html_id":"mvywacb-yid0-code","key":"IMm2boBWxr"},{"type":"outputs","id":"mNZstYMNtukTjdEi4_uW3","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"3943546bfdac27dfc780e53af2ab6caa","path":"/3943546bfdac27dfc780e53af2ab6caa.json"},"image/png":{"content_type":"image/png","hash":"b4d188d6c5e5ef1e02e3040b9f5443c5","path":"/b4d188d6c5e5ef1e02e3040b9f5443c5.png"},"text/plain":{"content":"<Figure size 4800x2400 with 128 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"mvywacb-yid0-outputs-0","html_id":"mvywacb-yid0-outputs-0","key":"p0R1gdNJKF"}],"identifier":"mvywacb-yid0-outputs","visibility":"show","html_id":"mvywacb-yid0-outputs","key":"Jb9pT7OBDx"}],"identifier":"mvywacb-yid0","label":"mvywAcb-Yid0","html_id":"mvywacb-yid0","visibility":"show","key":"MWJVtnhYi2"},{"type":"block","kind":"notebook-content","data":{"id":"EqKCdzHFYid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Same layer, with dog image input: some filters react only to dogs or cats","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pthP6ipyoE"}],"key":"MZF6USjpX8"}],"key":"TyiglR9TeS"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper layers learn representations that separate the classes","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"OPqGqHUMKd"}],"key":"VpHsiduHNY"}],"key":"PxZv2YkGvk"}],"key":"cZ48Oiw3wg"}],"identifier":"eqkcdzhfyid0","label":"EqKCdzHFYid0","html_id":"eqkcdzhfyid0","visibility":"show","key":"TkDvPn5bwZ"},{"type":"block","kind":"notebook-code","data":{"id":"lWHbeZdvYid0","tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"visualize_all_filters(cat_model, img_tensor_dog, layer_idx=9)","identifier":"lwhbezdvyid0-code","visibility":"hide","enumerator":"35","html_id":"lwhbezdvyid0-code","key":"eJyhJ03DWD"},{"type":"outputs","id":"6VugwAJolsQr-Y6y36bIQ","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"78e276117715297e29ab21da4e0025fe","path":"/78e276117715297e29ab21da4e0025fe.json"},"image/png":{"content_type":"image/png","hash":"634df8db8b7a1dc1911df81ead534de8","path":"/634df8db8b7a1dc1911df81ead534de8.png"},"text/plain":{"content":"<Figure size 4800x2400 with 128 Axes>","content_type":"text/plain"}}},"children":[],"identifier":"lwhbezdvyid0-outputs-0","html_id":"lwhbezdvyid0-outputs-0","key":"sXs6WzdVqL"}],"identifier":"lwhbezdvyid0-outputs","visibility":"show","html_id":"lwhbezdvyid0-outputs","key":"WIND7ZaQw3"}],"identifier":"lwhbezdvyid0","label":"lWHbeZdvYid0","html_id":"lwhbezdvyid0","visibility":"show","key":"HsjR2kfnZs"},{"type":"block","kind":"notebook-content","data":{"id":"Dt2x7EodYid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Spatial hierarchies","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gyS3vQz4ew"}],"identifier":"spatial-hierarchies","label":"Spatial hierarchies","html_id":"spatial-hierarchies","implicit":true,"key":"Rt9LtLEXiP"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deep convnets can learn ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nzWd6w6XDp"},{"type":"emphasis","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"spatial hierarchies","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kv7Tongm8K"}],"key":"DAAQg2dh8S"},{"type":"text","value":" of patterns","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"xodcCEah73"}],"key":"sVtKfysAWg"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"First layer can learn very local patterns (e.g. edges)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"xI7U3J1ze3"}],"key":"QySWjPbTsA"}],"key":"MDDUM4owtC"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Second layer can learn specific combinations of patterns","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"m1hYvXSS0l"}],"key":"L3GHuSfaMA"}],"key":"AKz6SlDHBF"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Every layer can learn increasingly complex ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"OfDckAqFeQ"},{"type":"emphasis","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"abstractions","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"sV9KIgTb8m"}],"key":"u5cmSucvLV"}],"key":"t0raFvbKP4"}],"key":"nGreFg9kig"}],"key":"QECuvnpUqw"}],"key":"PJCIJPB3gl"}],"key":"pv0z6C1zpp"},{"type":"image","style":{"width":"500px","marginLeft":"auto","marginRight":"auto"},"url":"/c4a047064311d2d3ef0fe35ae479458e.png","alt":"ml","key":"gLYR5NhpQl","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/10_hierarchy.png?raw=1","urlOptimized":"/c4a047064311d2d3ef0fe35ae479458e.webp"}],"identifier":"dt2x7eodyid0","label":"Dt2x7EodYid0","html_id":"dt2x7eodyid0","visibility":"show","key":"jUhqjp9v6Q"},{"type":"block","kind":"notebook-content","data":{"id":"34QNNpdxYid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Visualizing the learned filters","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KORXoEzHWH"}],"identifier":"visualizing-the-learned-filters","label":"Visualizing the learned filters","html_id":"visualizing-the-learned-filters","implicit":true,"key":"mWzP8mkHML"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Visualize filters by finding the input image that they are maximally responsive to","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"yMwiKjdCIK"}],"key":"sgIOzph8Ra"}],"key":"J6byCg4XkU"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Gradient ascent in input space","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"wJVeL6Wh0E"}],"key":"ob0VIqxQAW"},{"type":"text","value":": learn what input maximizes the activations for that filter","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"GAYTDg5PAs"}],"key":"FjLlqTm1xb"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Start from a random input image ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"b8dwf4Moz7"},{"type":"inlineMath","value":"X","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>","key":"fGHEpZsHdK"},{"type":"text","value":", freeze the kernel","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"LWvQFAKao0"}],"key":"WmwxvSF3zD"}],"key":"XnLpnn7t2W"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Loss = mean activation of output layer A, backpropagate to optimize ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ci1HG0KUvV"},{"type":"inlineMath","value":"X","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>","key":"yQsuwWMMnZ"}],"key":"trq9Tra3h8"}],"key":"poY2PRWtXf"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineMath","value":"X_{(i+1)} = X_{(i)} + \\frac{\\partial L(x, X_{(i)})}{\\partial X} * \\eta","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msub><mo>=</mo><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo>+</mo><mfrac><mrow><mi mathvariant=\"normal\">âˆ‚</mi><mi>L</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><msub><mi>X</mi><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">)</mo></mrow><mrow><mi mathvariant=\"normal\">âˆ‚</mi><mi>X</mi></mrow></mfrac><mo>âˆ—</mo><mi>Î·</mi></mrow><annotation encoding=\"application/x-tex\">X_{(i+1)} = X_{(i)} + \\frac{\\partial L(x, X_{(i)})}{\\partial X} * \\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0385em;vertical-align:-0.3552em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0385em;vertical-align:-0.3552em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.0785em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.4386em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0936em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\" style=\"margin-right:0.05556em;\">âˆ‚</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.5686em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\" style=\"margin-right:0.05556em;\">âˆ‚</span><span class=\"mord mathnormal mtight\">L</span><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">x</span><span class=\"mpunct mtight\">,</span><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07847em;\">X</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.3448em;margin-left:-0.0785em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5357em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">i</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3695em;\"><span></span></span></span></span></span></span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">â€‹</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">âˆ—</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">Î·</span></span></span></span>","key":"lxBB5TmPlh"}],"key":"EV7n6DSx0A"}],"key":"q3rfOn6SG0"}],"key":"mT0rubeNwl"}],"key":"xEKU2zzy4e"}],"key":"H2Mh46eEyg"}],"identifier":"34qnnpdxyid0","label":"34QNNpdxYid0","html_id":"id-34qnnpdxyid0","visibility":"show","key":"NL7x6nPaa9"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"-"}},"children":[{"type":"code","lang":"python","executable":true,"value":"from scipy.signal import convolve2d\n\nimport random\n\n# Function to generate green color shades\ndef generate_shades(size, randomness, brightness, striped=False):\n    matrix = np.zeros((size, size))\n    for i in range(size):\n        for j in range(size):\n            base_shade = max(0, min(1, brightness + randomness * random.uniform(-1, 1)))\n            if striped and i % 2 == 0:\n                matrix[i, j] = base_shade  # Brighter green for even rows\n            else:\n                matrix[i, j] = 0.5 * base_shade  # Dimmer green or normal shade\n                \n    return matrix\n\n# Function to highlight regions with green shades\ndef highlight_region_matrix(ax, cells, offset, color_matrix):\n    for (x, y) in cells:\n        color_value = color_matrix[y, x]  # Get green intensity value\n        color = (0, color_value, 0)  # Convert to RGB (only green channel)\n        ax.add_patch(plt.Rectangle((offset[0] + x, offset[1] + y), 1, 1, color=color, ec='black', lw=0.5))\n\n@interact\ndef visualize_gradient_ascent(step=(1, 100, 1)):\n    fig, ax = plt.subplots(figsize=(18, 6))\n    ax.set_xlim(-2, 26)\n    ax.set_ylim(-9, 2)\n    ax.axis('off')\n\n    # Define grid sizes and positions\n    grids = [(6, (0, 0)), (3, (9, 0)), (4, (15, 0))]\n\n    # Adjust randomness and brightness based on step\n    input_randomness = 1 / step\n    input_brightness = 0.5 + 0.5 * (step / 100)\n\n    # Generate color matrices (single green intensity values)\n    input_colors = generate_shades(6, input_randomness, input_brightness, striped=True)\n    kernel_colors = np.array([[0.25, 0.5, 0.25],[0.5, 1.0, 0.5],[0.25, 0.5, 0.25]])  \n    kernel_colors = kernel_colors / np.sum(kernel_colors)\n    output_colors = convolve2d(input_colors, kernel_colors,  mode='valid') # Convolution\n    kernel_colors = (kernel_colors - kernel_colors.min()) / (kernel_colors.max() - kernel_colors.min())\n\n    # Draw grids\n    for size, offset in grids:\n        draw_grid(ax, size, offset)\n\n    # Highlight regions with shades\n    highlight_region_matrix(ax, [(x, y) for x in range(6) for y in range(6)], (0, -5), input_colors)\n    highlight_region_matrix(ax, [(x, y) for x in range(3) for y in range(3)], (9, -2), kernel_colors)\n    highlight_region_matrix(ax, [(x, y) for x in range(4) for y in range(4)], (15, -3), output_colors)\n\n    # Titles\n    titles = [\"Input X\", \"Kernel (Frozen)\", \"Activations A\"]\n    positions = [(0, 1.5), (9, 1.5), (15, 1.5)]\n    for title, (x, y) in zip(titles, positions):\n        ax.text(x, y, title, fontsize=12, fontweight=\"bold\", ha=\"left\")\n\n    plt.show()","key":"KKv5z1sLmm"},{"type":"outputs","id":"NqDGj5T7_g7DSMM2r1SDM","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/vnd.jupyter.widget-view+json":{"content":"{\"model_id\":\"f5357520fb1646a0b764f6adaabf937e\",\"version_major\":2,\"version_minor\":0}","content_type":"application/vnd.jupyter.widget-view+json"},"text/plain":{"content":"interactive(children=(IntSlider(value=50, description='step', min=1), Output()), _dom_classes=('widget-interacâ€¦","content_type":"text/plain"}}},"children":[],"key":"AhSP7ekfNp"}],"key":"bWZx81W5OW"}],"key":"UmqSA6wPWM"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Visualization: initialization (top) and after 100 optimization steps (bottom)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"QjzsFe1EZH"}],"key":"cYVb6iw2XU"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Input image will show patterns that the filter responds to most","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qoWBKRZeD4"}],"key":"M2rr1N3WCz"}],"key":"rbDFLombyl"}],"key":"hbd5xXoQhe"}],"key":"R9ScIdg4lQ"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"-"}},"children":[{"type":"code","lang":"python","executable":true,"value":"if not interactive:\n    visualize_gradient_ascent(1)\n    visualize_gradient_ascent(100)","key":"VchI7yX5Ch"},{"type":"outputs","id":"BQVPCkh1qrzNmJVdnYd_E","children":[],"key":"s5b3TR8n2J"}],"key":"cNvff4g1AM"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Gradient Ascent in input space in PyTorch","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"uUv8ZUbMPW"}],"key":"TeeKOaR7ed"}],"key":"lgWruwlqwT"},{"type":"block","kind":"notebook-content","children":[{"type":"code","lang":"python","value":"# Create a random input tensor and tell Adam to optimize the pixels\nimg = np.random.uniform(150, 180, (sz, sz, 3)) / 255\nimg_tensor = torch.from_numpy(img.transpose(2, 0, 1)).to(self.device)\nimg_tensor.requires_grad_()\noptimizer = optim.Adam([img_tensor], lr=lr, weight_decay=1e-6)","position":{"start":{"line":1,"column":1},"end":{"line":7,"column":1}},"key":"WUrD4t4duC"},{"type":"code","lang":"python","value":"for _ in range(steps): \n    # Add our hook on the layer of interest to get the activations\n    hook = layer.register_forward_hook(hook_fn) \n    \n    # Run the input through the model\n    model(img_tensor) ","position":{"start":{"line":8,"column":1},"end":{"line":15,"column":1}},"key":"s1XaXj5Ng7"},{"type":"code","lang":"python","value":"    # Loss = Avg Activation of specific filter\n    loss = -activations[0, filter_idx].mean()\n    \n    # Update inputs to maximize activation\n    loss.backward() \n    optimizer.step()","position":{"start":{"line":16,"column":1},"end":{"line":23,"column":1}},"key":"vIOpzh9jVa"}],"key":"kp3iKfZqlq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"import cv2\n\nclass FilterVisualizer():\n    def __init__(self, model, size=56, upscaling_steps=12, upscaling_factor=1.2, device=None):\n        self.size = size\n        self.upscaling_steps = upscaling_steps\n        self.upscaling_factor = upscaling_factor\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = model.features if hasattr(model, 'features') else model\n        self.model = self.model.to(self.device).eval()\n        self.hook = None\n        self.activations = None\n\n        # Get indices of all Conv2d layers\n        self.conv_layers = [layer for layer in self.model.modules() if isinstance(layer, nn.Conv2d)]\n\n    def hook_fn(self, module, input, output):\n        self.activations = output\n\n    def register_hook(self, conv_layer_index):\n        if self.hook is not None:\n            self.hook.remove()\n        layer = self.conv_layers[conv_layer_index]\n        self.hook = layer.register_forward_hook(self.hook_fn)\n\n    def visualize(self, conv_layer_index, filter_idx, lr=0.1, opt_steps=20, blur=None):\n        sz = self.size\n        img = np.random.uniform(150, 180, (sz, sz, 3)) / 255  # Random noise image\n\n        self.register_hook(conv_layer_index)  # Attach hook\n\n        for _ in range(self.upscaling_steps):  # Iteratively upscale\n            img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).unsqueeze(0).float().to(self.device)\n            img_tensor.requires_grad_()\n            optimizer = optim.Adam([img_tensor], lr=lr, weight_decay=1e-6)\n\n            for _ in range(opt_steps):  # Optimize pixel values\n                optimizer.zero_grad()\n                self.model(img_tensor)\n                loss = -self.activations[0, filter_idx].mean()\n                loss.backward()\n                optimizer.step()\n\n            img = img_tensor.detach().cpu().numpy()[0].transpose(1, 2, 0)\n            self.output = img\n            sz = int(self.upscaling_factor * sz)  # Increase image size\n            img = cv2.resize(img, (sz, sz), interpolation=cv2.INTER_CUBIC)  # Upscale image\n            if blur is not None:\n                img = cv2.GaussianBlur(img, (blur, blur), 0)  # Apply blur to reduce noise\n\n        self.hook.remove()  # Remove hook after use\n        return self.output\n\n    def visualize_filters(self, conv_layer_index, num_filters=None, blur=None, filters=None):\n        filter_images = []\n\n        if filters:\n            for filter_idx in filters:\n                img = self.visualize(conv_layer_index, filter_idx, blur=blur)\n                filter_images.append(img)\n            num_filters = len(filters)\n        else:\n            # Visualize first to get activations and number of filters\n            img = self.visualize(conv_layer_index, 0, blur=blur)\n            filter_images.append(img)\n            if self.activations is not None:\n                total_filters = self.activations.shape[1]\n                num_filters = num_filters or total_filters\n                for filter_idx in range(1, num_filters):\n                    img = self.visualize(conv_layer_index, filter_idx, blur=blur)\n                    filter_images.append(img)\n            else:\n                raise RuntimeError(\"Failed to get layer activations.\")\n\n        self.show_filters(filter_images, num_filters, conv_layer_index)\n\n    def show_filters(self, filter_images, num_filters, layer_id):\n        cols = min(10, num_filters)  # Limit to max 10 columns\n        rows = (num_filters // cols) + int(num_filters % cols > 0)\n\n        fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n        axes = np.array(axes).flatten()  # Flatten in case of single row/col\n\n        for i, img in enumerate(filter_images):\n            axes[i].imshow(np.clip(img, 0, 1))\n            axes[i].axis('off')\n\n        # Remove empty subplots\n        for i in range(len(filter_images), len(axes)):\n            fig.delaxes(axes[i])\n\n        fig.subplots_adjust(wspace=0, hspace=0)\n        plt.tight_layout(pad=0, rect=[0.05, 0, 1, 1])  # Leave room on the left (x=0.05)\n        fig.supylabel(f\"Layer {layer_id}\", fontsize=12)\n        plt.show()","key":"PuYZRSNl4A"},{"type":"outputs","id":"2U-i6qMVxe1VRvtwH0VYw","children":[],"key":"eZGfu75RbC"}],"key":"b324kxMYEz"},{"type":"block","kind":"notebook-content","data":{"id":"IRLEv-kEYid0","slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"First layers respond mostly to colors, horizontal/diagonal edges","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"OITZc7TVrG"}],"key":"iu9A1KCPM1"}],"key":"X8lvHLAQab"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper layer respond to circular, triangular, stripy,... patterns","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kR63XHbRBS"}],"key":"fiIwcSr3iL"}],"key":"ihcFRiTXOL"}],"key":"MsdvdLTStW"}],"identifier":"irlev-keyid0","label":"IRLEv-kEYid0","html_id":"irlev-keyid0","key":"d67WAUcmpg"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualizer = FilterVisualizer(cat_model, size=64, upscaling_steps=10, upscaling_factor=1.2, device=accelerator)\nvisualizer.visualize_filters(conv_layer_index=1, filters=[0,2,5,10,14,16,17,19])\nvisualizer.visualize_filters(conv_layer_index=2, filters=[11,13,17,20,23,27,36,39])\nvisualizer.visualize_filters(conv_layer_index=3, filters=[0,1,3,5,10,13,16,17])","key":"eifAm3HP4R"},{"type":"outputs","id":"X510vLok9A6SU1VcjI1Cj","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"84b07d3e47f48ce79a00cb20d7a8a322","path":"/84b07d3e47f48ce79a00cb20d7a8a322.json"},"image/png":{"content_type":"image/png","hash":"864c0e3e9b2b66ac2932761c74ca5b19","path":"/864c0e3e9b2b66ac2932761c74ca5b19.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"glkFkS0P0x"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"9f8e0609da6dde97e7d7ee2363cdd9a1","path":"/9f8e0609da6dde97e7d7ee2363cdd9a1.json"},"image/png":{"content_type":"image/png","hash":"5e0d6cceee0fa0454300a61ce7086c91","path":"/5e0d6cceee0fa0454300a61ce7086c91.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"IkKUch6kem"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"2ea5290ede5d6e92dc41f73dcc5bbd13","path":"/2ea5290ede5d6e92dc41f73dcc5bbd13.json"},"image/png":{"content_type":"image/png","hash":"35c67fc0e17f591cb8b65c2975ef3b2e","path":"/35c67fc0e17f591cb8b65c2975ef3b2e.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"z4IzaohWqs"}],"key":"D9qSasEChK"}],"key":"nw8LNxKOXV"},{"type":"block","kind":"notebook-content","data":{"id":"abJz4IaqYid0","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We need to go deeper and train for much longer.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"unFWyF0ecV"}],"key":"NVsWAYGl0e"}],"key":"wKh4GrKU34"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Letâ€™s do this again for the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"tEJpZxfSwW"},{"type":"inlineCode","value":"VGG16","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"eEleOzCEbq"},{"type":"text","value":" network pretrained on ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Y5wcJBSkur"},{"type":"inlineCode","value":"ImageNet","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"rs3c6sJqYr"}],"key":"dC2NoS23gN"}],"key":"IHtoBxgJP5"}],"key":"OmVhtABOkv"},{"type":"code","lang":"python","value":"from torchvision.models import vgg16, VGG16_Weights\nmodel = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)","position":{"start":{"line":4,"column":1},"end":{"line":7,"column":1}},"identifier":"abjz4iaqyid0-code","enumerator":"36","html_id":"abjz4iaqyid0-code","key":"h4rOLClgzn"},{"type":"image","style":{"width":"70%","marginLeft":"auto","marginRight":"auto"},"url":"/922ede31b9064082591035652deb4cdd.png","alt":"ml","key":"ZMzU4WDIx5","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/09_vgg16.png?raw=1","urlOptimized":"/922ede31b9064082591035652deb4cdd.webp"}],"identifier":"abjz4iaqyid0","label":"abJz4IaqYid0","html_id":"abjz4iaqyid0","visibility":"show","key":"wm4ho53BgX"},{"type":"block","kind":"notebook-code","data":{"id":"s714tCKwYid0","slideshow":{"slide_type":"skip"},"tags":[]},"children":[{"type":"code","lang":"python","executable":true,"value":"from torchvision.models import vgg16, VGG16_Weights\n\n# Load VGG16 pretrained on ImageNet\nvgg16_model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n\n# Remove the fully connected layers (equivalent to include_top=False in Keras)\nvgg16_model_feat = vgg16_model.features\n\n# Set model to evaluation mode and move to GPU\nvgg16_model_feat.eval();","identifier":"s714tckwyid0-code","visibility":"hide","enumerator":"37","html_id":"s714tckwyid0-code","key":"Ruy6Zo0lwa"},{"type":"outputs","id":"yrNHSxLrjoGNQ2zhNzaSx","children":[],"identifier":"s714tckwyid0-outputs","visibility":"show","html_id":"s714tckwyid0-outputs","key":"LqsM9ED0mU"}],"identifier":"s714tckwyid0","label":"s714tCKwYid0","html_id":"s714tckwyid0","visibility":"show","key":"FHAk2IWdag"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Print model summary\nsummary(vgg16_model_feat, input_size=(1, 3, 224, 224))  # Pretraining images where 224x224","key":"zdOaT7Wv5M"},{"type":"outputs","id":"DQZYARsUa8uo9E1tTpN5I","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":55,"metadata":{},"data":{"text/plain":{"content":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nSequential                               [1, 512, 7, 7]            --\nâ”œâ”€Conv2d: 1-1                            [1, 64, 224, 224]         1,792\nâ”œâ”€ReLU: 1-2                              [1, 64, 224, 224]         --\nâ”œâ”€Conv2d: 1-3                            [1, 64, 224, 224]         36,928\nâ”œâ”€ReLU: 1-4                              [1, 64, 224, 224]         --\nâ”œâ”€MaxPool2d: 1-5                         [1, 64, 112, 112]         --\nâ”œâ”€Conv2d: 1-6                            [1, 128, 112, 112]        73,856\nâ”œâ”€ReLU: 1-7                              [1, 128, 112, 112]        --\nâ”œâ”€Conv2d: 1-8                            [1, 128, 112, 112]        147,584\nâ”œâ”€ReLU: 1-9                              [1, 128, 112, 112]        --\nâ”œâ”€MaxPool2d: 1-10                        [1, 128, 56, 56]          --\nâ”œâ”€Conv2d: 1-11                           [1, 256, 56, 56]          295,168\nâ”œâ”€ReLU: 1-12                             [1, 256, 56, 56]          --\nâ”œâ”€Conv2d: 1-13                           [1, 256, 56, 56]          590,080\nâ”œâ”€ReLU: 1-14                             [1, 256, 56, 56]          --\nâ”œâ”€Conv2d: 1-15                           [1, 256, 56, 56]          590,080\nâ”œâ”€ReLU: 1-16                             [1, 256, 56, 56]          --\nâ”œâ”€MaxPool2d: 1-17                        [1, 256, 28, 28]          --\nâ”œâ”€Conv2d: 1-18                           [1, 512, 28, 28]          1,180,160\nâ”œâ”€ReLU: 1-19                             [1, 512, 28, 28]          --\nâ”œâ”€Conv2d: 1-20                           [1, 512, 28, 28]          2,359,808\nâ”œâ”€ReLU: 1-21                             [1, 512, 28, 28]          --\nâ”œâ”€Conv2d: 1-22                           [1, 512, 28, 28]          2,359,808\nâ”œâ”€ReLU: 1-23                             [1, 512, 28, 28]          --\nâ”œâ”€MaxPool2d: 1-24                        [1, 512, 14, 14]          --\nâ”œâ”€Conv2d: 1-25                           [1, 512, 14, 14]          2,359,808\nâ”œâ”€ReLU: 1-26                             [1, 512, 14, 14]          --\nâ”œâ”€Conv2d: 1-27                           [1, 512, 14, 14]          2,359,808\nâ”œâ”€ReLU: 1-28                             [1, 512, 14, 14]          --\nâ”œâ”€Conv2d: 1-29                           [1, 512, 14, 14]          2,359,808\nâ”œâ”€ReLU: 1-30                             [1, 512, 14, 14]          --\nâ”œâ”€MaxPool2d: 1-31                        [1, 512, 7, 7]            --\n==========================================================================================\nTotal params: 14,714,688\nTrainable params: 14,714,688\nNon-trainable params: 0\nTotal mult-adds (G): 15.36\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 108.38\nParams size (MB): 58.86\nEstimated Total Size (MB): 167.84\n==========================================================================================","content_type":"text/plain"}}},"children":[],"key":"vGUmNIdPkM"}],"key":"xm8awSB0yT"}],"key":"wDB7Wlt4FB"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"First layers: very clear color and edge detectors","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KlAtmmc3cF"}],"key":"pJk1ka24qb"}],"key":"By9Cu4q6sB"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"3rd layer responds to arcs, circles, sharp corners","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"DzlEQU2DsG"}],"key":"Kqp4N2BMeb"}],"key":"s0INbxDE0k"}],"key":"hcoIwThFuN"}],"key":"U1HY45a0M6"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualizer = FilterVisualizer(vgg16_model, size=64, upscaling_steps=10, upscaling_factor=1.2, device=accelerator)\nvisualizer.visualize_filters(conv_layer_index=0, filters=[0,1,4,6,11,19,20,21])\nvisualizer.visualize_filters(conv_layer_index=1, filters=[0,1,2,3,4,5,14,21])\nvisualizer.visualize_filters(conv_layer_index=2, filters=[1,2,4,5,9,15,21,22])","key":"Qo9fgUTVVe"},{"type":"outputs","id":"YGXLgMJ7X1gUtH5XIFfuA","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"8a120d9197fff7479305cff039ac426a","path":"/8a120d9197fff7479305cff039ac426a.json"},"image/png":{"content_type":"image/png","hash":"70a60bc17a2b136aa9424e94beb11322","path":"/70a60bc17a2b136aa9424e94beb11322.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"yXBRNGgwe8"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"66778cc1fd1381355190a45ff2a6d34f","path":"/66778cc1fd1381355190a45ff2a6d34f.json"},"image/png":{"content_type":"image/png","hash":"a1f73f74ac1aee37a3958d499ddbab1b","path":"/a1f73f74ac1aee37a3958d499ddbab1b.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"smVIAwfwVF"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"5cf040a7c478d865f2ab8ebbe3329b06","path":"/5cf040a7c478d865f2ab8ebbe3329b06.json"},"image/png":{"content_type":"image/png","hash":"e6b0b9ea7e59c9e957187dbb5d12d3c3","path":"/e6b0b9ea7e59c9e957187dbb5d12d3c3.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"YT1Ejftmzq"}],"key":"m3SXdcA6RO"}],"key":"xj62pGhs7y"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper: more intricate patterns in different colors emerge","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aproKwnPtR"}],"key":"VThAtSUp7g"}],"key":"PgWagpnPAF"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Swirls, arches, boxes, circles,...","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"elRq3AWpVm"}],"key":"SGBN8r7v0N"}],"key":"RZvX8b9MMe"}],"key":"lpFcZXfD1P"}],"key":"T3wFONO7x9"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualizer.visualize_filters(conv_layer_index=3, filters=[0,4,5,9,14,16,21,22])\nvisualizer.visualize_filters(conv_layer_index=4, filters=[5,6,7,8,15,16,27,28])\nvisualizer.visualize_filters(conv_layer_index=5, filters=[0,7,12,13,15,17,22,28])","key":"yZQza2rnzl"},{"type":"outputs","id":"wANGVfPdR3Z07Pn0e0DTA","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"1d3fcd87499f8808d009d6bc2c449d9f","path":"/1d3fcd87499f8808d009d6bc2c449d9f.json"},"image/png":{"content_type":"image/png","hash":"f753c46fd3b6cc9a85c0308560c00ae4","path":"/f753c46fd3b6cc9a85c0308560c00ae4.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"NoNesksUIj"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"db052406da319633a4eca67d33a64300","path":"/db052406da319633a4eca67d33a64300.json"},"image/png":{"content_type":"image/png","hash":"48ef79d68dc3e06ee90dbc4e04a617f8","path":"/48ef79d68dc3e06ee90dbc4e04a617f8.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"ERWjFLUqbK"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"2f2ebb414d52e4713b71faaef16edd4c","path":"/2f2ebb414d52e4713b71faaef16edd4c.json"},"image/png":{"content_type":"image/png","hash":"790fffc761694b87bd0012b96d732b96","path":"/790fffc761694b87bd0012b96d732b96.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"pLLSAShxW9"}],"key":"pVnSU3LTrY"}],"key":"jXDXdFBK8h"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deeper: Filters specialize in all kinds of natural shapes","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"MFVdnK5VMi"}],"key":"U56CuSUJgj"}],"key":"t75ykXaCKx"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"More complex patterns (waves, landscapes, eyes) seem to appear","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bhz8Df3gGZ"}],"key":"Bn87a3RGfy"}],"key":"AaxkC3e5qN"}],"key":"wY4OIf6qRb"}],"key":"TsIoDnV8uB"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"visualizer.visualize_filters(conv_layer_index=6, filters=[0,1,2,8,13,17,25,28])\nvisualizer.visualize_filters(conv_layer_index=7, filters=[4,5,6,12,14,16,20,27])\nvisualizer.visualize_filters(conv_layer_index=8, filters=[3,6,10,15,16,20,26,29])","key":"UevCajkxZT"},{"type":"outputs","id":"iMq-Lro72ZVK_xZEhFMMh","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"7b639fe197470bb8b6db02b1406171d4","path":"/7b639fe197470bb8b6db02b1406171d4.json"},"image/png":{"content_type":"image/png","hash":"8a2913ef9a21152e5000961e491cccc7","path":"/8a2913ef9a21152e5000961e491cccc7.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"Ya2F1XMvC8"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"69e069327589361d7a53b207675aedb6","path":"/69e069327589361d7a53b207675aedb6.json"},"image/png":{"content_type":"image/png","hash":"334f7f19b3d57e4dd0213b934cf7c6c3","path":"/334f7f19b3d57e4dd0213b934cf7c6c3.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"AALcoyoq9k"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"8fc1711706f6f68075096051b31d8314","path":"/8fc1711706f6f68075096051b31d8314.json"},"image/png":{"content_type":"image/png","hash":"b6cccb44f35f38fe0b8df49931fb7f4a","path":"/b6cccb44f35f38fe0b8df49931fb7f4a.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"LGD7YUU2L2"}],"key":"ySxBeuOPH9"}],"key":"fNC4Ae1OoI"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Deepest layers have 512 filters each, each responding to very different patterns","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"mx8vpxd5ZW"}],"key":"lWnulDkV3y"}],"key":"qBefC8HmjQ"},{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"This 512-dimensional embedding separates distinct classes of images in â€˜feature spaceâ€™","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"xotoH8qQwS"}],"key":"j5neKTqXkI"}],"key":"y8On6cvZye"}],"key":"mHRWzQoiha"}],"key":"FBwEZZPATl"},{"type":"block","kind":"notebook-code","data":{"scrolled":true},"children":[{"type":"code","lang":"python","executable":true,"value":"visualizer.visualize_filters(conv_layer_index=10, filters=[10,12,14,20,22,25,26,28])\nvisualizer.visualize_filters(conv_layer_index=11, filters=[1,9,12,14,15,23,24,25])\nvisualizer.visualize_filters(conv_layer_index=12, filters=[0,11,18,21,34,48,52,84])","key":"LYkpZSxWqk"},{"type":"outputs","id":"sovhdiC9EQdKIh_mKQeNp","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"346fee6ae54da84d3835f178772be97f","path":"/346fee6ae54da84d3835f178772be97f.json"},"image/png":{"content_type":"image/png","hash":"2cc2e12ccc4d45322d31c0fb9af38520","path":"/2cc2e12ccc4d45322d31c0fb9af38520.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"Oj7UHNcIfs"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"e463d8634b1d8ad925be80993f0a0ab1","path":"/e463d8634b1d8ad925be80993f0a0ab1.json"},"image/png":{"content_type":"image/png","hash":"3b346a6c7968522ae325d23ef28625c6","path":"/3b346a6c7968522ae325d23ef28625c6.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"AujVvmHPtb"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"8fb9270222f0b84b1c834af7077e7210","path":"/8fb9270222f0b84b1c834af7077e7210.json"},"image/png":{"content_type":"image/png","hash":"6a5674e66f1269ef68f7b19f1ebdb14d","path":"/6a5674e66f1269ef68f7b19f1ebdb14d.png"},"text/plain":{"content":"<Figure size 4800x600 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"ZZg2N39wb4"}],"key":"IsP33sDMBn"}],"key":"d5L2V75SM5"},{"type":"block","kind":"notebook-content","data":{"id":"Co-4wIgXYid1","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Visualizing class activation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"U4Bcte3fUa"}],"identifier":"visualizing-class-activation","label":"Visualizing class activation","html_id":"visualizing-class-activation","implicit":true,"key":"kawJ8EPHfO"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We can also visualize which pixels of the input had the greatest influence on the final classification. Helps to interpret what the model is paying attention to.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"wOEJCgWEu0"}],"key":"XZnaObflLS"}],"key":"ANr49R6OlX"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Class activation maps","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"VoWohsCkJH"}],"key":"MeZ8eeRt3m"},{"type":"text","value":" : produces a heatmap over the input image","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Grnl1lgy6x"}],"key":"UQJ2ZGaM1B"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":4,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Choose a convolution layer, do Global Average Pooling (GAP) to get one output per channel","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"eSUJW8rBW1"}],"key":"Nmure2NWnJ"}],"key":"YcP9fQoFLL"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Get the weights between those outputs and the class of interest","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"QV211rCsqx"}],"key":"LCdu6KTjFx"}],"key":"iCyGfQri8F"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compute the weighted sum of all filter activations: combines what each filter is responding to and how much this affects the class prediction","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"rlIY0zLXPr"}],"key":"p90DRiT40f"}],"key":"H1AlcnDo3i"}],"key":"lHWWnIyEpb"}],"key":"RTZQvhZCJZ"}],"key":"HVncPduns8"},{"type":"image","style":{"width":"50%","marginLeft":"auto","marginRight":"auto"},"url":"/60568ee921951e10ac5e47117a336264.png","alt":"ml","key":"ozfsaKUhOX","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/activation_map.png?raw=1"}],"identifier":"co-4wigxyid1","label":"Co-4wIgXYid1","html_id":"co-4wigxyid1","visibility":"show","key":"Ju95JLSAWd"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Implementing gradCAM","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"o9PN6U3O2X"}],"identifier":"implementing-gradcam","label":"Implementing gradCAM","html_id":"implementing-gradcam","implicit":true,"key":"MrDfKf6rNn"},{"type":"code","lang":"python","value":"    # Hooks to capture activations and gradients\n    def forward_hook(module, input, output):\n        activations = output\n    def backward_hook(module, grad_input, grad_output):\n        gradients = grad_output[0]\n    target_layer.register_forward_hook(forward_hook)\n    target_layer.register_full_backward_hook(backward_hook)","position":{"start":{"line":2,"column":1},"end":{"line":10,"column":1}},"key":"lk1ptb6HqG"},{"type":"code","lang":"python","value":"    # Forward pass + get predicted class\n    pred_class = model(img_tensor).argmax(dim=1).item()\n\n    # For that class, do a backward pass to get gradients\n    model.zero_grad()\n    output[:, pred_class].backward()\n\n    # Compute Grad-CAM heatmap\n    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)  # GAP layer\n    heatmap = torch.sum(weights * activations, dim=1).squeeze()","position":{"start":{"line":11,"column":1},"end":{"line":22,"column":1}},"key":"KN5hP00bfv"}],"key":"y5UYIMDlC3"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"ResNet50 model, image of class Elephant, top-8 channels (highest weighst)","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"k1KOsKmxHa"}],"key":"uwIckk63AR"}],"key":"EOonf7HhAv"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from torchvision import models\n\ndef gradCAM(img_path, show_channels=True, top_k=8):\n    model = models.resnet50(pretrained=True)\n    model.eval()\n    target_layer = model.layer4[-1]\n\n    preprocess = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    original_img = cv2.imread(img_path)\n    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n    display_img = original_img.copy()\n    img_tensor = preprocess(original_img).unsqueeze(0)\n\n    activations = None\n    gradients = None\n\n    def forward_hook(module, input, output):\n        nonlocal activations\n        activations = output\n\n    def backward_hook(module, grad_input, grad_output):\n        nonlocal gradients\n        gradients = grad_output[0]\n\n    target_layer.register_forward_hook(forward_hook)\n    target_layer.register_full_backward_hook(backward_hook)\n\n    output = model(img_tensor)\n    pred_class = output.argmax(dim=1).item()\n    model.zero_grad()\n    output[:, pred_class].backward()\n\n    weights = torch.mean(gradients, dim=[2, 3], keepdim=True)  # [B, C, 1, 1]\n    activations = activations.detach().squeeze(0)              # [C, H, W]\n    weights = weights.detach().squeeze(0).squeeze(-1).squeeze(-1)  # [C]\n\n    # Visualize top-k channels\n    if show_channels:\n        # Get top-k channel indices by absolute weight\n        _, top_idxs = torch.topk(weights.abs(), k=top_k)\n        fig, axes = plt.subplots(1, top_k, figsize=(2.5 * top_k, 2.5))\n        for i, idx in enumerate(top_idxs):\n            channel_img = activations[idx].cpu().numpy()\n            channel_img = np.maximum(channel_img, 0)\n            channel_img /= channel_img.max() + 1e-10\n            channel_img = cv2.resize(channel_img, (original_img.shape[1], original_img.shape[0]))\n\n            weighted_img = channel_img * weights[idx].item()\n            weighted_img = np.clip(weighted_img, 0, 1)\n\n            axes[i].imshow(channel_img, cmap='viridis')\n            axes[i].axis('off')\n            axes[i].set_title(f\"w={weights[idx].item():.4f}\", fontsize=12)\n\n            #axes[1, i].imshow(weighted_img, cmap='inferno')\n            #axes[1, i].axis('off')\n            #axes[1, i].set_title(f\"w Ã— ch {idx.item()}\", fontsize=12)\n\n        plt.suptitle(\"Top Conv Channels & Weights\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n\n    # Final Grad-CAM map\n    heatmap = torch.sum(weights[:, None, None] * activations, dim=0).cpu().numpy()\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max() + 1e-10\n    heatmap_resized = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n    superimposed_img = cv2.addWeighted(original_img, 0.6, heatmap_colored, 0.4, 0)\n\n    # Show original + Grad-CAM\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    axs[0].imshow(display_img)\n    axs[0].axis(\"off\")\n    axs[0].set_title(\"Original Image\", fontsize=12)\n\n    axs[1].imshow(superimposed_img)\n    axs[1].axis(\"off\")\n    axs[1].set_title(\"Grad-CAM\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()","key":"by902AR3wP"},{"type":"outputs","id":"bjXq3KLkg1oUt7SXSJtHS","children":[],"key":"bARQiYSRfl"}],"key":"c6q4O0R0Tj"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"img_path = \"../notebooks/images/10_elephants.jpg\"\ngradCAM(img_path, show_channels=True, top_k=8)","key":"A39byexNOk"},{"type":"outputs","id":"T1bpW6LixihjCdTfZILcG","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"665de904a518285f41dd3c02686beed5","path":"/665de904a518285f41dd3c02686beed5.json"},"image/png":{"content_type":"image/png","hash":"01cfeab2dbb07941d8a3758faee67dcd","path":"/01cfeab2dbb07941d8a3758faee67dcd.png"},"text/plain":{"content":"<Figure size 6000x750 with 8 Axes>","content_type":"text/plain"}}},"children":[],"key":"tQ9SvXybAM"},{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"ec7164ff62f2db4c29faee019a43ea2d","path":"/ec7164ff62f2db4c29faee019a43ea2d.json"},"image/png":{"content_type":"image/png","hash":"827ece2583245157eb926569dea3dd8c","path":"/827ece2583245157eb926569dea3dd8c.png"},"text/plain":{"content":"<Figure size 3600x1800 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"BM7x7zmJSr"}],"key":"yCzZyh5sXc"}],"key":"xv2tI55xiV"},{"type":"block","kind":"notebook-content","data":{"id":"5bs5a5voYid2","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Transfer learning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"NBs45nDXPU"}],"identifier":"transfer-learning","label":"Transfer learning","html_id":"transfer-learning","implicit":true,"key":"LQ41xwAi0B"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We can re-use pretrained networks instead of training from scratch","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"cphGgtGFUv"}],"key":"whNDJ2dcvE"}],"key":"aWeV33BpKy"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Learned features can be a useful generic representation of the visual world","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"oI8s3ZvX0J"}],"key":"vJfF5YPO7Q"}],"key":"Rl9AsfMYVw"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"General approach:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"OOsAs4sFGx"}],"key":"vPN87Zg9tv"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Remove the original classifier head and add a new one","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"PQ5k4SgNu3"}],"key":"HqNYuRHQaA"}],"key":"vrzWBM426h"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Freeze the pretrained weights (backbone), then train as usual","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"J8HXPqEkSY"}],"key":"M14uAaTSQ6"}],"key":"korPIjw2N1"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Optionally unfreeze (and re-learn) part of the network","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"cH4RiSHldO"}],"key":"dingBdpNzL"}],"key":"sJYaudRDJA"}],"key":"y34b12NqPy"}],"key":"Yk1md2y27W"}],"key":"zsYUenlzgI"},{"type":"image","url":"/789bb803a15f62d686f2717f527fc39a.jpeg","width":1200,"key":"syWak46gRF","urlSource":"https://storage.googleapis.com/lds-media/images/transfer-learning-fine-tuning-approach.width-1200.jpg","urlOptimized":"/789bb803a15f62d686f2717f527fc39a.webp"}],"identifier":"5bs5a5voyid2","label":"5bs5a5voYid2","html_id":"id-5bs5a5voyid2","visibility":"show","key":"QSvsgSuBmM"},{"type":"block","kind":"notebook-content","data":{"id":"irN1lLgHYid2","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Using pre-trained networks: 3 ways","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"eJJMydnvgJ"}],"identifier":"using-pre-trained-networks-3-ways","label":"Using pre-trained networks: 3 ways","html_id":"using-pre-trained-networks-3-ways","implicit":true,"key":"QIBu25ea5n"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Fast feature extraction (for similar task, little data)","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"fCuW37T4bF"}],"key":"blZumCeLAC"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Run data through convolutional base to build new features","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"yEhzfYpd1t"}],"key":"m85OB0zJbr"}],"key":"Viac2NlwvX"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use embeddings as input to a dense layer (or another algorithm)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"XFBiFHD6rl"}],"key":"F75MNMa6D6"}],"key":"Q8MaZWEXfO"}],"key":"Q06gcDgfbo"}],"key":"K7F8D08WrY"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"End-to-end finetuning (for similar task, lots of data + data augmentation)","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"IgdtYZgNEi"}],"key":"Y23ECXUcPY"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Extend the convolutional base model with a new dense layer","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"sgfQaLvmm9"}],"key":"ks0JPbN0d3"}],"key":"X73UMAnev9"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Train it end to end on the new data","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"gXlxz2AYCp"}],"key":"OwEPMmCPBd"}],"key":"kmmMmgHUGa"}],"key":"J4zA9oaOyz"}],"key":"bQe0K0tkfQ"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Partial fine-tuning (for somewhat different task)","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"vrJY8UaJEC"}],"key":"ycEmLn8Od2"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":9,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Unfreeze a few of the top convolutional layers, and retrain","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"OaUY9hguQC"}],"key":"mcVXU6YhyC"}],"key":"e7AC36fEGD"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Update only the deeper (more task-specific layers)","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"z7WKGq9B3h"}],"key":"wBfJFZhET8"}],"key":"nxZwNts2Wa"}],"key":"cptwy2eg3p"}],"key":"ZKvBTvWWNI"}],"key":"CFfi50dOQu"},{"type":"image","style":{"width":"50%","marginLeft":"auto","marginRight":"auto"},"url":"/f8a16ea01ea15ae00f7316932d69bc93.png","alt":"ml","key":"KmhUzRX7lP","urlSource":"https://raw.githubusercontent.com/ML-course/master/master/notebooks/images/pretraining.png?raw=1"}],"identifier":"irn1llghyid2","label":"irN1lLgHYid2","html_id":"irn1llghyid2","visibility":"show","key":"ZbPan4rjrv"},{"type":"block","kind":"notebook-content","data":{"id":"2bWNYRaoYid2","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Fast feature extraction","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pMRhI0IAUG"}],"identifier":"fast-feature-extraction","label":"Fast feature extraction","html_id":"fast-feature-extraction","implicit":true,"key":"hUubXcWmnM"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Pretrained ResNet18 architecture, remove fully-connected layers","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"utlNoWi0tb"}],"key":"TfEPvY0Z76"}],"key":"fqxnNyHW8n"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Add new classification head, freeze all pretrained weights","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"pTga4DaB1E"}],"key":"DjS2wVeQIW"}],"key":"TrAsycnhJ8"}],"key":"ON8b22wF0c"},{"type":"code","lang":"python","value":"def __init__(self):\n    resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n    self.feature_dim = resnet.fc.in_features\n    self.resnet.fc = nn.Identity()                   # Remove old head\n    self.classifier = nn.Linear(self.feature_dim, 1) # New head\n    for param in self.backbone.parameters():  # Freeze backbone \n        param.requires_grad = False","position":{"start":{"line":5,"column":1},"end":{"line":13,"column":1}},"identifier":"2bwnyraoyid2-code","enumerator":"38","html_id":"id-2bwnyraoyid2-code","key":"sfO96YW3pV"},{"type":"code","lang":"python","value":"# Train\ndef forward(self, x):\n    features = self.resnet(x)\n    logits = self.classifier(features)\n    return logits.squeeze(1)","position":{"start":{"line":14,"column":1},"end":{"line":20,"column":1}},"key":"Tc9W3swtxO"}],"identifier":"2bwnyraoyid2","label":"2bWNYRaoYid2","html_id":"id-2bwnyraoyid2","visibility":"show","key":"W8HnCPGktJ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Faster data loader, but needs to be stored on file to be pickled\ndataset_code = \"\"\"\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset\n\nclass AlbumentationsImageDataset(Dataset):\n    def __init__(self, image_folder_dataset, transform=None):\n        self.image_folder = image_folder_dataset\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        img_path, label = self.image_folder.samples[idx]\n        image = cv2.imread(img_path)\n        if image is None:\n            raise RuntimeError(f\"Failed to load image at {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform:\n            image = self.transform(image=image)[\"image\"]\n\n        return image, label\n\n    def __len__(self):\n        return len(self.image_folder)\n\"\"\"\n\nwith open(\"fast_dataset.py\", \"w\") as f:\n    f.write(dataset_code.strip())","key":"ZR6gGKn3VK"},{"type":"outputs","id":"Cs56jFXOnf8GUOQwbqXqo","children":[],"key":"PyfHE3XcD0"}],"key":"iczHE85cQI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from albumentations.pytorch import ToTensorV2\nimport albumentations as A\nimport multiprocessing\nfrom fast_dataset import AlbumentationsImageDataset\n\nnum_workers = 4 #multiprocessing.cpu_count()\n\n# Much faster version of the data module\n# Uses Albumentations to do data augmentation in GPU instead of CPU\nclass FastCatDataModule(pl.LightningDataModule):\n    def __init__(self, data_dir, batch_size=20, img_size=(150, 150), num_workers=None):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.num_workers = num_workers if num_workers is not None else os.cpu_count()\n\n        self.train_transform = A.Compose([\n            A.RandomScale(scale_limit=(0.0, 0.25), p=1.0),  # Random zoom-in up to 125%\n            A.PadIfNeeded(min_height=self.img_size[0], min_width=self.img_size[1], border_mode=0),  # pad if needed\n            A.RandomCrop(height=self.img_size[0], width=self.img_size[1]),\n            A.HorizontalFlip(),\n            A.Rotate(limit=40),\n            A.Affine(shear=20),\n            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n            ToTensorV2()\n        ])\n\n        self.val_transform = A.Compose([\n            A.Resize(height=self.img_size[0], width=self.img_size[1]),\n            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n            ToTensorV2()\n        ])\n\n    def setup(self, stage=None):\n        train_dir = os.path.join(self.data_dir, \"train\")\n        val_dir = os.path.join(self.data_dir, \"validation\")\n\n        base_train_dataset = datasets.ImageFolder(train_dir)\n        base_val_dataset = datasets.ImageFolder(val_dir)\n\n        self.train_dataset = AlbumentationsImageDataset(base_train_dataset, transform=self.train_transform)\n        self.val_dataset = AlbumentationsImageDataset(base_val_dataset, transform=self.val_transform)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n","key":"ULQR75PCN6"},{"type":"outputs","id":"rWrel6U8zmtwZlcX3Ro_9","children":[],"key":"jEx9ldFTKS"}],"key":"xAuM2WD4V0"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"import torch.nn.functional as F\nimport torchvision.models as models\nimport pytorch_lightning as pl\nimport torch.nn as nn\nimport torch\n\nclass TransferCatClassifier(pl.LightningModule):\n    def __init__(self, finetune_mode=\"head\", learning_rate=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Load full pre-trained ResNet18\n        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        self.feature_dim = self.resnet.fc.in_features\n\n        # Replace FC head with your own\n        self.resnet.fc = nn.Identity()  # We'll do classification separately\n        self.classifier = nn.Linear(self.feature_dim, 1)\n\n        # Apply fine-tuning strategy\n        self.finetune_mode = finetune_mode\n        self.freeze_layers()\n\n        # Loss and metric\n        self.loss_fn = nn.BCEWithLogitsLoss()\n        self.accuracy = Accuracy(task=\"binary\")\n\n    def freeze_layers(self):\n        # Freeze all\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n\n        if self.finetune_mode == \"last_block\":\n            # Unfreeze last block (layer4)\n            for param in self.resnet.layer4.parameters():\n                param.requires_grad = True\n        elif self.finetune_mode == \"full\":\n            for param in self.resnet.parameters():\n                param.requires_grad = True\n        # else 'head': leave everything except classifier frozen\n\n    def forward(self, x):\n        features = self.resnet(x)\n        logits = self.classifier(features)\n        return logits.squeeze(1)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y.float())\n        acc = self.accuracy(logits, y.int())\n        self.log(\"train_loss\", loss, prog_bar=True)\n        self.log(\"train_acc\", acc, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y.float())\n        acc = self.accuracy(logits, y.int())\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        if self.finetune_mode == \"full\": # Keep LR small for backbone\n            return torch.optim.Adam([\n                {\"params\": self.resnet.parameters(), \"lr\": self.hparams.learning_rate * 0.1},\n                {\"params\": self.classifier.parameters(), \"lr\": self.hparams.learning_rate}\n            ])\n        else:\n            return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\ndef transfer_trainer(mode):\n    print(f\"Training mode: {mode}\")\n    data_module = FastCatDataModule(data_dir, batch_size=64)\n    model = TransferCatClassifier(finetune_mode=mode, learning_rate=1e-3)\n    metric_tracker = MetricTracker()\n\n    trainer = pl.Trainer(\n        max_epochs=30,\n        accelerator=accelerator,\n        devices=\"auto\",\n        log_every_n_steps=3,\n        deterministic=True,\n        callbacks=[metric_tracker]\n    )\n    trainer.fit(model, datamodule=data_module)\n    return metric_tracker.history","key":"PZ3lLjKoyR"},{"type":"outputs","id":"ytLsammOsqWMM4DipeRRI","children":[],"key":"DSmlsihDSW"}],"key":"WFGZuLZM5Q"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Only train the head\nif histories and \"cat_head\" in histories:\n    history_cat_head = histories[\"cat_head\"]\nelse:\n    history_cat_head = transfer_trainer(\"head\")","key":"bOIh1trV2L"},{"type":"outputs","id":"dtdpYEkTT5HqT4KaCDo2g","children":[],"key":"i8BpZuNStU"}],"key":"UEX5ktJ5wX"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Fast feature extraction for cats and dogs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"fjkupNJ3k5"}],"key":"x1tHC96xyV"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Training from scratch (see earlier): 85% accuracy after 30 epochs","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"h6GWDW7uE0"}],"key":"cduCk88Pat"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"With transfer learning: 94% in a few epochs","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"e9yxi5iqhE"}],"key":"j2nla6WxTP"}],"key":"Cr26RY4A1L"}],"key":"zwXxJMx1aR"}],"key":"cKOZR5Hjrk"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"However, not much capacity for learning more (weights are frozen)","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"F0UMXBlNOs"}],"key":"CrVW5R6z9e"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"We could add more dense layers, but are stuck with the conv layers","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"pBRfPiiNsj"}],"key":"WMACps7kdb"}],"key":"pQAdHxyDxB"}],"key":"n1O2nPZ21R"}],"key":"xn1DTsbnca"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"512 trainable parameters","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"E1avYae5Nr"}],"key":"C7IpUE9mWc"}],"key":"SyfhQv0ocZ"}],"key":"PBwMxWfIjL"}],"key":"mA3XJL7LPm"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_training(history_cat_head)","key":"Y4iji8VhvV"},{"type":"outputs","id":"dV505fxodusRRmRJGyUSZ","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"ab5153e287c8d1da041b922ac47f0c33","path":"/ab5153e287c8d1da041b922ac47f0c33.json"},"image/png":{"content_type":"image/png","hash":"74cd737111900edce8f302ce6076e556","path":"/74cd737111900edce8f302ce6076e556.png"},"text/plain":{"content":"<Figure size 3600x1200 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"jaSaQqY883"}],"key":"Ewtp4xoI37"}],"key":"Xv1XUeKmqi"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Partial finetuning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"rOwceHZP1N"}],"identifier":"partial-finetuning","label":"Partial finetuning","html_id":"partial-finetuning","implicit":true,"key":"asLCOryK40"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Freeze backbone as before","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bfo7U52wnQ"}],"key":"rUX9WGv8th"}],"key":"E5SIGj09zk"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Unfreeze the last convolutional block","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"lWnEJji9PG"}],"key":"bSyZmUZrbt"}],"key":"BPqc6D1KHG"}],"key":"m3CXOZY2nk"},{"type":"code","lang":"python","value":"# Freeze conv layers \nfor param in self.backbone.parameters():\n    param.requires_grad = False\n    \n# Unfreeze last block \nfor param in self.resnet.layer4.parameters():\n    param.requires_grad = True","position":{"start":{"line":5,"column":1},"end":{"line":13,"column":1}},"key":"B8J9h4ByVU"}],"key":"KLGsJCn0Ny"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Unfreeze last block\nif histories and \"cat_unfreeze\" in histories:\n    history_cat_unfreeze = histories[\"cat_unfreeze\"]\nelse:\n    history_cat_unfreeze = transfer_trainer(\"last_block\")","key":"CvwdCdYOSN"},{"type":"outputs","id":"-A04IY3NJ8I7TvN-fWku9","children":[],"key":"X4T3W9usCv"}],"key":"FdLnc1XXVd"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Partial finetuning for cats and dogs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"npPVysTylF"}],"key":"IajTz52sMr"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Performance increases slightly to 95% accuracy in few epochs","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"LuSHiz7eYe"}],"key":"hJf9Fi2657"}],"key":"FPXOlMXl8j"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"No further increase with longer training","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"BBCnL4s5vB"}],"key":"Wb9gggnCBf"}],"key":"tRRQVNBSOC"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"8.4 million trainable parameters","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"XFsBYlLREq"}],"key":"FDVpg1Wv78"}],"key":"I3FOIsq9kX"}],"key":"Kfo37RoP60"}],"key":"vcTdWw2mLN"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_training(history_cat_unfreeze)","key":"XCY4dE7FCE"},{"type":"outputs","id":"Im2RTpg8GwLn19iMOboH6","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"7fe504a00ce8a0783b6a0082ee4e9228","path":"/7fe504a00ce8a0783b6a0082ee4e9228.json"},"image/png":{"content_type":"image/png","hash":"2193093d0b96af4c1011b3767bee2ab2","path":"/2193093d0b96af4c1011b3767bee2ab2.png"},"text/plain":{"content":"<Figure size 3600x1200 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"bnfqTR7AbG"}],"key":"RHh6LeN6b0"}],"key":"GXFUyZpg9H"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"End-to-end finetuning","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KAWdXv3i9V"}],"identifier":"end-to-end-finetuning","label":"End-to-end finetuning","html_id":"end-to-end-finetuning","implicit":true,"key":"LqFdL9f8yG"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Freeze backbone as before. Unfreeze the last convolutional block","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"AwuhovbSuf"}],"key":"ctEz5GPOjS"}],"key":"WQegbdjBRO"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Use a gentler initial learning rate for the backbone (to avoid destruction)","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uUdhLTcAWG"}],"key":"C24kWS4dOv"}],"key":"SLoZjTlK3I"}],"key":"ScL60A469s"},{"type":"code","lang":"python","value":"# Make conv layers trainable\nfor param in self.resnet.parameters():\n    param.requires_grad = True\n\n# Use a gentle learning rate for the backbone\ndef configure_optimizers(self):\n    return torch.optim.Adam([\n        {\"params\": self.resnet.parameters(), \"lr\": self.hparams.learning_rate * 0.1},\n        {\"params\": self.classifier.parameters(), \"lr\": self.hparams.learning_rate}])","position":{"start":{"line":5,"column":1},"end":{"line":15,"column":1}},"key":"QEuMdVbqrG"}],"key":"i2cLK19iY6"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"# Full\nif histories and \"cat_full\" in histories:\n    history_cat_full = histories[\"cat_full\"]\nelse:\n    history_cat_full = transfer_trainer(\"full\")","key":"hRx3OzJapz"},{"type":"outputs","id":"DrbmTZo_ssmPogeWELrBU","children":[],"key":"AGIXILa9aC"}],"key":"Anhja2Nijo"},{"type":"block","kind":"notebook-content","data":{"slideshow":{"slide_type":"slide"}},"children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"End-to-end finetuning for cats and dogs","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jPlqfYuhcI"}],"key":"i9zryz7BYz"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Very similar behavior, but more noisy","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"rfkgprfr5O"}],"key":"EIDwlPLetq"}],"key":"bqmZJgZTya"},{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Our dataset is likely too small to learn a better embedding","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Lo5oFQWcq1"}],"key":"pvG3z3VU2p"}],"key":"HjpByDhsfR"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"11.2 million trainable parameters","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"nLQRDwgjuz"}],"key":"empWMOIcgT"}],"key":"tsTDyCiC3r"}],"key":"rFK9jAPgWu"}],"key":"clbBLKRKSF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"plot_training(history_cat_full)","key":"HFADv1BV8O"},{"type":"outputs","id":"DBn9yhB0b2D9fQ7_RJ13k","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"application/pdf":{"content_type":"application/pdf","hash":"8ec5460a49c9241a007f3fc3a6ac12e1","path":"/8ec5460a49c9241a007f3fc3a6ac12e1.json"},"image/png":{"content_type":"image/png","hash":"f61cc57757fa119525fd0b701c86f340","path":"/f61cc57757fa119525fd0b701c86f340.png"},"text/plain":{"content":"<Figure size 3600x1200 with 2 Axes>","content_type":"text/plain"}}},"children":[],"key":"LeEfUSItec"}],"key":"HPjqQkkFRh"}],"key":"bbBHEZMVmJ"},{"type":"block","kind":"notebook-code","data":{"slideshow":{"slide_type":"skip"}},"children":[{"type":"code","lang":"python","executable":true,"value":"import pickle \n\nhistories = {\"mnist\":history,\"cat\":history_cat,\"cat2\":history_cat2,\n             \"cat_head\":history_cat_head,\"cat_unfreeze\":history_cat_unfreeze,\"cat_full\":history_cat_full}\nwith open(\"../data/histories.pkl\", \"wb\") as f:\n    pickle.dump(histories, f)","key":"mnKSzp2sbo"},{"type":"outputs","id":"FM8H6LsQXWFq3Qaor9-2-","children":[],"key":"vank5DHgZX"}],"key":"zwu0prvCoi"},{"type":"block","kind":"notebook-content","data":{"id":"x-ZNfBiiYid4","slideshow":{"slide_type":"slide"},"tags":[]},"children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Take-aways","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"CCqyC5awDD"}],"identifier":"take-aways","label":"Take-aways","html_id":"take-aways","implicit":true,"key":"GXyOCyeDKM"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":2,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"2D Convnets are ideal for addressing image-related problems.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"zobOyaIQO3"}],"key":"zBH4ufyIM6"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"1D convnets are sometimes used for text, signals,...","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"p2yrw2ad0U"}],"key":"v0VGi8vQMF"}],"key":"VYMjtbXyBf"}],"key":"iR64frW95G"}],"key":"G8XwAIFWHI"},{"type":"listItem","spread":true,"position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Compositionality: learn a hierarchy of simple to complex patterns","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"p9cZm8I193"}],"key":"DzgaY15S4g"}],"key":"D5GroDtKuh"},{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Translation invariance: deeper networks are more robust to data variance","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"q3GURaRoBv"}],"key":"XsIJ4cJowz"}],"key":"na1kOc95lV"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Data augmentation helps fight overfitting (for small datasets)","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"tymhxcuuZr"}],"key":"bUSF2bcE9b"}],"key":"tdTbjVd9f2"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Representations are easy to inspect: visualize activations, filters, GradCAM","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"TgfYGo0pQ9"}],"key":"cFHA4wQZDa"}],"key":"AFe0zDyncu"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Transfer learning: pre-trained embeddings can often be effectively reused to learn deep models for small datasets","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"m95QJYLrYA"}],"key":"cMeyEGKLwU"}],"key":"scJEtzuc6N"}],"key":"z2Ay2K3nCy"}],"identifier":"x-znfbiiyid4","label":"x-ZNfBiiYid4","html_id":"x-znfbiiyid4","visibility":"show","key":"ocDTrW3UD4"}],"key":"eOgKF94RO5"},"references":{"cite":{"order":[],"data":{}}}}