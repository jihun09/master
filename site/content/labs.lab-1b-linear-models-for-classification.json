{"version":3,"kind":"Notebook","sha256":"721dace5efa9eb81776027406cc0ce317c76983fb3ec193674072cede70e7ea6","slug":"labs.lab-1b-linear-models-for-classification","location":"/labs/Lab 1b - Linear Models for Classification.ipynb","dependencies":[],"frontmatter":{"title":"Lab 1b: Linear classification","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"github":"https://github.com/ml-course/master","copyright":"2025. CC0 Licensed - Use as you like. Appropriate credit is very welcome","numbering":{"title":{"offset":1}},"source_url":"https://github.com/ml-course/master/blob/master/labs/Lab 1b - Linear Models for Classification.ipynb","edit_url":"https://github.com/ml-course/master/edit/master/labs/Lab 1b - Linear Models for Classification.ipynb","exports":[{"format":"ipynb","filename":"Lab 1b - Linear Models for Classification.ipynb","url":"/Lab 1b - Linear Mode-5173f04db85e6653c454cc8831b5f8f7.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"uYd5TLeXk9"},{"type":"link","url":"https://www.openml.org/d/40996","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Fashion-MNIST dataset","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"HHEpkupx82"}],"urlSource":"https://www.openml.org/d/40996","key":"nVJRQeyytt"},{"type":"text","value":" contains 70,000 images of Zalando fashion products, classified into 10 types of clothing, each represented by 28 by 28 pixel values. We’s see how well we can classify these with linear models. Let’s start with looking at our data:","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"vhQN6rluUD"}],"key":"XICvPRhEeR"}],"key":"wuJqoj0sbd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nfrom matplotlib import cm\n\n# Hide convergence warning for now\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)","key":"PUXi3brD4R"},{"type":"outputs","id":"jFPlwc9EygQ8T-ofqVApK","children":[],"key":"J7Aycxmsz3"}],"key":"KuXboQQ6Bi"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Download FMINST data. Takes a while the first time.\nfmnist = oml.datasets.get_dataset(40996)\nX, y, _, _ = fmnist.get_data(target=fmnist.default_target_attribute); \nfmnist_classes = {0:\"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", 5: \"Sandal\", \n                  6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}","key":"aPfyq63Fll"},{"type":"outputs","id":"pLW_3ed-6txHqNWHJ-IKS","children":[],"key":"iMVYfDq9sY"}],"key":"tAU8iMJmQZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Take some random examples, reshape to a 32x32 image and plot\nfrom random import randint\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor i in range(5):\n    n = randint(0,70000)\n    axes[i].imshow(X.values[n].reshape(28, 28), cmap=plt.cm.gray_r)\n    axes[i].set_xlabel((fmnist_classes[int(y.values[n])]))\n    axes[i].set_xticks(()), axes[i].set_yticks(())\nplt.show();","key":"qA17Nt6IgP"},{"type":"outputs","id":"dXZK5JFHhlkj8m5ENrgrk","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"5a24bc3816a1d60f704292146e5a5790","path":"/5a24bc3816a1d60f704292146e5a5790.png"},"text/plain":{"content":"<Figure size 720x360 with 5 Axes>","content_type":"text/plain"}}},"children":[],"key":"piBQBYreVY"}],"key":"hynu5T7N6Q"}],"key":"V0EDGPBXez"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 1: A quick benchmark","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"lj1sxcSW9E"}],"identifier":"exercise-1-a-quick-benchmark","label":"Exercise 1: A quick benchmark","html_id":"exercise-1-a-quick-benchmark","implicit":true,"key":"ZqVbcftFBA"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"First, we’ll try the default ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"H8DRhYA6lc"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Logistic Regression","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"D9jZDsHbKf"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html","key":"EuFNejGeSx"},{"type":"text","value":" and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"GUJtEl8cz8"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html?highlight=linearsvc#sklearn.svm.LinearSVC","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Linear SVMs","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"CmKr0NclJW"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html?highlight=linearsvc#sklearn.svm.LinearSVC","key":"mJGpmYviga"},{"type":"text","value":". Click the links to read the documentation. We’ll also compare it to ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nFnw3WPWYl"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"k-Nearest Neighbors","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"TV0J64oYBE"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html","key":"ENznbTeNDQ"},{"type":"text","value":" as a point of reference. To see whether our models are overfitting, we also evaluate the training set error. This can be done using ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"sW85bVg835"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"inlineCode","value":"cross_validate","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"DsnlI7Bkno"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html","key":"UhagPePLRL"},{"type":"text","value":" instead of  ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nWOXZVJ5Bq"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"inlineCode","value":"cross_val_scores","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"J1ZdxNGuRF"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score","key":"DOlJl7Uhbw"},{"type":"text","value":".","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"li0VjZ6Wsm"}],"key":"fiGzzAQyyD"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"For now we are just interested in a quick approximation, so we don’t use the full dataset for our experiments. Instead, we use 10% of our samples:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"AKx2lj1UDp"}],"key":"x4vUpZ2DPz"}],"key":"HK4ymVtWWu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Take a 10% stratified subsample to speed up experimentation\nXs, _, ys, _ = train_test_split(X,y, stratify=y, train_size=0.1)","key":"AOnozW3xZ0"},{"type":"outputs","id":"Dg8ltnHSryAQBZOjZHDlg","children":[],"key":"YpK9mXQSvk"}],"key":"XPfv9pP5Wl"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"With this small sample of our data we can now train and evaluate the three classifiers.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"bUhJB4Tg3r"}],"key":"jaCvUz5t7G"}],"key":"njCl5gKK8C"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 1.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nJmKmkmewX"}],"identifier":"exercise-1-1","label":"Exercise 1.1","html_id":"exercise-1-1","implicit":true,"key":"DtqUHVH7IQ"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Implement a function below which evaluates each classifier passed into it on the given data, and then returns both the train and test scores of each as a list. You are allowed to import additional functions from whichever module you like, but you should be able to complete the function with ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"LgjusDfaow"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"inlineCode","value":"cross_validate","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"YDmLVGMM3g"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html","key":"IlhCP2flA3"},{"type":"text","value":" function and standard Python built-ins. Below the function you will find example output.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qBv56WnnC9"}],"key":"hfiw5kQsAp"}],"key":"WxhZFxLM7Q"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"def evaluate_learners(classifiers, X, y):\n    \"\"\" Evaluate each classifier in 'classifiers' with cross-validation on the provided (X, y) data. \n    \n    Given a list of scikit-learn classifiers [Classifier1, Classifier2, ..., ClassifierN] return two lists:\n     - a list with the scores obtained on the training samples for each classifier,\n     - a list with the test scores obtained on the test samples for each classifier.\n     The order of scores should match the order in which the classifiers were originally provided. E.g.:     \n     [Classifier1 train score, ..., ClassifierN train score], [Classifier1 test score, ..., ClassifierN test score]\n    \"\"\"\n    pass\n\n# # Example output:\n# train_scores, test_scores = ([[0.92 , 0.924, 0.916, 0.917, 0.921],  # Classifier 1 train score for each of 5 folds.\n#                               [0.963, 0.962, 0.953, 0.912, 0.934],  # Classifier 2 train score for each of 5 folds.\n#                               [0.867, 0.868, 0.865, 0.866, 0.866]], # Classifier 3 train score for each of 5 folds.\n#                              [[0.801, 0.811, 0.806, 0.826, 0.804],  # Classifier 1 test score for each of 5 folds.\n#                               [0.766, 0.756, 0.773, 0.756, 0.741],  # Classifier 2 test score for each of 5 folds.\n#                               [0.804, 0.814, 0.806, 0.821, 0.806]]) # Classifier 3 test score for each of 5 folds.","key":"NY83X088la"},{"type":"outputs","id":"i9uSJtTwVlEkX3DwpKb07","children":[],"key":"nohgsSiuxH"}],"key":"XufolGodti"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 1.2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"H0Y2fLq5pi"}],"identifier":"exercise-1-2","label":"Exercise 1.2","html_id":"exercise-1-2","implicit":true,"key":"DgoEG3laEG"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Call the function you created with a Logistic Regression, Linear SVM, and k-Nearest Neighbors Classifier.\nStore the return values in the variables ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Ko7f6DyjRV"},{"type":"inlineCode","value":"train_scores","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"C97fTSYEij"},{"type":"text","value":" and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qbNNYd5ZvL"},{"type":"inlineCode","value":"test_scores","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"msCduPv493"},{"type":"text","value":". Then, run the code given below to produce a plot visualizing the scores.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"uxXw6nuFLU"}],"key":"YEDOOA0wKP"}],"key":"wdNgrqZKVd"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Dummy code. Replace with the actual classifiers and scores\nclassifiers = [LogisticRegression()]\ntrain_scores, test_scores = [[0.6,0.7,0.8]], [[0.5,0.6,0.7]]","key":"M8JgGZRJ0k"},{"type":"outputs","id":"fKODBA-cbWzB7onef8lBh","children":[],"key":"SZfNllBfU9"}],"key":"xfnoeAmUWb"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Plot a bar chart of the train and test scores of all the classifiers, including the variance as error bars\nfig, ax = plt.subplots()\nwidth=0.3\nax.barh(np.arange(len(train_scores)), np.mean(test_scores, axis=1), width,\n        yerr= np.std(test_scores, axis=1), color='green', label='test')\nax.barh(np.arange(len(train_scores))-width, np.mean(train_scores, axis=1), width,\n        yerr= np.std(train_scores, axis=1), color='red', label='train')\nfor i, te, tr in zip(np.arange(len(train_scores)),test_scores,train_scores):\n    ax.text(0, i, \"{:.4f} +- {:.4f}\".format(np.mean(te),np.std(te)), color='white', va='center')\n    ax.text(0, i-width, \"{:.4f} +- {:.4f}\".format(np.mean(tr),np.std(tr)), color='white', va='center')\nax.set(yticks=np.arange(len(train_scores))-width/2, yticklabels=[c.__class__.__name__ for c in classifiers])\nax.set_xlabel('Accuracy')\nax.legend(bbox_to_anchor=(1.05, 1), loc=2)\n\nplt.show()","key":"wAiBijcie4"},{"type":"outputs","id":"ePzxSZbmNAwUyC5zVX-nK","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"image/png":{"content_type":"image/png","hash":"a0073c25fd1a7cdda19a6dd78efc110a","path":"/a0073c25fd1a7cdda19a6dd78efc110a.png"},"text/plain":{"content":"<Figure size 432x288 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"LsEupmSX1c"}],"key":"pN4jXzEXwE"}],"key":"cQ6TW0Mtu3"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 1.3","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ub9kM29X1e"}],"identifier":"exercise-1-3","label":"Exercise 1.3","html_id":"exercise-1-3","implicit":true,"key":"v1om9XIfaA"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Interpret the plot. Which is the best classifier? Are any of the models overfitting? If so, what can we do to solve this? Is there a lot of variance in the results?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"XjAifOcN0L"}],"key":"NO1ncnWtji"}],"key":"uM5c9TXVp0"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 2: Regularization","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"sLVtqycxGv"}],"identifier":"exercise-2-regularization","label":"Exercise 2: Regularization","html_id":"exercise-2-regularization","implicit":true,"key":"XY5sB3xApm"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"We will now tune these algorithm’s main regularization hyperparameter: the misclassification cost in SVMs (C), the regularization parameter in logistic regression (C), and the number of neighbors (n_neighbors) in kNN. We expect the optimum for the C parameters to lie in ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"XNImUTopdo"},{"type":"inlineMath","value":"[10^{-12},10^{12}]","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>12</mn></mrow></msup><mo separator=\"true\">,</mo><mn>1</mn><msup><mn>0</mn><mn>12</mn></msup><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[10^{-12},10^{12}]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">12</span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\">1</span><span class=\"mord\"><span class=\"mord\">0</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">12</span></span></span></span></span></span></span></span></span><span class=\"mclose\">]</span></span></span></span>","key":"bhNWxBkCA7"},{"type":"text","value":" and for n_neighbors between 1 and 50. C should be varied on a log scale (i.e. [0.01, 0.1, 1, 10, 100]) and k should be varied uniformly (i.e. [1,2,3,4]).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"m8TC0v77ch"}],"key":"A1XKMQHmQB"}],"key":"vByXE6UjcC"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 2.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FadpuGqL0P"}],"identifier":"exercise-2-1","label":"Exercise 2.1","html_id":"exercise-2-1","implicit":true,"key":"IgciiHDmAD"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Vary the regularization parameters in the range given above and, for each classifier, create a line plot that plots both the training and test score for every value of the regularization hyperparameter. Hence, you should produce 3 plots, one for each classifier. Use the default 5-fold cross validation for all scores, but only plot the means.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"onX6BjZtW0"}],"key":"ueY09DI1KC"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Hints:","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"RSbplVQLhk"}],"key":"hXN4yUQVK4"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":5,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"Think about the time complexity of these models. Trying too many hyperparameter values may take too much time.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"IUoZGkqCJZ"}],"key":"f8v3b9YRdL"}],"key":"PGtK10Kb6n"},{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"You can make use of numpy’s ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"OPEHyAGf72"},{"type":"link","url":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"logspace","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"B9qrjqBgWf"}],"urlSource":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.logspace.html","key":"snyVq47Aa8"},{"type":"text","value":", ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"T6FBOjihcv"},{"type":"link","url":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html?highlight=geomspace#numpy.geomspace","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"geomspace","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"dk1vRQX4Jy"}],"urlSource":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.geomspace.html?highlight=geomspace#numpy.geomspace","key":"Vay6IjZL0S"},{"type":"text","value":", and ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"KxcLoK6O4N"},{"type":"link","url":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html#numpy.linspace","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"linspace","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"CkWo52mRxr"}],"urlSource":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html#numpy.linspace","key":"ZtXxpSb9Ts"},{"type":"text","value":" functions.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"yJZzcuN2U0"}],"key":"CDcf8s8xtm"}],"key":"LPLiwvPrUR"},{"type":"listItem","spread":true,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"You can use matplotlib’s default ","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"kWAKoP5XzM"},{"type":"link","url":"https://matplotlib.org/tutorials/introductory/pyplot.html","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"plot","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"TF8UABKzVS"}],"urlSource":"https://matplotlib.org/tutorials/introductory/pyplot.html","key":"lm5LRpWrjF"},{"type":"text","value":" function to plot the train and test scores.","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"Endy8xX7MY"}],"key":"ZO7iGNQGwQ"}],"key":"PTYKYhfIaz"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"You can manually loop over the hyperparameter ranges, or you can already check out scikit-learn’s ","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"qOMNvAFWWn"},{"type":"link","url":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"GridSearchCV","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"cy0PRQ3Z2W"}],"urlSource":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html","key":"VTP9EBLXnH"},{"type":"text","value":" function to save some programming. We’ll see it again later in the course.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"OKbVq8x7o6"}],"key":"lWb0DGXt55"}],"key":"dEkCAidXUY"}],"key":"FCFWowmUTy"}],"key":"RwIGyKmqLI"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":4,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Solution","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"Y0hPNSfjCX"}],"identifier":"solution","label":"Solution","html_id":"solution","implicit":true,"key":"Qkrs14Fqj4"}],"key":"QTrglOwUqx"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Generic plot for 1D grid search\n# grid_search: the result of the GridSearchCV\n# param_name: the name of the parameter that is being varied\ndef plot_tuning(grid_search, param_name, ax):\n    ax.plot(grid_search.param_grid[param_name], grid_search.cv_results_['mean_test_score'], marker = '.', label = 'Test score')\n    ax.plot(grid_search.param_grid[param_name], grid_search.cv_results_['mean_train_score'], marker = '.', label = 'Train score')\n    ax.set_ylabel('score (ACC)')\n    ax.set_xlabel(param_name)\n    ax.legend()\n    ax.set_xscale('log')\n    ax.set_title(grid_search.best_estimator_.__class__.__name__)\n    bp, bs = grid_search.best_params_[param_name], grid_search.best_score_\n    ax.text(bp,bs,\"  C:{:.2E}, ACC:{:.4f}\".format(bp,bs))","key":"bCKGKyucJj"},{"type":"outputs","id":"6acrmUgXPa2RsHpGXkVP9","children":[],"key":"mQWixKSvtb"}],"key":"ER989MwDWe"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 2.2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"XyhYbNQQ5V"}],"identifier":"exercise-2-2","label":"Exercise 2.2","html_id":"exercise-2-2","implicit":true,"key":"voLp77G5hO"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Interpret the plots. When are the methods underfitting? When are they overfitting? How sensitive are they to the regularization hyperparameter?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bbwX9CRAUk"}],"key":"jbZDv5J2N0"}],"key":"fnDCTqQwwg"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 3: Interpreting misclassifications","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"zTIFx0W6v2"}],"identifier":"exercise-3-interpreting-misclassifications","label":"Exercise 3: Interpreting misclassifications","html_id":"exercise-3-interpreting-misclassifications","implicit":true,"key":"uu6DCDkOWF"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Chances are that your models are not yet perfect. It is important to understand what kind of errors it still makes. Let’s take a closer look at which instances are misclassified and which classes are often confused.\nTrain the logistic regression model with ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"AU5sBF2zU3"},{"type":"inlineCode","value":"C=1e-7","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"nsPOyPlTN2"},{"type":"text","value":". Train the model on a training set, and make predictions for a test set (both sets should be  sampled from our 10% subsample).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"bvpt8d6zPO"}],"key":"h7MeIp7fmK"}],"key":"BCu8lHSFyZ"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Create a stratified train-test split on a sample\nX_train, X_test, y_train, y_test = train_test_split(Xs,ys, stratify=ys, random_state=0)","key":"BJxGkiomS2"},{"type":"outputs","id":"7d6QoIaoyBGu7fc27Xe-l","children":[],"key":"h4U95KXV1k"}],"key":"eZCpyJxxSD"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 3.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"RZqWKYbpl4"}],"identifier":"exercise-3-1","label":"Exercise 3.1","html_id":"exercise-3-1","implicit":true,"key":"tUh8oLUjvq"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Train the classifier as described above, obtain the predictions ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"GiqmJiLeIf"},{"type":"inlineCode","value":"y_pred","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"goMnJ0j4ur"},{"type":"text","value":" on the test set, and identify all the misclassified samples ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"QszFwkex0X"},{"type":"inlineCode","value":"misclassified_samples","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"dt1HxOyMJr"},{"type":"text","value":". Then, run the visualization code below to study the misclassifications","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ksUrbYCvz4"}],"key":"pYV5vj4m7W"}],"key":"QUvKxqRZBz"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Implement the code to obtain the actual predictions on the test set\ny_pred = list(y_test) # dummy values, replace y_test with the actual predictions\n\n# Implement the code to obtain the indices of the misclassified samples\n# Example output:\n# misclassified_samples = [  11,   12,   14,   23,   30,   34,   39,   46,   50,   52,   55]\nmisclassified_samples = [0,1,2,3,4] # dummy values","key":"LwyBVGZJ7K"},{"type":"outputs","id":"uXmQAfi30BKAHz_nWcxX3","children":[],"key":"bpioUK54jQ"}],"key":"esYHTjYhpu"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Visualize the (first five) misclassifications, together with the predicted and actual class\nfig, axes = plt.subplots(1, 5,  figsize=(10, 5))\nfor nr, i in enumerate(misclassified_samples[:5]):\n    axes[nr].imshow(X_test.values[i].reshape(28, 28), cmap=plt.cm.gray_r)\n    axes[nr].set_xlabel(\"Predicted: %s,\\n Actual : %s\" % (fmnist_classes[int(y_pred[i])],fmnist_classes[int(y_test.values[i])]))\n    axes[nr].set_xticks(()), axes[nr].set_yticks(())\n\nplt.show();","key":"pAkKONMF3r"},{"type":"outputs","id":"cekoWEd68dhC0YattHbwq","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"d198b4130bb4b43112061511589c28b3","path":"/d198b4130bb4b43112061511589c28b3.png"},"text/plain":{"content":"<Figure size 720x360 with 5 Axes>","content_type":"text/plain"}}},"children":[],"key":"mmlf5pa1YJ"}],"key":"Rw3LXU0R4K"}],"key":"Kmut1xuagY"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 3.2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qGr6UQaviq"}],"identifier":"exercise-3-2","label":"Exercise 3.2","html_id":"exercise-3-2","implicit":true,"key":"zGM8aDdoxZ"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Interpret the results. Are these misclassifications to be expected?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"A3yUE0EKeR"}],"key":"rsnxNOctqW"}],"key":"WRgMe8IOkw"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 3.3.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"gqAlv7Luod"}],"identifier":"exercise-3-3","label":"Exercise 3.3.","html_id":"exercise-3-3","implicit":true,"key":"sNSIoSO5pM"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Run the code below on your results to draw the complete confusion matrix and get more insight on the systematic misclassifications\nof your model. A confusion matrix shows the amount of examples in for each pair of true and predicted classes. Interpret the results.\nDoes your model produce certain types of error more often than other types?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"XZUe75KQPN"}],"key":"b3VvZ1U3c4"}],"key":"Wo7aV7WVdF"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nfig, ax = plt.subplots()\nim = ax.imshow(cm)\nax.set_xticks(np.arange(10)), ax.set_yticks(np.arange(10))\nax.set_xticklabels(list(fmnist_classes.values()), rotation=45, ha=\"right\")\nax.set_yticklabels(list(fmnist_classes.values()))\nax.set_ylabel('True')\nax.set_xlabel('Predicted')\nfor i in range(100):\n    ax.text(int(i/10),i%10,cm[i%10,int(i/10)], ha=\"center\", va=\"center\", color=\"w\")","key":"L1ixxSQwLF"},{"type":"outputs","id":"pA2yZnGvqAkI-Wmm0K7el","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"image/png":{"content_type":"image/png","hash":"8d4f20705a03fa18717dd009e58c78ab","path":"/8d4f20705a03fa18717dd009e58c78ab.png"},"text/plain":{"content":"<Figure size 432x288 with 1 Axes>","content_type":"text/plain"}}},"children":[],"key":"mxQNzjOdKd"}],"key":"JeFRc7xPpL"}],"key":"RcH33l81uv"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 4: Interpreting model parameters","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"F3r229t9X1"}],"identifier":"exercise-4-interpreting-model-parameters","label":"Exercise 4: Interpreting model parameters","html_id":"exercise-4-interpreting-model-parameters","implicit":true,"key":"SPZXYgUv5Y"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Finally, we’ll take a closer look at the model parameters, i.e. the coefficients of our linear models. Since we are dealing with 28x28 pixel images, we have to learn 784 coefficients. What do these coefficients mean? We’ll start by plotting them as 28x28 pixel images.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"pkZnZ7YGOT"}],"key":"kbjppJW12C"}],"key":"orZuKjRJd1"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 4.1","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z93dGbIoAm"}],"identifier":"exercise-4-1","label":"Exercise 4.1","html_id":"exercise-4-1","implicit":true,"key":"iLz2yStQ6Y"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Train a Logistic Regression model and a Linear SVM using their tuned hyperparameters from exercise 2.\nWhen in doubt, use ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Yqzieyyyvi"},{"type":"inlineCode","value":"C=1e-7","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"BBVoT6keXR"},{"type":"text","value":" for LogReg and ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"veTFjmTk3b"},{"type":"inlineCode","value":"C=1e-8","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Hma835ETYy"},{"type":"text","value":" for the SVM.\nPass the trained model to the provided plotting function. Interpret the results in detail.\nWhy do you get multiple plots per model? What do the features represent in your data.\nDoes it seems like the models pay attention to the right features?\nDo you models seem to ignore certain features? Do you observe differences in quality between the different classes? Do you observe any differences between the models?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Z2u3MaPAOe"}],"key":"u4jbOrB0Ys"}],"key":"VYi5mQhywq"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Plots the coefficients of the given model as 28x28 heatmaps. \n# The `name` attribute is optional, it is simply a title for the produced figure\ndef plot_coefficients(model, name=None):\n    fig, axes = plt.subplots(1,10,figsize=(20,2))\n    fig.suptitle(name if name else model.__class__.__name__)\n    for i, ax in enumerate(axes):\n        m = ax.imshow(model.coef_[i].reshape(28,28))\n        ax.set_xlabel(fmnist_classes[i])\n        ax.set_xticks(()), ax.set_yticks(())\n    fig.colorbar(m, ax=axes.ravel().tolist())","key":"KAvvwk8XE4"},{"type":"outputs","id":"lDFB-XAYFTy0aVpaYpVID","children":[],"key":"aCOzqd0ZgT"}],"key":"kvfH7xSE3e"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 4.2","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"WhCOCfOZTg"}],"identifier":"exercise-4-2","label":"Exercise 4.2","html_id":"exercise-4-2","implicit":true,"key":"dkOYYJgvVa"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Repeat the previous exercise, but now only with logistic regression. In addition to a tuned version, also add a model that overfits a lot and one that underfits a lot. Interpret and explain the results.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"hQE2J5atNJ"}],"key":"OD7koJMH4R"}],"key":"HFzCeqZv6I"}],"key":"RwqllZ8QSU"},"references":{"cite":{"order":[],"data":{}}}}