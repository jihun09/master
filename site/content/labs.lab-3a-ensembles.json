{"version":3,"kind":"Notebook","sha256":"92c60e47ce169761feee1d3f296c5694ca7842130da4a9acade9acd0337926a9","slug":"labs.lab-3a-ensembles","location":"/labs/Lab 3a - Ensembles.ipynb","dependencies":[],"frontmatter":{"title":"Lab 3: Ensembles","content_includes_title":false,"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"github":"https://github.com/ml-course/master","copyright":"2025. CC0 Licensed - Use as you like. Appropriate credit is very welcome","numbering":{"title":{"offset":1}},"source_url":"https://github.com/ml-course/master/blob/master/labs/Lab 3a - Ensembles.ipynb","edit_url":"https://github.com/ml-course/master/edit/master/labs/Lab 3a - Ensembles.ipynb","exports":[{"format":"ipynb","filename":"Lab 3a - Ensembles.ipynb","url":"/Lab 3a - Ensembles-19d8fbe5808d24db92d5e967819f57ee.ipynb"}]},"widgets":{},"mdast":{"type":"root","children":[{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Using trees to detect trees","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"acfA9KiewQ"}],"identifier":"using-trees-to-detect-trees","label":"Using trees to detect trees","html_id":"using-trees-to-detect-trees","implicit":true,"key":"bmd69mFbjz"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"We will be using tree-based ensemble methods on the ","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"aZ9AP7zxG9"},{"type":"link","url":"https://www.openml.org/d/180","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Covertype dataset","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"lVrEmm3qyb"}],"urlSource":"https://www.openml.org/d/180","key":"Xht2fhPTRZ"},{"type":"text","value":".\nIt contains about 100,000 observations of 7 types of trees (Spruce, Pine, Cottonwood, Aspen,...) described by 55 features describing elevation, distance to water, soil type, etc.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"DQ2t7U42xF"}],"key":"T60n8EnoPu"}],"key":"OZt7Ei0gh0"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Auto-setup when running on Google Colab\nif 'google.colab' in str(get_ipython()):\n    !pip install openml\n\n# General imports\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport openml as oml\nimport seaborn as sns","key":"nWE580LaWX"},{"type":"outputs","id":"s57V3Cti33_gS4YUWyl3C","children":[],"key":"aZMpvo5ZBo"}],"key":"uDci4Q1194"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Download Covertype data. Takes a while the first time.\ncovertype = oml.datasets.get_dataset(180)\nX, y, _, _ = covertype.get_data(target=covertype.default_target_attribute, dataset_format='array'); \nclasses = covertype.retrieve_class_labels()\nfeatures = [f.name for i,f in covertype.features.items()][:-1]","key":"OT12c2Sm8n"},{"type":"outputs","id":"l2JNSHNwIxIFlWRGs0k_k","children":[],"key":"tzFv10sFeu"}],"key":"tewauCR30z"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"classes","key":"MZ2vow0lHs"},{"type":"outputs","id":"fTXFQuON6eRfiNvM7swVe","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":3,"metadata":{},"data":{"text/plain":{"content":"['Aspen',\n 'Cottonwood_Willow',\n 'Douglas_fir',\n 'Krummholz',\n 'Lodgepole_Pine',\n 'Ponderosa_Pine',\n 'Spruce_Fir']","content_type":"text/plain"}}},"children":[],"key":"Mh54hTigpr"}],"key":"lo7uZBoDZQ"}],"key":"skIbZw36yU"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"features[0:20]","key":"aa9wILJ36D"},{"type":"outputs","id":"-q9fbcrU8xHw52TcFtsmK","children":[{"type":"output","jupyter_data":{"output_type":"execute_result","execution_count":4,"metadata":{},"data":{"text/plain":{"content":"['elevation',\n 'aspect',\n 'slope',\n 'horizontal_distance_to_hydrology',\n 'Vertical_Distance_To_Hydrology',\n 'Horizontal_Distance_To_Roadways',\n 'Hillshade_9am',\n 'Hillshade_Noon',\n 'Hillshade_3pm',\n 'Horizontal_Distance_To_Fire_Points',\n 'wilderness_area1',\n 'wilderness_area2',\n 'wilderness_area3',\n 'wilderness_area4',\n 'soil_type_1',\n 'soil_type_2',\n 'soil_type_3',\n 'soil_type_4',\n 'soil_type_5',\n 'soil_type_6']","content_type":"text/plain"}}},"children":[],"key":"bbiZrjmyUk"}],"key":"zcNEGxFYyW"}],"key":"W9qJslhyTp"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"To understand the data a bit better, we can use a scatter matrix. From this, it looks like elevation is a relevant feature.\nDouglas Fir and Aspen grow at low elevations, while only Krummholz pines survive at very high elevations.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kIYXVkt799"}],"key":"Wx7XnAMbKb"}],"key":"PN5eam3RQI"},{"type":"block","kind":"notebook-code","children":[{"type":"code","lang":"python","executable":true,"value":"# Using seaborn to build the scatter matrix\n# only first 3 columns, first 1000 examples\nn_points = 1500\ndf = pd.DataFrame(X[:n_points,:3], columns=features[:3])\ndf['class'] = [classes[i] for i in y[:n_points]]\nsns.set(style=\"ticks\")\nsns.pairplot(df, hue=\"class\");","key":"PFdEC4KSgq"},{"type":"outputs","id":"IG_3y5boQOkg9VkaxqMrl","children":[{"type":"output","jupyter_data":{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"202089d09c5a4e87772184d58b503851","path":"/202089d09c5a4e87772184d58b503851.png"},"text/plain":{"content":"<Figure size 676.725x540 with 12 Axes>","content_type":"text/plain"}}},"children":[],"key":"EVLNAOdSQT"}],"key":"eYlZf9Pt1H"}],"key":"hKeW1akhPF"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 1: Random Forests","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"jKLKqY5mvs"}],"identifier":"exercise-1-random-forests","label":"Exercise 1: Random Forests","html_id":"exercise-1-random-forests","implicit":true,"key":"B7msH9FiIq"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Implement a function ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"mIOh9SjCLY"},{"type":"inlineCode","value":"evaluate_RF","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"yEPclqRMQE"},{"type":"text","value":" that measures the performance of a Random Forest Classifier, using trees\nof (max) depth 2,8,32,64, for any number of trees in the ensemble (","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"YwbUtzjAP5"},{"type":"inlineCode","value":"n_estimators","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"vmgX4qenRo"},{"type":"text","value":").\nFor the evaluation you should measure accuracy using 3-fold cross-validation.\nUse ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"IoHSR6RGYY"},{"type":"inlineCode","value":"random_state=1","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ZT9gB3OiAg"},{"type":"text","value":" to ensure reproducibility. Finally, plot the results for at least 5 values of ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"VEfuX51hdy"},{"type":"inlineCode","value":"n_estimators","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"xyO96GuAeN"},{"type":"text","value":" ranging from 1 to 30. You can, of course, reuse code from earlier labs and assignments. Interpret the results.\nYou can take a 50% subsample to speed the plotting.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"uNKP75dhhx"}],"key":"ru87zbRZFO"}],"key":"DHcGFeiBLh"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 2: Other measures","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"tX8uHhphtw"}],"identifier":"exercise-2-other-measures","label":"Exercise 2: Other measures","html_id":"exercise-2-other-measures","implicit":true,"key":"uYHALgZvOi"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Repeat the same plot but now use balanced_accuracy as the evaluation measure. See the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"dLByrK4WhX"},{"type":"link","url":"https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"documentation","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"aIlBWQfSOO"}],"urlSource":"https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score","key":"p5uHL6pGhu"},{"type":"text","value":".\nOnly use the optimal max_depth from the previous question. Do you see an important difference?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"GzaiJ4Cnxa"}],"key":"MLUGydmFqz"}],"key":"Ip5GkRjjH2"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 3: Feature importance","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pBmw6JxeG1"}],"identifier":"exercise-3-feature-importance","label":"Exercise 3: Feature importance","html_id":"exercise-3-feature-importance","implicit":true,"key":"H7xL21z8e3"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Retrieve the feature importances according to the (tuned) random forest model. Which feature are most important?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qHx8Hc0mud"}],"key":"K1DsuasSMf"}],"key":"ZdahFCB6OO"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 4: Feature selection","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"oS10WramYX"}],"identifier":"exercise-4-feature-selection","label":"Exercise 4: Feature selection","html_id":"exercise-4-feature-selection","implicit":true,"key":"xjtWzDjRH6"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Re-build your tuned random forest, but this time only using the first 10 features.\nReturn both the balanced accuracy and training time. Interpret the results.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"KhvtGrICIH"}],"key":"ycx23rUYQl"}],"key":"x86P2GODGc"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 5: Confusion matrix","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ItHSbb03R8"}],"identifier":"exercise-5-confusion-matrix","label":"Exercise 5: Confusion matrix","html_id":"exercise-5-confusion-matrix","implicit":true,"key":"VaLRheRvwO"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"Do a standard stratified holdout and generate the confusion matrix of the tuned random forest. Which classes are still often confused?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Epb3GXMimE"}],"key":"OJE7psIzTt"}],"key":"jR3KuqryiN"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 6: A second-level model","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"qD2FiXwp9J"}],"identifier":"exercise-6-a-second-level-model","label":"Exercise 6: A second-level model","html_id":"exercise-6-a-second-level-model","implicit":true,"key":"MqYPXryQx5"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Build a binary model specifically to correctly choose between the first and the second class.\nSelect only the data points with those classes and train a new random forest. Do a standard stratified split and plot the resulting ROC curve. Can we still improve the model by calibrating the threshold?","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"veqICGr1eB"}],"key":"ADf4V9WCCE"}],"key":"PbHDdZSaVh"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 7: Model calibration","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"nnSFQUC1FL"}],"identifier":"exercise-7-model-calibration","label":"Exercise 7: Model calibration","html_id":"exercise-7-model-calibration","implicit":true,"key":"AMyZoWgODr"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"For the trained binary random forest model, plot a calibration curve (see ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"jwq9kcsBfx"},{"type":"link","url":"https://ml-course.github.io/engineer/slides_html/03%20-%20Model%20Selection.slides.html#/40","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"children":[{"type":"text","value":"course notebook","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"QdcDyuxS4c"}],"urlSource":"https://ml-course.github.io/engineer/slides_html/03%20-%20Model%20Selection.slides.html#/40","key":"TnqDMo9xDl"},{"type":"text","value":").\nNext, try to correct for this using Platt Scaling (or sigmoid scaling).","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"hUdtftnIjH"}],"key":"kCIMQoCZgR"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Probability calibration should be done on new data not used for model fitting. The class ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"u4qrfdP9cu"},{"type":"link","url":"https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"CalibratedClassifierCV","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AV7Y3iMSLu"}],"urlSource":"https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html#sphx-glr-auto-examples-calibration-plot-calibration-curve-py","key":"Iu9Pr6ik5T"},{"type":"text","value":" uses a cross-validation generator and estimates for each split the model parameter on the train samples and the calibration of the test samples. The probabilities predicted for the folds are then averaged. Already fitted classifiers can be calibrated by CalibratedClassifierCV via the parameter cv=”prefit”. ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"xEQ3O00vmX"},{"type":"link","url":"https://scikit-learn.org/stable/modules/calibration.html","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"Read more","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"y3fyZU9W51"}],"urlSource":"https://scikit-learn.org/stable/modules/calibration.html","key":"ttnGaOZfSm"}],"key":"OE49qk3yHn"}],"key":"TOKVGjQuRf"},{"type":"block","kind":"notebook-content","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Exercise 8: Gradient Boosting","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"aMVVkI4jci"}],"identifier":"exercise-8-gradient-boosting","label":"Exercise 8: Gradient Boosting","html_id":"exercise-8-gradient-boosting","implicit":true,"key":"mwuOrAZTw1"},{"type":"paragraph","position":{"start":{"line":2,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Implement a function ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"hlqLnR5mPk"},{"type":"inlineCode","value":"evaluate_GB","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"lHlPSoumml"},{"type":"text","value":" that measures the performance of ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"Z7AEMRaTM2"},{"type":"inlineCode","value":"GradientBoostingClassifier","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"S4zJXsA6uT"},{"type":"text","value":" or the ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"qDXFUvy3fa"},{"type":"inlineCode","value":"XGBoostClassifier","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"kLcmFZ74oX"},{"type":"text","value":" for\ndifferent learning rates (0.01, 0.1, 1, and 10). As before, use a 3-fold cross-validation. You can use a 5% stratified sample of the whole dataset.\nFinally plot the results for ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"DuZR1Y9vmi"},{"type":"inlineCode","value":"n_estimators","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"VCARE1QRmh"},{"type":"text","value":" ranging from 1 to 100. Run all the GBClassifiers with ","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"iIGyvZeW6P"},{"type":"inlineCode","value":"random_state=1","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"ih5RogeBb1"},{"type":"text","value":" to ensure reproducibility.","position":{"start":{"line":2,"column":1},"end":{"line":2,"column":1}},"key":"wbpr5CbR0X"}],"key":"SrVCNoVkcC"}],"key":"hAOY8JKvBT"},{"type":"block","kind":"notebook-content","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Implement a function that plots the score of ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"KYFr3EPkAB"},{"type":"inlineCode","value":"evaluate_GB","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FlkYEEa9QR"},{"type":"text","value":" for ","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PXdDwoxVbB"},{"type":"inlineCode","value":"n_estimators","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"BLcjXWtuFi"},{"type":"text","value":" = 10,20,30,...,100 on a linear scale.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"PJAh4y40a0"}],"key":"AUy38dEn4Z"}],"key":"SHzzlP4YEK"}],"key":"WjJ8TyIUZH"},"references":{"cite":{"order":[],"data":{}}}}